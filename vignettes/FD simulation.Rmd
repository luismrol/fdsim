---
title: "Functional data simulation"
author: "Luis Miguel Roldan"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    #latex_engine: xelatex
    toc: true
    citation_package: natbib
    keep_tex: true

bibliography: lib.bib
header-includes:
  - \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
  - \usepackage{xcolor}
  - \usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}
  - \usepackage{setspace}
  - \doublespacing 
---

Functional observations are values or vectors of a function $X(t)$ mapped on values $t \in T$. The mean function of the process $X(t)$ can be defined as $\mu(t) = \text{E}(X(t))$ and the covariance function, $\Sigma(s,t) = \text{cov}_X(s, t)$. The sample mean function can be estimated as $$\bar{x}(t) = \frac{1}{N}\sum_{i=1}^Nx_i(t)$$,

Where N is the sample size, and the covariance function as:

$$\text{cov}_X(t_1, t_2) = \frac{1}{N-1}\sum_{i=1}^N [x_i(t_1)- \bar{x}(t_1)] [x_i(t_2)- \bar{x}(t_2)] $$.

A transformation $\mathbb{Z}_X(t) = X(t) - \bar{x}(t)$ can be used to define the sample covariance matrix estimator

$$S_X = \frac{1}{N-1} I_P \mathbb{Z}_X(t)\mathbb{Z}_X^T(t)$$ where $P$ is the number of different $t\in T$, $N$ is the number of functional observations and $I_P$ is the identity matrix of order $P$. The $t_1, t_2$ entry of the estimated sample covariance matrix is the covariance function evaluated at $t_1, t_2$.

Each observation of the univariate functional process is defined as

$$x_i(t) = \mu_X + \mathbb{Z}_{X_i}(t)$$

If we assume that $\mathbb{Z}_{X_i} \sim \mathbf{N} (0,\Sigma_X)$ , a univariate functional sample can be simulated based on a multivariate normal distribution

$$X(t) \sim \mathbf{N}(\mu_X(t), \Sigma_X)$$

Starting from a standard normal multivariate process

$$X'(t) \sim \mathbf{N}(\vec0, \mathbb{I}_{P\times P})$$

$X(t)$ can be simulated by establishing

$$X'(t) = (X(t) - \mu(t)) S^{-1/2}$$ And, as a consequence,

$$ \mathbb{Z}_X (t) = X(t) - \mu(t) =  X'(t) S^{1/2}$$

It is possible to get to $\mathbb{Z}_X(t)$ starting from $\Sigma_X$ through factorization, given that $\Sigma_X$ is positive definite.

#### Cholesky Decomposition

If $\Sigma_X$ is positive definite, the Cholesky decomposition enables the factorization

$$\Sigma_X = Q_CQ_C^T=\Sigma_{X_C}^{1/2}\Sigma_{X_C}^{1/2^T}$$

Given that $\Sigma_X$ is a covariance matrix, by definition it should be definite semipositive and, hence, enable for the implementation of the Cholesky decomposition. Nevertheless, in practice the covariance matrix can degenerate due to linear dependency. An option to this is either using pivoting for generating the Cholesky decomposition of a positive semidefinite matrix, or using the function `Matrix::nearPD` to find the nearest Positive Definite matrix. For our simulations, the pivoting method generated a higher deviation from the original matrix than the NearPD method.

Another option, implemented in `mvtnorm::rmvnorm` starts from eigendecomposition, replacing the negative eigenvalues to 0.

#### Eigendecomposition

For any diagonalizable matrix,

$$\Sigma_X = P\Lambda P^T=  P\Lambda^{1/2}\Lambda^{1/2}T P^T = Q_E Q_E^T= \Sigma_{X_E}^{1/2}\Sigma_{X_E}^{1/2^T}$$

Where P is a matrix with the eigenvalues of $\Sigma_X$ as columns, and $\Lambda=\vec{\lambda}I$ is a diagonal matrix with the eigenvalues in its diagonal. For negative values of $\lambda_i$, $\Lambda^{1/2} =\vec{\lambda^{1/2}}I $ is not real. To correct this issue in the simulation, we replace by 0 the negative eigenvalues.

##### Remark

Since \$Q_E \neq Q_C \$, the actual simulated values are not equal, even when the sample covariance matrix is similar.

## Defining $\Sigma_X$

$\Sigma_X$ can be simulated as any positive semidefinite matrix. Nevertheless, *smoothness* is usually referred as a property of functional data \citep{Ramsay2005}, as compared to multivariate or time series data. To model smoothness, an exponential decay function is used in simulation, with two possible specifications

-   Absolute Exponential Decay: $$ \Sigma_{ij}  = 
    e^{-\frac{|t_1 - t_2|} { \delta}}$$
-   Cuadratic Exponential Decay: $$ \Sigma_{ij}  = e^{-\frac{(t_1 - t_2)^2} { \delta^2}}$$

For all $t_1, t_2$ between 0 and 1.

#### Bivariate (Multivariate) correlation

For the second one, it is assume that the correlation between variables is uniform along the domain $t$. This means that if there are two functional variables, there is only one value for the correlation $rho$, and a correlation matrix like

$$\rho_X= \begin{bmatrix}
1&\rho_{12}\\
\rho_{12} & 1
\end{bmatrix}$$

### Simulation algorithm

The algorithm for the simulation is the following:

1.  Generate NxPxL i.i.d. random values from a Normal (0,1) distribution, where N = number of curves, P = Number of points inside each curve, L = number of variables (number of different mean curves).
2.  Build $\rho_x$ and $\Sigma_x$, and $I*\vec{|t-0.5|}$, if necessary. Those are the covariance kernel components.
3.  Factorize the three matrices in (2) using the same technique (either Eigendecomposition or Choleski).
4.  Perform multiplication of the Factors of the matrices to obtain $\mathbb{Z}_X$

## Examples

The following code chunk shows an example for only one bivariate curve (light curves are simulated.)

$$\mu_1 = 3*\cos(2\pi t) \\ \mu2 = 10-3*\cos(2\pi t) \\ \rho = 1 \\ N=1 \\ \text{exp. decay = Cuadratic}$$

```{r 1st}
remotes::install_github("luismrol/fdsim")
library(fdsim)
set.seed(123)

## Define a grid (only useful for mean and plotting)
grid <-seq(0,1, by = 1/99)

## Define a mean vector
mu1 <- 3*t(cos(2*pi*grid)) #+ t(cos(grid2))
mu2<- 10+ -3*t(cos(2*pi*grid))
mu = rbind(mu1, mu2)

## Build bivariate functional data

Y<-mfd_sim(1, mu, method = c("eigen"), covar = c("sq", "sq"), rho =1)

colnames(Y[[1]])<-grid
plot(x= grid, y = Y[[1]][1,], col ="white", main = "Multivariate data", ylab = "value", ylim = c(min(mu)-3,max(mu)+3))
for (i in 1:nrow(Y[[1]])){
  lines(grid, Y[[1]][i,], col = "gray")
}
for (i in 1:nrow(Y[[2]])){
  lines(grid, Y[[2]][i,], col = "pink")
}
lines(grid, mu[1,], col = "black")
lines(grid, mu[2,], col = "red")
# Legend
legend("topright",
       legend = c("Group 1 simulated", "Group 2 simulated", 
                  "Mean Group 1", "Mean Group 2"),
       col = c("gray", "pink", "black", "red"),
       lty = 1, lwd = c(0.5, 0.5, 2, 2), 
       bty = "n", cex = 0.7)

```

As can be seen, the deviation from the mean is forced to be equal for both variables. This is not the case if the $\rho$ parameter is modified. The following plot shows a case with $\rho = 0.5$

```{r 2nd}
#remotes::install_github("luismrol/fdsim")
library(fdsim)
set.seed(123)

Y<-mfd_sim(1, mu, method = c("eigen"), covar = c("sq", "sq"), rho =0.5)

colnames(Y[[1]])<-grid
plot(x= grid, y = Y[[1]][1,], col ="white", main = "Multivariate data", ylab = "value", ylim = c(min(mu)-3,max(mu)+3))
for (i in 1:nrow(Y[[1]])){
  lines(grid, Y[[1]][i,], col = "gray")
}
for (i in 1:nrow(Y[[2]])){
  lines(grid, Y[[2]][i,], col = "pink")
}
lines(grid, mu[1,], col = "black")
lines(grid, mu[2,], col = "red")
legend("topright",
       legend = c("Group 1 simulated", "Group 2 simulated", 
                  "Mean Group 1", "Mean Group 2"),
       col = c("gray", "pink", "black", "red"),
       lty = 1, lwd = c(0.5, 0.5, 2, 2), 
       bty = "n", cex = 0.7)

```

In this case the behavior is different but possitively related, with negative deviations from the mean occurring in both sides at the same time.

Changing the exponential decay factor also gives us a different view of the data. The following has the original specification (with $\rho = 1$) but with absolute exponential decay rate.

```{r 3rd}
#remotes::install_github("luismrol/fdsim")
library(fdsim)
set.seed(123)

Y<-mfd_sim(1, mu, method = c("eigen"), covar = c("abs", "abs"), rho =1)

colnames(Y[[1]])<-grid
plot(x= grid, y = Y[[1]][1,], col ="white", main = "Multivariate data", ylab = "value", ylim = c(min(mu)-3,max(mu)+3))
for (i in 1:nrow(Y[[1]])){
  lines(grid, Y[[1]][i,], col = "gray")
}
for (i in 1:nrow(Y[[2]])){
  lines(grid, Y[[2]][i,], col = "pink")
}
lines(grid, mu[1,], col = "black")
lines(grid, mu[2,], col = "red")
legend("topright",
       legend = c("Group 1 simulated", "Group 2 simulated", 
                  "Mean Group 1", "Mean Group 2"),
       col = c("gray", "pink", "black", "red"),
       lty = 1, lwd = c(0.5, 0.5, 2, 2), 
       bty = "n", cex = 0.7)

```

Generating 100 observations would lead to the following behavior:

```{r 4th}
#remotes::install_github("luismrol/fdsim")
library(fdsim)
set.seed(123)

Y<-mfd_sim(100, mu, method = c("eigen"), covar = c("abs", "abs"), rho =1)

colnames(Y[[1]])<-grid
plot(x= grid, y = Y[[1]][1,], col ="white", main = "Multivariate data", ylab = "value", ylim = c(min(mu)-3,max(mu)+3))
for (i in 1:nrow(Y[[1]])){
  lines(grid, Y[[1]][i,], col = "gray")
}
for (i in 1:nrow(Y[[2]])){
  lines(grid, Y[[2]][i,], col = "pink")
}
lines(grid, mu[1,], col = "black")
lines(grid, mu[2,], col = "red")
legend("topright",
       legend = c("Group 1 simulated", "Group 2 simulated", 
                  "Mean Group 1", "Mean Group 2"),
       col = c("gray", "pink", "black", "red"),
       lty = 1, lwd = c(0.5, 0.5, 2, 2), 
       bty = "n", cex = 0.7)

```

Modifying the heteroskedasticity parameter to TRUE, we have observations that are more variable near the center:

```{r 5th}
#remotes::install_github("luismrol/fdsim")
library(fdsim)
set.seed(123)

Y<-mfd_sim(100, mu, method = c("eigen"), covar = c("abs", "abs"), rho =1)

colnames(Y[[1]])<-grid
plot(x= grid, y = Y[[1]][1,], col ="white", main = "Multivariate data", ylab = "value", ylim = c(min(mu)-3,max(mu)+3))
for (i in 1:nrow(Y[[1]])){
  lines(grid, Y[[1]][i,], col = "gray")
}
for (i in 1:nrow(Y[[2]])){
  lines(grid, Y[[2]][i,], col = "pink")
}
lines(grid, mu[1,], col = "black")
lines(grid, mu[2,], col = "red")
legend("topright",
       legend = c("Group 1 simulated", "Group 2 simulated", 
                  "Mean Group 1", "Mean Group 2"),
       col = c("gray", "pink", "black", "red"),
       lty = 1, lwd = c(0.5, 0.5, 2, 2), 
       bty = "n", cex = 0.7)

```
