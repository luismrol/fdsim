
@article{Ramirez2011,
	title = {Una comparación cuantitativa de las guerras civiles colombianas, 1830-2010},
	url = {http://www.revistas.unal.edu.co/index.php/anpol/article/view/43689/44967},
	author = {Giraldo Ramírez, Jorge and Fortou, Jose Antonio},
	year = {2011},
	keywords = {1830, a quantitative comparison of, definiciones operacionales, duración, guerras civiles en colombia, poderes armados, s, severidad, the colombian civil wars},
	pages = {3--21},
	file = {PDF:/home/miguel/Zotero/storage/TP6P9HCK/2011-Una_comparacin_cuantitativa_de_las_guerras_civiles_colombianas_1830-2010.pdf:application/pdf},
}

@article{Stachl2019,
	title = {Behavioral {Patterns} in {Smartphone} {Usage} {Predict} {Big} {Five} {Personality} {Traits}},
	doi = {https://doi.org/10.31234/osf.io/ks4vd},
	abstract = {The understanding, quantification and evaluation of individual differences in behavior, feelings and thoughts have always been central topics in psychological science. An enormous amount of previous work on individual differences in behavior is exclusively based on data from self-report questionnaires. To date, little is known about how individuals actually differ in their objectively quantifiable behaviors and how differences in these behaviors relate to big five personality traits. Technological advances in mobile computer and sensing technology have now created the possiblity to automatically record large amounts of data about humans' natural behavior. The collection and analysis of these records makes it possible to analyze and quantify behavioral differences at unprecedented scale and efficiency. In this study, we analyzed behavioral data obtained from 743 participants in 30 consecutive days of smartphone sensing (25,347,089 logging-events). We computed variables (15,692) about individual behavior from five semantic categories (communication \& social behavior, music listening behavior, app usage behavior, mobility, and general day-\& nighttime activity). Using a machine learning approach (random forest, elastic net), we show how these variables can be used to predict self-assessments of the big five personality traits at the factor and facet level. Our results reveal distinct behavioral patterns that proved to be differentially-predictive of big five personality traits. Overall, this paper shows how a combination of rich behavioral data obtained with smartphone sensing and the use of machine learning techniques can help to advance personality research and can inform both practitioners and researchers about the different behavioral patterns of personality.},
	author = {Stachl, Clemens and Au, Quay and Schoedel, Ramona and Buschek, Daniel and Völkel, Sarah Theres and Oldemeier, Michelle and Ullmann, Theresa and Hussmann, Heinrich and Bischl, Bernd},
	year = {2019},
	keywords = {behavior, big five personality, interpretable machine, learning, machine learning, mobile-sensing, smartphones, trait-prediction},
	pages = {1--24},
	file = {PDF:/home/miguel/Zotero/storage/3KNRDM58/201X_STACHL_BEHAVIORAL PATTERNS.pdf:application/pdf},
}

@article{Eigenbrod2018,
	title = {How {Digital} {Nudges} {Influence} {Consumers} – {The} {Role} of {Social} and {Privacy} {Nudges} in {Retargeting}},
	volume = {2018},
	issn = {0065-0668},
	doi = {10.5465/ambpp.2018.11298abstract},
	abstract = {Retargeting is an innovative online marketing technique because it can provide consumer-specific advertising content based on consumers' browsing behavior that meet consumers' prefer-ences and interests. Although this advertising form offers great opportunities of bringing back customers who have left an online store without to complete a purchase, retargeting is risky be-cause the necessary data collection leads to strong privacy concerns which in turn, trigger con-sumer reactance and decreasing trust. Digital nudges - small design modifications in digital choice environments which guide peoples' behavior - present a promising concept to bypass these negative consequences of retargeting. In order to explore the positive effects of digital nudges in retargeting banners, we conducted a between-subject experiment with a subsequent survey which examines the impacts of social nudges (likes of friends) and privacy nudges (disclosure of privacy policy and purpose of retargeting banners). Whereas the social nudge led to a negative impact on consumers' privacy concerns and a positive impact on consumers' booking behavior, the privacy nudge did not have any significant impact. A combination of social nudge and privacy nudge showed that the privacy nudge negatively moderated the positive relationship between social nudge and consumers' booking behavior. The derived implications provide a theory for under-standing nudges in digital environments and we offer design principles for practitioners that ena-ble better retargeting outcomes. [ABSTRACT FROM AUTHOR] Copyright of Academy of Management Annual Meeting Proceedings is the property of Academy of Management and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	number = {1},
	journal = {Academy of Management Proceedings},
	author = {Eigenbrod, Laura and Janson, Andreas and Leimeister, Jan Marco},
	year = {2018},
	pages = {11298},
	file = {PDF:/home/miguel/Zotero/storage/7GG5C7W2/2019_EIGENBROD_DIGITAL NUDGES.pdf:application/pdf},
}

@article{ErnstandYoung,
	title = {Implementing {Behavioral} {Analytics} to drive costumer value: insurers cannot afford to wait},
	url = {http://www.ghbook.ir/index.php?name=فرهنگ و رسانه های نوین&option=com_dbook&task=readonline&book_id=13650&page=73&chkhashk=ED9C9491B4&Itemid=218&lang=fa&tmpl=component},
	author = {{Ernst and Young}},
	file = {PDF:/home/miguel/Zotero/storage/2NYGMM5Q/201X_ERNST AND YOUNG_IMPLEMENTING BA.pdf:application/pdf},
}

@article{Wang2019,
	title = {Integrating taxonomies into theory-based digital health interventions for behavior change: {A} holistic framework},
	volume = {21},
	issn = {14388871},
	doi = {10.2196/resprot.8055},
	abstract = {Digital health interventions (DHIs) have been emerging in the last decade. Due to their interdisciplinary nature, DHIs are guided and influenced by theories (eg, behavioral theories, behavior change technologies, and persuasive technology) from different research communities. However, DHIs are always coded using various taxonomies and reported in insufficient perspectives. This inconsistency and incomprehensiveness will cause difficulty in conducting systematic reviews and sharing contributions among communities. Therefore, based on existing related work, we propose a holistic framework that embeds behavioral theories, behavior change technique taxonomy, and persuasive system design principles. Including four development steps, two toolboxes, and one workflow, our framework aims to guide DHI developers to design, evaluate, and report their work in a formative and comprehensive way.},
	number = {1},
	journal = {Journal of Medical Internet Research},
	author = {Wang, Yunlong and Fadhil, Ahmed and Lange, Jan Philipp and Reiterer, Harald},
	year = {2019},
	note = {arXiv: 1810.08812},
	keywords = {Behavior change technique, Behavior change technique taxonomy, Digital health interventions, Persuasive system design, Persuasive technology},
	file = {PDF:/home/miguel/Zotero/storage/LIV4FB9P/20XX_WANG_INTEGRATING TAXONOMIES.pdf:application/pdf},
}

@article{Bou-Harb2017a,
	title = {Big {Data} {Behavioral} {Analytics} {Meet} {Graph} {Theory}: {On} {Effective} {Botnet} {Takedowns}},
	volume = {31},
	issn = {08908044},
	doi = {10.1109/MNET.2016.1500116NM},
	abstract = {Cyberspace continues to host highly sophisticated malicious entities that have demonstrated their ability to launch debilitating, intimidating, and disrupting cyber attacks. Recently, such entities have been adopting orchestrated, often botmaster-coordinated, stealthy attack strategies aimed at maximizing their targets' coverage while minimizing redundancy and overlap. The latter entities, which are typically dubbed as bots within botnets, are ominously being leveraged to cause drastic Internet-wide and enterprise impacts by means of severe misdemeanors. While a plethora of literature approaches have devised operational cyber security techniques for the detection of such botnets, very few have tackled the problem of how to promptly and effectively takedown such botnets. In the past three years, we have received 12 GB of daily malicious real darknet data (i.e., Internet traffic destined to half a million routable but unallocated IP addresses or sensors) from more than 12 countries. This article exploits such data to propose a novel Internet-scale cyber security capability that fuses big data behavioral analytics in conjunction with formal graph theoretical concepts to infer and attribute Internet-scale infected bots in a prompt manner and identify the niche of the botnet for effective takedowns. We validate the accuracy of the proposed approach by employing 100 GB of the Carna botnet, which is a very recent real malicious Internet-scale botnet. Since performance is also an imperative metric when dealing with big data for network security, this article further provides a comparison between two trending big data processing architectures: The almost standard Apache Hadoop system, and a more traditional and simplistic multi-Threaded programming approach, by employing 1 TB of real darknet data. Several recommendations and possible future research work derived from the previous experiments conclude this article.},
	number = {1},
	journal = {IEEE Network},
	author = {Bou-Harb, Elias and Debbabi, Mourad and Assi, Chadi},
	year = {2017},
	pages = {18--26},
	file = {PDF:/home/miguel/Zotero/storage/7XYYYB6R/201x_bou-harb_big data.pdf:application/pdf},
}

@article{Scch,
	title = {Predictive {Behavioral} {Analytics} from {Spatial}-{Temporal} {Data}},
	author = {{scch}},
	file = {PDF:/home/miguel/Zotero/storage/HI5AJ937/2018_SCCH_KVS_Profile_PredictiveBehavior.pdf:application/pdf},
}

@article{McAfee,
	title = {What {User} and {Entity} {Behavior} {Analytics} {Looks} {Like} {Now}},
	author = {{McAfee}},
	pages = {1--10},
	file = {PDF:/home/miguel/Zotero/storage/EM92D8CE/201X_MCAFFEE_BA.pdf:application/pdf},
}

@article{Niara2016,
	title = {No {Compromise} {Behavioral} {Analytics}},
	author = {{Niara}},
	year = {2016},
	pages = {1--2},
	file = {PDF:/home/miguel/Zotero/storage/6ZYHNT5Y/201X_NIARA_SOLUTIONS BRIEF.pdf:application/pdf},
}

@article{Trendalyze.com,
	title = {Turn {Insights} into {Instant} {Actions} ™},
	author = {{Trendalyze.com}},
	file = {PDF:/home/miguel/Zotero/storage/VU2RI8R5/201X_TRENDALYZE BROCHURE.pdf:application/pdf},
}

@article{Touma2017,
	title = {Framework for behavioral analytics in anomaly identification},
	volume = {10190},
	issn = {1996756X},
	doi = {10.1117/12.2266374},
	abstract = {© COPYRIGHT SPIE. Behavioral Analytics (BA) relies on digital breadcrumbs to build user profiles and create clusters of entities that exhibit a large degree of similarity. The prevailing assumption is that an entity will assimilate the group behavior of the cluster it belongs to. Our understanding of BA and its application in different domains continues to evolve and is a direct result of the growing interest in Machine Learning research. When trying to detect security threats, we use BA techniques to identify anomalies, defined in this paper as deviation from the group behavior. Early research papers in this field reveal a high number of false positives where a security alert is triggered based on deviation from the cluster learned behavior but still within the norm of what the system defines as an acceptable behavior. Further, domain specific security policies tend to be narrow and inadequately represent what an entity can do. Hence, they: a) limit the amount of useful data during the learning phase; and, b) lead to violation of policy during the execution phase. In this paper, we propose a framework for future research on the role of policies and behavior security in a coalition setting with emphasis on anomaly detection and individual's deviation from group activities.},
	journal = {Ground/Air Multisensor Interoperability, Integration, and Networking for Persistent ISR VIII},
	author = {Touma, Maroun and Bertino, Elisa and Rivera, Brian and Verma, Dinesh and Calo, Seraphin},
	year = {2017},
	note = {ISBN: 9781510608818},
	keywords = {behavioral analytics, coalition operations, generative models, policy},
	pages = {101900H},
	file = {PDF:/home/miguel/Zotero/storage/GWNBREIL/201X_TOUMA ET AL_FRAMEWORK FOR BEHAVIORAL ANALYTICS.pdf:application/pdf},
}

@article{Solano2019,
	title = {Risk-{Based} {Static} {Authentication} in {Web} {Applications} with {Behavioral} {Biometrics} and {Session} {Context} {Analytics}},
	issn = {16113349},
	doi = {10.1007/978-3-030-29729-9_1},
	author = {Solano, Jesus and Camacho, Luis and Correa, Alejandro and Deiro, Claudio and Vargas, Javier and Ochoa, Martín},
	year = {2019},
	note = {ISBN: 9783030297282},
	keywords = {behavioral dynamics, machine learn-, static authentication},
	pages = {3--23},
	file = {PDF:/home/miguel/Zotero/storage/KZRDGS62/201x_solano_risk based.pdf:application/pdf},
}

@article{InternalRevenueService2017,
	title = {Behavioral {Insights} {Kit}},
	url = {https://www.zhaw.ch/de/sml/institute-zentren/imm/ueber-uns/behavioral-insights-kit/},
	abstract = {This Behavioral Insights Toolkit was created as a practical resource for use by IRS employees and researchers seeking to incorporate Behavioral Insights into their work. This Toolkit describes the field of Behavioral Insights, its potential benefits, and how Behavioral Insights can be practically applied to serve taxpayers and help the IRS achieve its mission. It highlights examples of opportunity areas where Behavioral Insights has been applied both internally at the IRS and across the globe.},
	author = {{Internal Revenue Service}},
	year = {2017},
	pages = {1--73},
	file = {PDF:/home/miguel/Zotero/storage/QF5B8ZME/2017_IRS_BEHAVIORAL INSIGHTS.pdf:application/pdf},
}

@book{EuropeanComission2019,
	title = {Behavioural study on the digitalisation of the marketing and distance selling of retail financial services},
	isbn = {978-92-9478-097-3},
	author = {{European Comission}},
	year = {2019},
	note = {Issue: April},
	file = {PDF:/home/miguel/Zotero/storage/XQDKFLLF/2015_EUROPEAN COMISSION.pdf:application/pdf},
}

@article{SocialandBehavioralSciencesTeam2016,
	title = {Social and {Behavioral} {Sciences} {Team} {Annual} {Report} {Executive} {Office} of the {President} {National} {Science} and {Technology} {Council}},
	abstract = {Dear Colleagues: The Obama Administration has made significant progress both in using and building evidence to improve government performance and deliver better results at a lower cost for the Nation. A strong body of evidence demonstrates that research insights about behavior, when incorporated into the design of Federal programs and policies, have significantly improved Americans' lives, whether by boosting retirement savings nationwide or helping more low-income students get to college each year. Building on this work, in 2014, the White House Office of Science and Technology Policy assem-bled the Social and Behavioral Sciences Team (SBST)—a cross-agency group of experts in applied behavioral science that translates findings and methods from the social and behavioral sciences into improvements in Federal policies and programs. The Social and Behavioral Sciences Team Annual Report represents SBST's successful efforts in its first year to apply behavioral insights to Federal policy and to measure the impact of these applications on key policy outcomes using rigorous evaluations. The report covers two core areas on which SBST focused in its first year: streamlining access to programs and improving government efficiency. As a result of SBST projects, more Service-members are saving for retirement, more students are going to college and better managing their student loans, more Veterans are taking advantage of education and career counseling benefits, more small farms are gaining access to credit, and more families are securing health insurance coverage. And improvements in government program integrity and efficiency are saving taxpayer dollars.},
	number = {September},
	author = {{Social and Behavioral Sciences Team}},
	year = {2016},
	pages = {63},
	file = {PDF:/home/miguel/Zotero/storage/UYBMZPRK/m-api-679a9617-eb8d-745d-7bec-d309a52b945f.pdf:application/pdf},
}

@article{MrPatriceMullerLondonEconomicsProjectdirectorMsMetteDamgaardLondonEconomicsProjectmanagerandleadauthorMsAnnabelLitchfieldLondonEconomicsMrMarkLewisLondonEconomicsDrJuliaHornle2011,
	title = {Consumer behaviour in a digital environment},
	abstract = {This study analyses consumer behaviour and the interaction between consumers and businesses in the digital environment. At issue is how consumers benefit from the digital environment and whether and how they change their purchasing behaviour. A number of barriers to e-commerce and a more integrated European digital market are identified and specific policy recommendations are provided.},
	author = {Mr Patrice Muller, London Economics (Project director) Ms Mette Damgaard, London Economics (Project manager {and} lead author) Ms Annabel Litchfield, London Economics Mr Mark Lewis, London Economics Dr Julia Hörnle, Queen Mary University of London},
	year = {2011},
	pages = {134},
	file = {PDF:/home/miguel/Zotero/storage/QVT2LT7F/2011_EUROPEAN PARLIAMENT.pdf:application/pdf},
}

@article{Weinmann2016,
	title = {Digital {Nudging}},
	volume = {58},
	issn = {18670202},
	doi = {10.1007/s12599-016-0453-1},
	abstract = {People's decisions are influenced by the decision environment. In fact, no choice is made in a vacuum, as there is no neutral way to present choices. Presenting choices in certain ways — even if this happens unintendedly — can thus "nudge" people and change their behavior in predictable ways. "Nudging" is a concept from behavioral economics that describes how relatively minor changes to decision environments (e.g., setting defaults) influence decision outcomes — which often remain unnoticed by the decision maker. We extend the nudging concept to the digital environment. We define "digital nudging" as the use of user-interface design elements to guide people’s behavior in digital choice environments. We propose a digital-nudging process and identify opportunities for future research.},
	number = {6},
	journal = {Business and Information Systems Engineering},
	author = {Weinmann, Markus and Schneider, Christoph and Brocke, Jan vom},
	year = {2016},
	note = {Publisher: Springer Fachmedien Wiesbaden},
	keywords = {Human–computer interaction, Information systems design, Nudging, Online choice architecture},
	pages = {433--436},
	file = {PDF:/home/miguel/Zotero/storage/TP529N3T/2015_WEINMANN_DIGITAL NUDGING.pdf:application/pdf},
}

@article{Hekler2016,
	title = {Advancing {Models} and {Theories} for {Digital} {Behavior} {Change} {Interventions}},
	volume = {51},
	issn = {18732607},
	url = {http://dx.doi.org/10.1016/j.amepre.2016.06.013},
	doi = {10.1016/j.amepre.2016.06.013},
	abstract = {To be suitable for informing digital behavior change interventions, theories and models of behavior change need to capture individual variation and changes over time. The aim of this paper is to provide recommendations for development of models and theories that are informed by, and can inform, digital behavior change interventions based on discussions by international experts, including behavioral, computer, and health scientists and engineers. The proposed framework stipulates the use of a state-space representation to define when, where, for whom, and in what state for that person, an intervention will produce a targeted effect. The “state” is that of the individual based on multiple variables that define the “space” when a mechanism of action may produce the effect. A state-space representation can be used to help guide theorizing and identify crossdisciplinary methodologic strategies for improving measurement, experimental design, and analysis that can feasibly match the complexity of real-world behavior change via digital behavior change interventions.},
	number = {5},
	journal = {American Journal of Preventive Medicine},
	author = {Hekler, Eric B. and Michie, Susan and Pavel, Misha and Rivera, Daniel E. and Collins, Linda M. and Jimison, Holly B. and Garnett, Claire and Parral, Skye and Spruijt-Metz, Donna},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {825--832},
	file = {PDF:/home/miguel/Zotero/storage/N9X8SR93/2016_HEKLER_ADVANCING MODELS.pdf:application/pdf},
}

@article{Mirsch2017,
	title = {Digital {Nudging}: {Altering} {User} {Behavior} in {Digital} {Environments}},
	abstract = {Individuals make increasingly more decisions on screens, such as those on websites or mobile apps. However, the nature of screens and the vast amount of information available online make individuals particularly prone to deficient decisions. Digital nudging is an approach based on insights from behavioral economics that applies user interface (UI) design elements to affect the choices of users in digital environments. UI design elements include graphic design, specific content, wording or small features. To date, little is known about the psychological mechanisms that underlie digital nudging. To address this research gap, we conducted a systematic literature review and provide a comprehensive overview of relevant psychological effects and exemplary nudges in the physical and digital sphere. These insights serve as a valuable basis for researchers and practitioners that aim to study or design information systems and interventions that assist user decision making on screens.},
	journal = {Proceedings of 13th International Conference on Wirtschaftsinformatik},
	author = {Mirsch, Tobias and Lehrer, Christiane and Jung, Reinhard},
	year = {2017},
	keywords = {Behavioral Economics, Choice Architecture, Digital Nudging, Human-Computer Interaction, User Interface Design},
	pages = {634--648},
	file = {PDF:/home/miguel/Zotero/storage/4DZ4ITYM/m-api-4f78807f-8507-d14b-45ef-22a220bb9eeb.pdf:application/pdf},
}

@article{IHSMarkit2017,
	title = {The economic value of behavioural targeting in digital advertising},
	abstract = {Digital advertising generates annual revenue of €41.9 billion1 in Europe, growing at a double-digit rate of 12.3\% year-on-year in 2016. An increasing percentage of this revenue and underlying growth is attributable to the use of data, and behavioural targeting in particular. Data is fundamentally transforming the mechanisms and procedures that underpin digital advertising in two ways: Firstly, data fuels transactional or workflow automation mechanisms that use a set of rules applied by software and algorithms. Commonly known as programmatic advertising, these mechanisms make digital advertising more effective and efficient. Programmatic advertising allows advertising performance to be measured and adjusted in real-time as campaigns evolve. It provides the technology to deliver advertising to users individually within milliseconds, without cumbersome manual processes. On the internet, audiences’ attention is highly fragmented across different websites and apps. Programmatic advertising connects different media properties, such as newspapers, and aggregates their advertising supply. This lowers barriers to market participation for publishers and others looking to secure advertising revenue. Smaller publishers can compete for advertising money that they would otherwise not be able to tap into, because they are not known to advertisers or lack the strong sales force of larger media companies. Secondly, data allows to address consumers in a way that is more relevant to their circumstances and experience (targeted advertising). Behavioural data, which could include on-site and off-site browsing, purchase behaviour and signals of intent, allows companies to tailor advertising messages to consumers and reach them when their message is most relevant. This makes advertising more worthwhile and less interruptive for the consumer, and more effective for the advertiser. As people’s tastes, cultural affiliations and life journeys have diversified, traditional criteria for segmenting consumers (age groups, income brackets or occupational status, for example) have become insufficient. Behavioural data allows advertisers to identify and describe new consumer groups, such as ‘auto intenders’ who are seeking to buy a car. This reduces ‘wastage’, or advertising to the wrong consumers, reduces frustration and ensures consumers encounter products and messages that are genuinely useful to them.},
	author = {{IHS Markit}},
	year = {2017},
	pages = {5 pages},
	file = {PDF:/home/miguel/Zotero/storage/JZQX7TWP/2017_IHS_BEHAVIORAL DATA ADVERTISING.pdf:application/pdf},
}

@article{UNICEF2013,
	title = {Behavioural {Insights} at the {United} {Nations}: {Achieving} {Agenda} 2030},
	author = {{UNICEF}},
	year = {2013},
	pages = {1--18},
	file = {PDF:/home/miguel/Zotero/storage/W23MMRJE/2017_ONU_BEHAVIORAL INSIGHTS.pdf:application/pdf},
}

@article{Krajnovic2016,
	title = {Digital marketing and behavioral {Economics}},
	number = {October},
	author = {Krajnovic, Aleksandra and Sikiric, Dominik and Bosna, Jurica},
	year = {2016},
	file = {PDF:/home/miguel/Zotero/storage/EQJY8UEC/2016_KRAJNOVIC_DIGITAL MARKETING.pdf:application/pdf},
}

@article{Afif,
	title = {Behavioral {Science} {Around} the {World}: {Profiles} of 10 {Countries}},
	journal = {Country profiles},
	author = {Afif, Seina and Islan, Wade and Calvo-Gonzalez, Oscar and Dalton, Abigail},
	file = {PDF:/home/miguel/Zotero/storage/PJ97PGBG/2019_BANCO MUNDIAL_PERFILES.pdf:application/pdf},
}

@article{Foster2017,
	title = {Applying behavioural insights to organisations: {Theoretical} underpinnings ({EC}-{OECD} {Seminar} {Series} on {Designing} better economic development policies for regions and cities)},
	number = {May},
	author = {Foster, Lori},
	year = {2017},
	file = {PDF:/home/miguel/Zotero/storage/VJCGV8SV/m-api-321dff37-cd3c-174f-3862-b5dfd62ba71f.pdf:application/pdf},
}

@article{Martin2017,
	title = {Building {Behavioral} {Science} {Capability} in {Your} {Company}},
	url = {http://www.ghbook.ir/index.php?name=فرهنگ و رسانه های نوین&option=com_dbook&task=readonline&book_id=13650&page=73&chkhashk=ED9C9491B4&Itemid=218&lang=fa&tmpl=component},
	author = {Martin, Steve and Ferrere, Antoine},
	year = {2017},
	file = {PDF:/home/miguel/Zotero/storage/A6SEAASN/2019_Building Behavioral Science Capability in Your Company.pdf:application/pdf},
}

@article{Dickson2018,
	title = {Behavioral science in business : {Nudging} , debiasing , and managing the irrational mind},
	issn = {0168-0102},
	url = {https://www.mckinsey.com/business-functions/organization/our-insights/behavioral-science-in-business-nudging-debiasing-and-managing-the-irrational-mind},
	doi = {10.1016/j.neures.2011.07.500},
	abstract = {Behavioral science has become a hot topic in companies and organizations trying to address the biases that drive day-to-day decisions and actions. Although},
	number = {February},
	journal = {McKinsey Podcast},
	author = {Dickson, T. and Sperling, J. and Guntner, A.},
	year = {2018},
	pages = {8},
	file = {PDF:/home/miguel/Zotero/storage/LU8NVFBH/2018_Mckinsey_nudging, debiasing.pdf:application/pdf},
}

@article{NICE-Actimize2017,
	title = {The emergence of behavioral analytics in {Financial} {Markets} {Compliance}},
	author = {{NICE-Actimize}},
	year = {2017},
	pages = {1--16},
	file = {PDF:/home/miguel/Zotero/storage/LHNX7SHV/2017_NIVE ACTIMIZE.pdf:application/pdf},
}

@article{Bootcamp2019,
	title = {{BE} {Bootcamp}},
	author = {Bootcamp, Behavioral Economics and Leaders, Product},
	year = {2019},
}

@article{Mintz2017,
	title = {Behavioral {Analytics} for {Myopic} {Agents}},
	url = {http://arxiv.org/abs/1702.05496},
	abstract = {Many multi-agent systems have the structure of a single coordinator providing behavioral or financial incentives to a large number of agents. Two challenges faced by the coordinator are a finite budget from which to allocate incentives, and an initial lack of knowledge about the utility function of the agents. Here, we present a behavioral analytics approach to solve the coordinator's problem when agents make decisions by maximizing utility functions that depend on prior system states, inputs, and other parameters that are initially unknown and subject to temporal dynamics. Our behavioral analytics framework involves three steps: first, we develop a behavioral model that describes the decision-making process of an agent; second, we use data to estimate behavioral model parameters for each agent and then use these estimates to predict future decisions of each agent; and third, we use the estimated behavioral model parameters to optimize a set of costly incentives to provide to each agent. In this paper, we describe a specific set of tools, models, and approaches that fit into this framework, and that adapt models and incentives as new information is collected by repeating the second and third steps of this framework. Furthermore, we prove that incentives computed by this adaptive approach are asymptotically optimal with respect to a given loss function that describes the coordinator's objective. We optimize incentives using a decomposition scheme, where each sub-problem solves the coordinator's problem for a single agent, and the master problem is a pure integer program. We conclude with a simulation study to evaluate the effectiveness of our behavioral analytics approach in designing personalized treatment plans for a weight loss program. The results show our approach maintains efficacy of the program while reducing costs by up to 60\%, while adaptive heuristics provide less savings.},
	author = {Mintz, Yonatan and Aswani, Anil and Kaminsky, Philip and Flowers, Elena and Fukuoka, Yoshimi},
	year = {2017},
	note = {arXiv: 1702.05496},
	keywords = {health care, optimization, statistical inference},
	file = {PDF:/home/miguel/Zotero/storage/WJ5VY9VW/2017_BEHAVIORAL ANALYTICS FOR MIOPIC AGENTS.pdf:application/pdf},
}

@article{Kahneman1979,
	title = {Prospect {Theory}: {An} {Analysis} of {Decision} under {Risk}},
	volume = {47},
	issn = {00129682},
	doi = {10.2307/1914185},
	abstract = {This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling},
	number = {2},
	journal = {Econometrica},
	author = {Kahneman, Daniel and Tversky, Amos},
	year = {1979},
	note = {ISBN: 9781315196350},
	pages = {263},
	file = {PDF:/home/miguel/Zotero/storage/MNXZJYDC/1979_KAHNEMAN Y TVERSKY_ PROSPECT THEORY.pdf:application/pdf},
}

@article{IPSOS2019,
	title = {Fueling growth with behavioral science},
	author = {{IPSOS}},
	year = {2019},
	file = {PDF:/home/miguel/Zotero/storage/PRRATRZX/2019_IPSOS_FUELING GROWTH.pdf:application/pdf},
}

@article{Monash2018,
	title = {Multimodal * {Multisensor} {Behavioral2Analytics} : {Going2Deeper2into2Human} * {Centred} {Design2} {Getting} ' {Started} :' {Resources}},
	author = {Monash, Technologies},
	year = {2018},
	file = {PDF:/home/miguel/Zotero/storage/5K5BZEFZ/2018_SHARON OVIATT_MULTIMODAL MULTISENSOR.pdf:application/pdf},
}

@article{Forcepoint,
	title = {Forcepoint {Behavioral} {Analytics}},
	journal = {Solution Briefs},
	author = {{Forcepoint}},
	pages = {1--5},
	file = {PDF:/home/miguel/Zotero/storage/I63ZLQ9G/2019_FORCEPOINT_CORPORATIVO.pdf:application/pdf},
}

@techreport{Mcnab,
	title = {Uncover {Infected} {Hosts} and {Security} {Threats} with {Behavioral} {Analytics}},
	author = {Mcnab, Chris},
	note = {Volume: 1
Issue: 6},
	file = {PDF:/home/miguel/Zotero/storage/7SMH2EF9/201X_ALPHASOC BROCHURE.pdf:application/pdf},
}

@article{Okeke2018a,
	title = {Good vibrations: {Can} a digital nudge reduce digital overload?},
	doi = {10.1145/3229434.3229463},
	abstract = {Digital overuse on mobile devices is a growing problem in everyday life. This paper describes a generalizable mobile intervention that combines nudge theory and negative reinforcement to create a subtle, repeating phone vibration that nudges a user to reduce their digital consumption. For example, if a user has a daily Facebook limit of 30 minutes but opens Facebook past this limit, the user's phone will issue gentle vibrations every five seconds, but the vibration stops once the user navigates away from Facebook. We evaluated the intervention through a three-week controlled experiment with 50 participants on Amazon's Mechanical Turk platform with findings that show daily digital consumption was successfully reduced by over 20\%. Although the reduction did not persist after the intervention was removed, insights from qualitative feedback suggest that the intervention made participants more aware of their app usage habits; and we discuss design implications of episodically applying our intervention in specific everyday contexts such as education, sleep, and work. Taken together, our findings advance the HCI community's understanding of how to curb digital overload.},
	journal = {MobileHCI 2018 - Beyond Mobile: The Next 20 Years - 20th International Conference on Human-Computer Interaction with Mobile Devices and Services, Conference Proceedings},
	author = {Okeke, Fabian and Sobolev, Michael and Dell, Nicola and Estrin, Deborah},
	year = {2018},
	note = {ISBN: 9781450358989},
	keywords = {Digital Nudge, Digital Overload, Intervention, Negative Reinforcement, Smartphones, Social Media, Vibration},
	file = {PDF:/home/miguel/Zotero/storage/Z33M4RQ8/2018_OKEKE_GOOD VIBRATIONS.pdf:application/pdf},
}

@article{Jana2019,
	title = {{AppMine}: {Behavioral} {Analytics} for {Web} {Application} {Vulnerability} {Detection}},
	url = {http://arxiv.org/abs/1908.01928},
	abstract = {Web applications in widespread use have always been the target of large-scale attacks, leading to massive disruption of services and financial loss, as in the Equifax data breach. It has become common practice to deploy web application in containers like Docker for better portability and ease of deployment. We design a system called AppMine for lightweight monitoring of web applications running in Docker containers and detection of unknown web vulnerabilities. AppMine is an unsupervised learning system, trained only on legitimate workloads of web application, to detect anomalies based on either traditional models (PCA and one-class SVM), or more advanced neural-network architectures (LSTM). In our evaluation, we demonstrate that the neural network model outperforms more traditional methods on a range of web applications and recreated exploits. For instance, AppMine achieves average AUC scores as high as 0.97 for the Apache Struts application (with the CVE-2017-5638 exploit used in the Equifax breach), while the AUC scores for PCA and one-class SVM are 0.81 and 0.83, respectively.},
	number = {2},
	author = {Jana, Indranil and Oprea, Alina},
	year = {2019},
	note = {arXiv: 1908.01928},
	file = {PDF:/home/miguel/Zotero/storage/9TURQMEX/2019_OPREA_BEHAVIORAL ANALYTICS FOR WEB APPLICATION.pdf:application/pdf},
}

@techreport{Davis2018,
	title = {Behavioral {Design} for {Digital} {Financial} {Services}},
	institution = {Ideas42},
	author = {Davis, Katy and Kau, Maddie and Kim, Abigail},
	year = {2018},
	note = {Issue: March},
	pages = {49},
	file = {PDF:/home/miguel/Zotero/storage/R4WWY4CS/2018_DAVIS ET AL_ BEHAVIORAL DESIGN.pdf:application/pdf},
}

@article{Wirth2000,
	title = {{CRISP}-{DM} : {Towards} a {Standard} {Process} {Model} for {Data} {Mining}},
	doi = {10.1.1.198.5133},
	abstract = {The CRISP-DM (CRoss Industry Standard Process for Data Mining) project proposed a comprehensive process model for carrying out data mining projects. The process model is independent of both the industry sector and the technology used. In this paper we argue in favor of a standard process model for data mining and report some experiences with the CRISP-DM process model in practice. We applied and tested the CRISP-DM methodology in a response modeling application project. The final goal of the project was to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations. The initial projects were performed by experienced data mining people; future projects are to be performed by people with lower technical skills and with very little time to experiment with different approaches. It turned out, that the CRISP-DM methodology with its distinction of generic and specialized process models provides both the structure and the flexibility necessary to suit the needs of both groups. The generic CRISP-DM process model is useful for planning, communication within and outside the project team, and documentation. The generic check-lists are helpful even for experienced people. The generic process model provides an excellent foundation for developing a specialized process model which prescribes the steps to be taken in detail and which gives practical advice for all these steps.},
	number = {24959},
	journal = {Proceedings of the Fourth International Conference on the Practical Application of Knowledge Discovery and Data Mining},
	author = {Wirth, Rüdiger},
	year = {2000},
	pages = {29--39},
	file = {PDF:/home/miguel/Zotero/storage/XU3AUTL5/CRISP-DM.pdf:application/pdf},
}

@book{Corr2009,
	title = {The {Cambridge} {Handbook} of {Personality} {Psychology}},
	isbn = {978-0-521-86218-9},
	url = {http://www.ghbook.ir/index.php?name=فرهنگ و رسانه های نوین&option=com_dbook&task=readonline&book_id=13650&page=73&chkhashk=ED9C9491B4&Itemid=218&lang=fa&tmpl=component},
	publisher = {Cambridge university Press},
	author = {Corr, Philip and Matthews, Gerald},
	year = {2009},
	file = {PDF:/home/miguel/Zotero/storage/798H4EPL/2008_PersonalityPsychology.pdf:application/pdf},
}

@article{SURA2018b,
	title = {Tendencias de {Tecnología}},
	author = {{SURA}},
	year = {2018},
	file = {PDF:/home/miguel/Zotero/storage/W6YX6HVJ/Tecnología.pdf:application/pdf},
}

@article{Kahneman1991,
	title = {{JUDGMENT} {AND} {DECISION} {MAKING}: {A} {Personal} {View}},
	volume = {2},
	issn = {14679280},
	doi = {10.1111/j.1467-9280.1991.tb00121.x},
	abstract = {Focuses on the concept of judgement and decision making.  Development of the heuristic and biases approach; Characterization of the study of decision making and judgement; Samples of research in judgement and decision making.},
	number = {3},
	journal = {Psychological Science},
	author = {Kahneman, Daniel},
	year = {1991},
	pages = {142--145},
	file = {PDF:/home/miguel/Zotero/storage/HZWKEIEX/1991_Kahneman_A personal view.pdf:application/pdf},
}

@article{SURA2018a,
	title = {Tendencias del consumidor},
	author = {{SURA}},
	year = {2018},
	pages = {1--235},
	file = {PDF:/home/miguel/Zotero/storage/XU726JDT/Consumidor.pdf:application/pdf},
}

@article{SURA2018,
	title = {Megatendencias},
	url = {http://www.ghbook.ir/index.php?name=فرهنگ و رسانه های نوین&option=com_dbook&task=readonline&book_id=13650&page=73&chkhashk=ED9C9491B4&Itemid=218&lang=fa&tmpl=component},
	author = {{SURA}},
	year = {2018},
	file = {PDF:/home/miguel/Zotero/storage/WY33SDZI/Megatendencias.pdf:application/pdf},
}

@book{Holyoak1385,
	title = {The cambridge handbook of thinking and reasoning},
	isbn = {978-0-521-82417-0},
	url = {http://www.ghbook.ir/index.php?name=فرهنگ و رسانه های نوین&option=com_dbook&task=readonline&book_id=13650&page=73&chkhashk=ED9C9491B4&Itemid=218&lang=fa&tmpl=component},
	publisher = {Cambridge university Press},
	author = {Holyoak, Keith J. and Morrison, Robert G.},
	year = {1385},
	file = {PDF:/home/miguel/Zotero/storage/IDMGWYA9/2005_The Cambridge Handbook of Thinking and Reasoning.pdf:application/pdf},
}

@article{Diapouli2017,
	title = {Behavioural analytics using process mining in on-line advertising},
	volume = {2028},
	issn = {16130073},
	abstract = {Online behavioural targeting is one of the most popular business strategies on the display advertising today. It is based primarily on analysing web user behavioural data with the usage of machine learning techniques with the aim to optimise web advertising. Being able to identify "unknown" and "first time seen" customers is of high importance in online advertising since a successful guess could identify "possible prospects" who would be more likely to purchase an advertisement's product. By identifying prospective customers, online advertisers may be able to optimise campaign performance, maximise their revenue as well as deliver advertisements tailored to a variety of user interests. This work presents a hybrid approach benchmarking machine-learning algorithms and attribute preprocessing techniques in the context of behavioural targeting in process oriented environments. The performance of our suggested methodology is evaluated using the key performance metric in online advertising which is the predicted conversion rate. Our experimental results indicate that the presented process mining framework can significantly identify prospect customers in most cases. Our results seem promising, indicating that there is a need for further workflow research in online display advertising.},
	journal = {CEUR Workshop Proceedings},
	author = {Diapouli, Maria and Kapetanakis, Stelios and Petridis, Miltos and Evans, Roger},
	year = {2017},
	keywords = {Classification, Online display advertising, Process mining, Process oriented workflows},
	pages = {147--156},
	file = {PDF:/home/miguel/Zotero/storage/L94SGBF6/201X_DIAPOULI.pdf:application/pdf},
}

@article{Ahlquist2015,
	title = {Development of a digital framework for the computation of complex material and morphological behavior of biological and technological systems},
	volume = {60},
	issn = {00104485},
	url = {http://dx.doi.org/10.1016/j.cad.2014.01.013},
	doi = {10.1016/j.cad.2014.01.013},
	abstract = {Research in material behavior involves the study of relationships between material composition and capacities to negotiate internal and external pressures. Tuning material composition for performance allows for the integration of multifaceted functionality and embedded responsiveness within minimal material means. The relationships of material composition and system performance can be dissected into properties of topology (in count, type and association), forces (as the simulation of contextual pressures), and materiality (material properties and constraints of fabrication). When resourcing information about these aspects of material behavior from biological or technological systems, the physical precedents, as specimens and/or models, serve as the primary, and often sole, exemplar. While this is necessary to initiate the study of material make-up as it relates to specific morphological performance, there is an inherent limit when asking how and to what degree the knowledge resourced from that instance applies when alterations from the norm are generated. This research proposes the possibility for testing variants of a morphological system using physical models as the precedent while incorporating multiple means of computational analysis for extensive exploration. The framework begins with the initial stage of deducing principles, regarding material organization and behavior, through comparative physical and computational study. Subsequently, through methods of abduction, new vocabularies of form and potentials in performance are generated primarily through computational exploration. The framework is shaped by research into the design and materialization of complex pre stressed form- and bending-active architectures. A novel aspect of this framework is the development of a software environment called springFORM. In this environment, material behavior is simulated using basic springbased (particle system) methods. The novel contribution of this software is in providing means for both manual and algorithmic manipulations of mesh topologies and material properties during the formfinding process. A series of architectural prototypes, which range in scale, define rules for the relationship between topological-material complexity and the sequencing of particular exploratory methods. The studies define the value of the physical precedent as it engenders further material prototypes, springbased explorations and simulations with finite element analysis. These rules and methods are further elaborated upon through studying the particularly fascinating structural capacity of banana leaf stalks, a material system which is stiff in bending yet highly flexible in torsion. Of interest is a functional robustness which allows for the negotiation of both self-weight and wind loading for a large and fully integrated leaf structure. Methods of simulation and meta-heuristics are developed to address the continual material and topological differentiation of the banana leaf stalk. Case studies are based upon examination of specimens from the species Musa acuminata and Ensete ventricosum. Mechanical properties and geometric descriptions of isolated moments within the stalk provide the basis for computational comparison. Fundamental properties and behaviors are extracted from the plant specimens, yet a full description is not possible because of the plant's intricate spatial structure. In this case, the computational means serve to elucidate upon the behavior of the complete system as well as provide avenues for exploringits variants. This paper describes an extensible and calibrated framework which can foster enhanced biomimetic insights by explorations which are based upon but extend well beyond initial biological and/or technological precedents.},
	journal = {CAD Computer Aided Design},
	author = {Ahlquist, Sean and Kampowski, Tim and Torghabehi, Omid Oliyan and Menges, Achim and Speck, Thomas},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Biomimetic research, Computational design, Material behavior, Spring-based simulation},
	pages = {84--104},
	file = {PDF:/home/miguel/Zotero/storage/R2N7CSDC/2015_AHLQUIST.pdf:application/pdf},
}

@article{Zhang2016,
	title = {Situational driving anger, driving performance and allocation of visual attention},
	volume = {42},
	issn = {13698478},
	url = {http://dx.doi.org/10.1016/j.trf.2015.05.008},
	doi = {10.1016/j.trf.2015.05.008},
	abstract = {The effects of situational (state) driving anger on driving performance and allocation of driver visual attention were studied using a driving simulator experiment. A total of 24 licensed drivers, half being experienced and half novices, took part in this study. Each participant completed two similar drives, one in an emotion-neutral condition and one in an angry state. The anger emotion state was induced using a 5-min long traffic-related video clip. The results showed that compared with emotion-neutral drivers, drivers in an angry state tended to drive faster, maintain less headway while following a lead vehicle, and accept shorter gaps when performing left-turns. Moreover, when angry, drivers tended to adopt later and harder braking in the lane merging event, indicating a failure to respond properly to an imminent crash that fell into the peripheral areas of the road. Responses to emergency situations that happened in the centre areas of the road, however, were unaffected by situational anger. Results on eye movement data revealed that when angry, drivers scanned a narrower area and applied a more heuristic processing style, both of which may increase the chance of missing potential hazards in peripheral areas. Furthermore, it was found that increased experience did not better prepare drivers for the adverse influences of situational anger. Recommendations for intervention strategies and further research are presented.},
	journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
	author = {Zhang, Tingru and Chan, Alan H.S. and Ba, Yutao and Zhang, Wei},
	year = {2016},
	note = {Publisher: Elsevier Ltd},
	keywords = {Decreased fixation dispersion, Eye movements, Risky driving, Simulator, Situational anger},
	pages = {376--388},
	file = {PDF:/home/miguel/Zotero/storage/IMURJPXT/2016_ZHANG.pdf:application/pdf},
}

@article{Wieneke2016,
	title = {Privacy-related decision-making in the context of wearable use},
	volume = {67},
	url = {http://aisel.aisnet.org/pacis2016/67},
	journal = {PACIS 2016 Proceedings},
	author = {Wieneke, Alexander and Lehrer, Christiane and Zeder, Raphael and Jung, Reinhard and Wieneke, Alexander ; and Lehrer, Christiane ; and Zeder, Raphael ; and Reinhard, Jung},
	year = {2016},
	file = {PDF:/home/miguel/Zotero/storage/YWC3DT9V/2016_WINECKE.pdf:application/pdf},
}

@article{Frysak2018,
	title = {Digital gamified effort feedback mechanism to enhance information coverage in multi-attribute decision-making},
	volume = {2018-Janua},
	doi = {10.1109/IISA.2017.8316410},
	abstract = {In situations where information processing would exceed cognitive limitations or the decision is not considered important enough to justify higher effort investments, people apply heuristics to reduce the effort to invest in processing information at the cost of an increased error rate. While DSS are designed to extend the cognitive limits of humans, they are based on mathematical models hard to understand by non-experts. We investigate feedback mechanisms as viable solution for both problems. This paper reports on preliminary results of an experiment with 166 participants testing two versions of a gamified effort feedback mechanism to convince human decision-makers to invest more effort on covering more unique information elements in multi-attribute decision-making (MADM) tasks. Results indicate our gamified effort feedback mechanism provided during the decision task to be effective to increase information coverage.},
	journal = {2017 8th International Conference on Information, Intelligence, Systems and Applications, IISA 2017},
	author = {Frysak, Josef and Bernroider, Edward W.N.},
	year = {2018},
	note = {ISBN: 9781538637319},
	keywords = {DSS, Feedback, Gamification, Information coverage, MADM},
	pages = {1--6},
	file = {PDF:/home/miguel/Zotero/storage/LJ3X4KUC/201X_FRYSAK.pdf:application/pdf},
}

@article{Hossain2019,
	title = {Energy consumption reduction of {Bangladesh} using nudge via smart grid: {An} empirical approach},
	doi = {10.1109/ICREST.2019.8644515},
	abstract = {A smart grid is a digitally intertwined electrical network which delivers electricity to consumers through a multidirectional controlling system. Such systems allow analysis, monitoring and communication within the power transmission network to increase efficiency, minimize energy consumption and maximize the use of power used between production to consumption. The present power transmission system in Bangladesh is piled atop very old and rickety system, Bangladesh suffers from a massive load-shedding problem based on multifaceted issues. Alongside improving on power generation capacity and moving towards a smart grid system, the country needs to consider and move towards an energy efficient culture using the smart grid systems in order to further reduce power shortage problem. This paper will assess the effectiveness of behavioral science and nudge theory in developing an energy-conscious culture to save energy waste. The many examples of energy efficiency behaviors implemented around the world will be discussed in detail trying to find an approach appropriate for the country. The intent of this paper is to draw the attention of policymakers, engineering researchers and professionals towards such passive and low-cost approaches that has high potential to become successful on a microeconomic and macroeconomic scale.},
	journal = {1st International Conference on Robotics, Electrical and Signal Processing Techniques, ICREST 2019},
	author = {Hossain, Md Imran and Islam, S. M.Raisul},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538680124},
	keywords = {Energy efficiency behavior, Energy policy, Load shedding, Nudge theory, Smart grid},
	pages = {7--12},
	file = {PDF:/home/miguel/Zotero/storage/L3YWWFQQ/2019_HOSSAIN.pdf:application/pdf},
}

@article{Perko2017,
	title = {Behaviour-based short-term invoice probability of default evaluation},
	volume = {257},
	issn = {03772217},
	doi = {10.1016/j.ejor.2016.08.039},
	abstract = {In this paper, the effect of behavioural analytics on short-term default predictions at the invoice level is addressed by answering a question that slightly diverges from the traditional probability of default definition: ‘What is the probability that this invoice will be paid within the next 30 days?’ Resultantly improving short-term liquidity planning accuracy and supporting financial management in companies. To provide a valid answer to the research question, a set of issues needs to be resolved, including identifying an appropriate data set, increasing the data predictive power, and creating and testing predictive models. Since the appropriate data set is not yet presented, we primarily focus on the first two issues: identifying appropriate data and raising its predictive power. In this paper, we propose to build predictive models upon a new data source from multiple companies, acquired by business partners' data sharing concept. Furthermore, we upgrade these data with behavioural analysis to test the assumption that the probability of default depends not only on payment capability but also on payment preparedness. The predictive power of shared invoice data and the effects of behavioural analysis are tested in a two-phase experiment: first, basic shared data are used to predict short-term invoice defaults, and in the second phase, the behavioural analysis results are included in the dataset. Lastly, the predictive models’ test results are compared. Both results are positive: the already high accuracy of models, build upon basic data is significantly upgraded in models, using the behaviour analysis extended data set.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Perko, Igor},
	year = {2017},
	keywords = {Behavioural analytics, Credit risk, Data sharing, Probability of default, Strategic default},
	pages = {1045--1054},
	file = {PDF:/home/miguel/Zotero/storage/X6E84K28/2016_PERKO.pdf:application/pdf},
}

@article{Okeke2018,
	title = {Towards a framework for mobile behavior change research},
	doi = {10.1145/3183654.3183706},
	abstract = {Behavior change is one of the most important problems faced by people and researchers today. Behavioral researchers have begun adopting smartphones as data-collection tools in psychological and behavioral science because these devices can study people in their everyday life, objectively measure behavior (using mobile sensing), and implement interventions. From a literature review of recent research on mobile behavior change, we identify three design components: mobile sensing, user contexts, and digital nudges. Informed by these components, we designed three example mobile research applications and propose a solution-focused, conceptual framework for deploying behavior change studies using mobile phones. We discuss future directions for research in psychological and behavioral science as these fields embrace mobile technology.},
	journal = {ACM International Conference Proceeding Series},
	author = {Okeke, Fabian and Sobolev, Michael and Estrin, Deborah},
	year = {2018},
	note = {ISBN: 9781450354202},
	keywords = {Digital Nudge, Behavior Change, Behavioral Science, Context, Framework, Mobile Phone, Mobile Sensing},
	pages = {1--6},
	file = {PDF:/home/miguel/Zotero/storage/KA4582GS/2017_OKEKE.pdf:application/pdf},
}

@book{Meske2019,
	title = {The {Potential} {Role} of {Digital} {Nudging} in the {Digital} {Transformation} of the {Healthcare} {Industry}},
	isbn = {978-3-030-23537-6},
	url = {http://dx.doi.org/10.1007/978-3-030-23538-3_25},
	abstract = {New information technology has led to an ongoing transformation of the healthcare industry, supporting caregivers and caretakers. In some cases, such technologies may not be used in practice as intended by those who designed or implemented them. In other cases, the full potential of such technologies in terms of guiding user behavior has not been exhausted to the fullest. This is where “digital nudging” can help to overcome according issues. Nudging was invented in behavioral economics and aims at eliciting behavior that is beneficial for the individual, at the same time respecting the individual’s own preferences and freedom of choice. Digital nudging can therefore help to guide user behavior in information systems. In this work, we investigate the potentials of this concept in hospitals. We come to the conclusion that digital nudging in hospitals can positively influence the use of technology, new value creation, the change of structures and consequently financial dimensions of digital transformation, supporting not only caregivers but also caretakers. © 2019, Springer Nature Switzerland AG.},
	publisher = {Springer International Publishing},
	author = {Meske, Christian and Amojo, Ireti and Poncette, Akira-Sebastian and Balzer, Felix},
	year = {2019},
	doi = {10.1007/978-3-030-23538-3_25},
	note = {Issue: July
ISSN: 16113349},
	keywords = {Digital nudging,Healthcare,Hospital,User behavior,},
	file = {PDF:/home/miguel/Zotero/storage/X7CRDKH6/2019_DIGITAL NUDGING.pdf:application/pdf},
}

@article{Andion2018,
	title = {Smart {Behavioral} {Analytics} over a {Low}-{Cost} {IoT} {Wi}-{Fi} {Tracking} {Real} {Deployment}},
	volume = {2018},
	issn = {15308677},
	doi = {10.1155/2018/3136471},
	abstract = {In a more and more urbanized World, the so-called Smart Cities need to be driven by the principles of efficiency and sustainability. Information and Communications Technologies and, in particular, the Internet of Things will play a key role on this, since they will allow monitoring and optimizing all the municipal services that exist and shall exist. People flow monitoring stands out in this context due to its wide range of applications, spanning from monitoring transport infrastructure to physical security applications. There are different techniques to perform people flow monitoring, presenting pros and cons, as in any other engineering problem. Typically, the options that provide the most accurate results are also the most expensive ones, whereas there are cases where presence detection in given areas is enough and cost is a limiting factor. The main goal of this paper is to prove that a minimal deployment of sensors, combined with the adequate analysis and visualization algorithms, can render useful results. In order to achieve this goal, a dataset is used with 1-year data from a real infrastructure composed of 9 Wi-Fi tracking sensors deployed in the Telecommunications Engineering School of Universidad Politécnica de Madrid, which is visited by 4000 people daily and covers 1.8 hectares. The data analysis includes time and occupancy, position of people, and identification of common behaviors, as well as a comparison of the accuracy of the considered solution with actual data and a video monitoring system available at the library of the school. The obtained insights can be used for optimizing the management and operation of the school, as well as for other similar infrastructures and, in general, for other kind of applications which require not very accurate people flow monitoring at low cost.},
	journal = {Wireless Communications and Mobile Computing},
	author = {Andión, Javier and Navarro, José M. and López, Gregorio and Álvarez-Campana, Manuel and Dueñas, Juan C.},
	year = {2018},
	file = {PDF:/home/miguel/Zotero/storage/BTCQ33PH/2018_ANDIÓN.pdf:application/pdf},
}

@article{Dogan2018,
	title = {In-store behavioral analytics technology selection using fuzzy decision making},
	volume = {31},
	issn = {17410398},
	doi = {10.1108/JEIM-02-2018-0035},
	abstract = {Purpose: With the emerging technologies, collecting and processing data about the behaviors of customers or employees in a specific location has become possible. The purpose of this paper is to evaluate existing data collection technologies. Design/methodology/approach: Technology evaluation problem is handled as a multi-criteria decision-making (MCDM) problem. In this manner, a decision model containing four criteria and eight sub-criteria and four alternatives are formed. The problem is solved using hesitant analytic hierarchy process (AHP) and trapezoidal fuzzy numbers (TrFN). Findings: The results show that the most important sub-criteria are: accuracy, quantity, ıntrospective and cost. Decision makers’ evaluate for alternatives, namely wireless fidelity (WiFi), camera, radio-frequency identification and Bluetooth. The best alternative is found as Bluetooth which is followed by WiFi and Camera. Research limitations/implications: Technology evaluation problem, just like many other MCDM problems are solved using expert evaluations. Thus, the generalizability of the findings is low. Originality/value: In this paper, technology selection problem has been handled using hesitant AHP for the first time. In addition, the original methodology is extended by using TrFN to represent the expert evaluations in a better way.},
	number = {4},
	journal = {Journal of Enterprise Information Management},
	author = {Dogan, Onur and Öztaysi, Basar},
	year = {2018},
	keywords = {Behavioural analytics, Analytic hierarchy process, Hesitant fuzzy sets, Technology selection},
	pages = {612--630},
	file = {PDF:/home/miguel/Zotero/storage/6M6RPV7N/2018_DOGAN.pdf:application/pdf},
}

@article{Khan2018,
	title = {Game theoretical demand response management and short-term load forecasting by knowledge based systems on the basis of priority index},
	volume = {7},
	issn = {20799292},
	doi = {10.3390/electronics7120431},
	abstract = {Demand Response Management (DRM) is considered one of the crucial aspects of the smart grid as it helps to lessen the production cost of electricity and utility bills. DRM becomes a fascinating research area when numerous utility companies are involved and their announced prices reflect consumer’s behavior. This paper discusses a Stackelberg game plan between consumers and utility companies for efficient energy management. For this purpose, analytical consequences (unique solution) for the Stackelberg equilibrium are derived. Besides this, this paper presents a distributed algorithm which converges for consumers and utilities. Moreover, different power consumption activities on the basis of time series are becoming a basic need for load prediction in smart grid. Load forecasting is taken as the significant concerns in the power systems and energy management with growing technology. The better precision of load forecasting minimizes the operational costs and enhances the scheduling of the power system. The literature has discussed different techniques for demand load forecasting like neural networks, fuzzy methods, Naïve Bayes, and regression based techniques. This paper presents a novel knowledge based system for short-term load forecasting. The algorithms of Affinity Propagation and Binary Firefly Algorithm are integrated in knowledge based system. Besides, the proposed system has minimum operational time as compared to other techniques used in the paper. Moreover, the precision of the proposed model is improved by a different priority index to select similar days. The similarity in climate and date proximity are considered all together in this index. Furthermore, the whole system is distributed in sub-systems (regions) to measure the consequences of temperature. Additionally, the predicted load of the entire system is evaluated by the combination of all predicted outcomes from all regions. The paper employs the proposed knowledge based system on real time data. The proposed scheme is compared with Deep Belief Network and Fuzzy Local Linear Model Tree in terms of accuracy and operational cost. In addition, the presented system outperforms other techniques used in the paper and also decreases the Mean Absolute Percentage Error (MAPE) on a yearly basis. Furthermore, the novel knowledge based system gives more efficient outcomes for demand load forecasting.},
	number = {12},
	journal = {Electronics (Switzerland)},
	author = {Khan, Mahnoor and Javaid, Nadeem and {Sajjad} and {Abdullah} and Naseem, Adnan and Ahmed, Salman and Riaz, Muhammad Sajid and Akbar, Mariam and Ilahi, Manzoor},
	year = {2018},
	keywords = {Behavioral analytics, Date proximity, Demand response, Knowledge based systems, Priority index, Similar day, Stackelberg game},
	file = {PDF:/home/miguel/Zotero/storage/DTSXSDSR/2018_KHAN.pdf:application/pdf},
}

@article{Pang2014,
	title = {Human behavioral analytics system for video surveillance},
	doi = {10.1109/ICCSCE.2014.7072683},
	abstract = {Video surveillance system is an important system to monitor safety and security for public place. Unfortunately, most of them do not have additional. In this paper, we have proposed an algorithm for detect human aggressive behavior by using Microsoft Kinect sensor. In our proposed algorithm, Cartesian coordinate formula is used for calculate position of body joints detected by Microsoft Kinect sensor. By analysis position of body joints, we can differentiate whether it is a normal or aggressive behavior. Experiment results show that our proposed method has an average accuracy of 95.83\% for punching pose, and almost perfectly detected for kicking and normal behavior.},
	number = {November},
	journal = {Proceedings - 4th IEEE International Conference on Control System, Computing and Engineering, ICCSCE 2014},
	author = {Pang, Junn Min and Yap, Vooi Voon and Soh, Chit Siang},
	year = {2014},
	note = {Publisher: IEEE
ISBN: 9781479956869},
	keywords = {fall detection, human aggressive behavior analysis, Human basic behavior analysis, kinect},
	pages = {23--28},
	file = {PDF:/home/miguel/Zotero/storage/7MD8ZJAC/2014_PANG.pdf:application/pdf},
}

@article{Meliones2018,
	title = {Developing video games with elementary adaptive artificial intelligence in unity: {An} intelligent systems approach},
	volume = {2018-Janua},
	doi = {10.1109/IntelliSys.2017.8324230},
	abstract = {Video games have increasingly demonstrated a great deal of audiovisual realism, in par with the massive performance improvement of computer systems. At the same time, their Artificial Intelligence (AI) component falls short in terms of realism because it is usually based on non-adaptive methods. Adaptive AI mechanisms can help increase video game realism allowing the game to adapt in real-time to the game progress and the user behavior. Following a short overview of the progress of AI in video games in the past years, this paper highlights the creation of modern video games with basic and elementary adaptive game AI using the Unity game development framework. Particular emphasis is on the details of the AI component. First, a shooter game with basic AI is created. Finally, an action-adventure video game is created featuring elementary case-based adaptive AI. The objective in this game is to create enemies which are able to perceive changes in the environment and adapt their strategies accordingly. Proposed AI practices can migrate into relevant real world applications, such as video surveillance and intrusion detection systems, mission critical autonomous networked patrolling and/or save and rescue robots, vision and hearing assistive applications, intelligent video and behavioral analytics to detect and predict threats etc.},
	number = {September},
	journal = {2017 Intelligent Systems Conference, IntelliSys 2017},
	author = {Meliones, Apostolos and Plas, Ioannis},
	year = {2018},
	note = {ISBN: 9781509064359},
	keywords = {adaptive game AI, case-based adaptive game AI, game AI, intrusion detection, networked patrolling robots, Unity},
	pages = {104--111},
	file = {PDF:/home/miguel/Zotero/storage/T6QFPCKU/2017_MELIONES.pdf:application/pdf},
}

@article{Wlodarczak2015,
	title = {Behavioural health analytics using mobile phones},
	volume = {2},
	doi = {10.4108/sis.2.5.e6},
	abstract = {Big Data analytics in healthcare has become a very active area of research since it promises to reduce costs and to improve health care quality. Behavioural analytics analyses a patients behavioural patterns with the goal of early detection if a patient becomes symptomatic and triggering treatment even before a disease outbreak happens. Behavioural analytics allows a more precise and personalised treatment and can even monitor whole populations for events such as epidemic outbreaks. With the prevalence of mobile phones, they have been used to monitor the health of patients by analysing their behavioural and movement patterns. Cell phones are always on devices and are usually close to their users. As such they can be used as social sensors to create "automated diaries" of their users. Specialised apps passively collect and analyse user data to detect if a patient shows some deviant behaviour indicating he has become symptomatic. These apps first learn a patients normal daily patterns and alert a health care centre if it detects a deviant behaviour. The health care centre can then call the patient and check on his well-being. These apps use machine learning techniques to for reality mining and predictive analysis. This paper describes some of these techniques that have been adopted recently in eHealth apps.},
	number = {5},
	journal = {ICST Transactions on Scalable Information Systems){\textless}/},
	author = {Wlodarczak, P. and Soar, P. and Ally, M.},
	year = {2015},
	keywords = {machine learning, 0, 2015, 2015 p, 3, accepted on 30 june, access article distributed under, behavioural health analytics, big data, by, commons attribution licence, copyright, creative, creativecommons, distribution and, ehealth, http, licensed to icst, licenses, mobile sensing, org, predictive analytics, published on 02 july, reality mining, received on 30 june, the terms of the, this is an open, which permits unlimited use, wlodarczak et al},
	pages = {e6},
	file = {PDF:/home/miguel/Zotero/storage/FDXFXNKP/2015_WLODARCZAK.pdf:application/pdf},
}

@article{Ellis2019,
	title = {Are smartphones really that bad? {Improving} the psychological measurement of technology-related behaviors},
	volume = {97},
	issn = {07475632},
	doi = {10.1016/j.chb.2019.03.006},
	abstract = {Understanding how people use technology remains important, particularly when measuring the impact this might have on individuals and society. To date, research within psychological science often frames new technology as problematic with overwhelmingly negative consequences. However, this paper argues that the latest generation of psychometric tools, which aim to assess smartphone usage, are unable to capture technology related experiences or behaviors. As a result, many conclusions concerning the psychological impact of technology use remain unsound. Current assessments have also failed to keep pace with new methodological developments and these data-intensive approaches challenge the notion that smartphones and related technologies are inherently problematic. The field should now consider how it might re-position itself conceptually and methodologically given that many ‘addictive’ technologies have long since become intertwined with daily life.},
	journal = {Computers in Human Behavior},
	author = {Ellis, David A.},
	year = {2019},
	keywords = {Smartphones, Behavioral analytics, Psychometrics, Technology use},
	pages = {60--66},
	file = {PDF:/home/miguel/Zotero/storage/C22UX8IW/2019_ELLIS.pdf:application/pdf},
}

@article{Grissom2019,
	title = {Behavioral {Sciences} of {Terrorism} and {Political} {Aggression} {Decision}-theoretic behavioral analytics : risk management and terrorist intensity},
	volume = {4472},
	doi = {10.1080/19434472.2018.1551917},
	author = {Grissom, Terry V and Mcilhatton, David and Delisle, James R and Hardy, Mike and Grissom, Terry V and Mcilhatton, David and Delisle, James R},
	year = {2019},
	note = {Publisher: Taylor \& Francis},
	file = {PDF:/home/miguel/Zotero/storage/6DQBZ82D/2019_HARDY.pdf:application/pdf},
}

@article{Subramani2017,
	title = {{EAI} {Endorsed} {Transactions} {Preprint} {Extracting} {Actionable} {Knowledge} from {Domestic} {Violence} {Discourses} on {Social} {Media}},
	doi = {10.4108/XX.X.X.XX},
	abstract = {Domestic Violence (DV) is considered as big social issue and there exists a strong relationship between DV and health impacts of the public. Existing research studies have focused on social media to track and analyse real world events like emerging trends, natural disasters, user sentiment analysis, political opinions, and health care. However there is less attention given on social welfare issues like DV and its impact on public health. Recently, the victims of DV turned to social media platforms to express their feelings in the form of posts and seek the social and emotional support, for sympathetic encouragement, to show compassion and empathy among public. But, it is difficult to mine the actionable knowledge from large conversational datasets from social media due to the characteristics of high dimensions, short, noisy, huge volume, high velocity, and so on. Hence, this paper will propose a novel framework to model and discover the various themes related to DV from the public domain. The proposed framework would possibly provide unprecedentedly valuable information to the public health researchers, national family health organizations, government and public with data enrichment and consolidation to improve the social welfare of the community. Thus provides actionable knowledge by monitoring and analysing continuous and rich user generated content.},
	author = {Subramani, Sudha and O'connor, Manjula},
	year = {2017},
	note = {arXiv: 1807.02391v1},
	keywords = {Actionable knowledge, Domestic Violence, MapReduce, Pattern Mining, Topic Model},
	pages = {1--11},
	file = {PDF:/home/miguel/Zotero/storage/JZYI49LW/2018_SARKER.pdf:application/pdf},
}

@article{Shah2019,
	title = {Compromised user credentials detection in a digital enterprise using behavioral analytics},
	volume = {93},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2018.09.064},
	doi = {10.1016/j.future.2018.09.064},
	abstract = {In today's digital age, the digital transformation is necessary for almost every competitive enterprise in terms of having access to the best resources and ensuring customer satisfaction. However, due to such rewards, these enterprises are facing key concerns around the risk of next-generation data security or cybercrime which is continually increasing issue due to the digital transformation four essential pillars—cloud computing, big data analytics, social and mobile computing. Data transformation-driven enterprises should ready to handle this next-generation data security problem, in particular, the compromised user credential (CUC). When an intruder or cybercriminal develops trust relationships as a legitimate account holder and then gain privileged access to the system for misuse. Many state-of-the-art risk mitigation tools are being developed, such as encrypted and secure password policy, authentication, and authorization mechanism. However, the CUC has become more complex and increasingly critical to the digital transformation process of the enterprise's database by a cybercriminal, we propose a novel technique that effectively detects CUC at the enterprise-level. The proposed technique is learning from the user's behavior and builds a knowledge base system (KBS) which observe changes in the user's operational behavior. For that reason, a series of experiments were carried out on the dataset that collected from a sensitive database. All empirical results are validated through well-known evaluation measures, such as (i) accuracy, (ii) sensitivity, (iii) specificity, (iv) prudence accuracy, (v) precision, (vi) f-measure, and (vii) error rate. The experiments show that the proposed approach obtained weighted accuracy up to 99\% and overall error of about 1\%. The results clearly demonstrate that the proposed model efficiently can detect CUC which may keep an organization safe from major damage in data through cyber-attacks.},
	journal = {Future Generation Computer Systems},
	author = {Shah, Saleh and Shah, Babar and Amin, Adnan and Al-Obeidat, Feras and Chow, Francis and Moreira, Fernando Joaquim Lopes and Anwar, Sajid},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cluster-level pattern, Compromised activities detection, Compromised user detection, Knowledge-base system, Prudence analysis},
	pages = {407--417},
	file = {PDF:/home/miguel/Zotero/storage/PJKRXELL/2018_SHAH.pdf:application/pdf},
}

@article{Zafeiropoulos2018,
	title = {Detaching the design, development and execution of big data analysis processes: {A} case study based on energy and behavioral analytics},
	doi = {10.1109/GIOTS.2018.8534525},
	abstract = {Numerous tools and approaches are evolving towards the support of data mining and analysis processes, focusing on part or the overall lifecycle of such processes. In parallel, penetration of data analytics tools in the market is continuously increasing, along with their adoption by various stakeholders, including data scientists, decision and policy makers and business analysts. However, given the wide diversity in the needs for realizing an analysis and the level of expertise of the various stakeholders, there is a need for design and implementation of analysis toolkits that can support part or the overall lifecycle of an analysis process, without imposing dependencies on the type of tool or technology to be used. In the current manuscript, an approach for detaching the design, development and execution of big data analysis processes is detailed, focusing on the realization of energy and behavioral analytics, targeted to supporting the increase of energy efficiency in smart buildings through behavioral change of the citizens. The overall architectural approach as well as the set of energy and behavioral analysis processes integrated are detailed.},
	journal = {2018 Global Internet of Things Summit, GIoTS 2018},
	author = {Zafeiropoulos, Anastasios and Fotopoulou, Eleni and Gonzalez-Vidal, Aurora and Skarmeta, Antonio},
	year = {2018},
	note = {ISBN: 9781538664513},
	keywords = {behavioral analytics, Data analytics, energy analytics, Open APIs, OpenCPU},
	file = {PDF:/home/miguel/Zotero/storage/RYBV3EH2/2018_ZAFEIROPOULOS.pdf:application/pdf},
}

@article{Duarte2016,
	title = {Ordering matters: {An} experimental study of ranking influence on results selection behavior during {Exploratory} {Search}},
	volume = {2},
	doi = {10.5220/0005828704270434},
	abstract = {The design of Exploratory Search tools acquires more importance as the amount of information on the Web grows. Accordingly, informed design decisions concerning the users' behavior during search activities can be used in novel approaches for Exploratory Search tools. With regard to users' behavior on search result selection, literature indicates higher ranked results tend to attract more attention and, therefore, more hits. However, the cause of such behavior is not clear. We experimentally investigate the hypothesis of this behavior being due to ranking. A group of 72 participants was asked to select a search result from a randomly ordered results list. The experiment was carried out on paper to remove specific search engine and digital medium biases. We obtained evidence indicating there is a trend towards choosing higher ranked results even in a different medium and in the context of an Exploratory Search, corroborating to the hypothesis that ranking has influence on users' selection behavior.},
	number = {Iceis},
	journal = {ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems},
	author = {Duarte, Emanuel Felipe and Nanni, Lucas Pupulin and Geraldi, Ricardo Theis and Oliveira, Edson and Feltrim, Valéria Delisandra and Pereira, Roberto},
	year = {2016},
	note = {ISBN: 9789897581878},
	keywords = {Exploratory Search, Ranking influence, Relevance judgement, Search user interfaces},
	pages = {427--434},
	file = {PDF:/home/miguel/Zotero/storage/9D23H5WW/2016_DUARTE.pdf:application/pdf},
}

@article{Mahapatra2016,
	title = {{LMS} weds {WhatsApp}: {Bridging} digital divide using {MIMs}},
	doi = {10.1145/2899475.2899485},
	abstract = {Recently, Mobile Instant Messaging Services (MIMs) such as WhatsApp have shown tremendous potential in enabling communication among diverse set of people. Such services have an even more critical role to play in developing regions. Due to the digital divide, a much higher prevalence of mobile-only internet connection has been reported, where millions of users leapfrogged to mobile-internet entirely skipping the desktop-based internet phase. In this paper, we report findings from a longitudinal field study conducted in a private higher education institution in India. The aim of the study was to explore the potential of an integrated blended learning setup which combines WhatsApp with a Learning Management System (LMS). The study was performed in a class of 20 final year engineering students over a period of three months. Our findings suggest that there is a systematic bias in the usage of WhatsApp vs. LMS based on several factors, including specifics of the learning activity, student behavior, and status of the course in the semester, and the time of the day. Synchronous and asynchronous interaction on WhatsApp are perceived to be engaging, support collaboration and aid learning by complementing LMS-based and face-to-face learning.},
	journal = {W4A 2016 - 13th Web for All Conference},
	author = {Mahapatra, Jyotirmaya and Srivastava, Saurabh and Yadav, Kuldeep and Shrivastava, Kundan and Deshmukh, Om},
	year = {2016},
	note = {ISBN: 9781450341387},
	keywords = {Accessible learning management, Blended learning, Education, Instant messaging services, Learning management system},
	file = {PDF:/home/miguel/Zotero/storage/J72DARWI/2016_MAHAPATRA.pdf:application/pdf},
}

@article{Esposito2017,
	title = {Nudging to prevent the purchase of incompatible digital products online: {An} experimental study},
	volume = {12},
	issn = {19326203},
	doi = {10.1371/journal.pone.0173333},
	abstract = {Ensuring safe and satisfactory online shopping activity, especially among vulnerable consumers such as elderly and less educated citizens, is part of a larger set of consumer policy objectives seeking to strengthen trust in the electronic marketplace. This article contributes to that goal by testing the effectiveness of nudges intended to prevent the purchase of 'incompatible' digital products (i.e., those which cannot be used with the devices owned by consumers or the systems they operate). We ran a computerised lab experiment (n = 626) examining three types of nudges, the effects of age and education, and interaction effects between these variables and the nudges. Results show that emotive warning messages and placing incompatibility information at the checkout page rather than earlier in the purchasing process were effective in reducing the purchase of incompatible goods. Age was also a relevant factor: older participants were more likely to purchase incompatible goods. In addition, there was an interaction effect between all nudges and age: two nudges exacerbated the effect of age, while another mitigated it. These results suggest nudges can be an effective policy tool, confirm a generational gap in online behaviour, and highlight how nudges can moderate the effect of socio-demographic variables.},
	number = {3},
	journal = {PLoS ONE},
	author = {Esposito, Gabriele and Hernández, Penélope and Van Bavel, René and Vila, José},
	year = {2017},
	note = {ISBN: 1111111111},
	pages = {1--15},
	file = {PDF:/home/miguel/Zotero/storage/MCXVJGLE/2017_ESPOSITO.pdf:application/pdf},
}

@article{McNeill2017,
	title = {Estimating local commuting patterns from geolocated {Twitter} data},
	volume = {6},
	issn = {21931127},
	url = {http://dx.doi.org/10.1140/epjds/s13688-017-0120-x},
	doi = {10.1140/epjds/s13688-017-0120-x},
	abstract = {The emergence of large stores of transactional data generated by increasing use of digital devices presents a huge opportunity for policymakers to improve their knowledge of the local environment and thus make more informed and better decisions. A research frontier is hence emerging which involves exploring the type of measures that can be drawn from data stores such as mobile phone logs, Internet searches and contributions to social media platforms and the extent to which these measures are accurate reflections of the wider population. This paper contributes to this research frontier, by exploring the extent to which local commuting patterns can be estimated from data drawn from Twitter. It makes three contributions in particular. First, it shows that heuristics applied to geolocated Twitter data offer a good proxy for local commuting patterns; one which outperforms the current best method for estimating these patterns (the radiation model). This finding is of particular significance because we make use of relatively coarse geolocation data (at the city level) and use simple heuristics based on frequency counts. Second, it investigates sources of error in the proxy measure, showing that the model performs better on short trips with higher volumes of commuters; it also looks at demographic biases but finds that, surprisingly, measurements are not significantly affected by the fact that the demographic makeup of Twitter users differs significantly from the population as a whole. Finally, it looks at potential ways of going beyond simple frequency heuristics by incorporating temporal information into models.},
	number = {1},
	journal = {EPJ Data Science},
	author = {McNeill, Graham and Bright, Jonathan and Hale, Scott A.},
	year = {2017},
	note = {arXiv: 1612.01785
Publisher: The Author(s)},
	keywords = {geolocation, mobility, social media, transport},
	file = {PDF:/home/miguel/Zotero/storage/TPCY3KZS/2017_MCNEILL.pdf:application/pdf},
}

@article{Hsieh2015,
	title = {Ranking {Online} {Customer} {Reviews} with the {SVR} {Model}},
	doi = {10.1109/IRI.2015.88},
	abstract = {On the online E-Commerce platform, customer reviews provides valuable opinions and relevant content, which will affect the perches behavior of other customers. Since the amount of online review grow fast, it is hard to read them all, therefore, a system that can find the reviews with better quality is necessary. In order to better understand the quality of reviews. In this paper, we proposed a system that can rank the reviews based on a set of linguistic features and a Support vector regression (SVR) model as a scorer. To evaluate our system, we collect 3730 Chinese reviews in eight product categories (books, digital cameras, tablet PC, backpacks, movies, men shoes, toys and cell phones) from Amazon.cn with the voting result of whether the review is helpful or not. Since the voting result might be biased by voting time and total voting number. We defined 4 types of evaluation index and compare the regression result to each index.},
	journal = {Proceedings - 2015 IEEE 16th International Conference on Information Reuse and Integration, IRI 2015},
	author = {Hsieh, Hsien You and Wu, Shih Hung},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781467366564},
	keywords = {Customer review, Linguistic features, Online e-commerce platform, Support Vector Regression, Text mining},
	pages = {550--555},
	file = {PDF:/home/miguel/Zotero/storage/V3BMTVHA/2015_HSIEH.pdf:application/pdf},
}

@article{Cho2015,
	title = {Perceptions of social norms surrounding digital piracy: {The} effect of social projection and communication exposure on injunctive and descriptive social norms},
	volume = {48},
	issn = {07475632},
	url = {http://dx.doi.org/10.1016/j.chb.2015.02.018},
	doi = {10.1016/j.chb.2015.02.018},
	abstract = {Using a national sample of 620 Internet users in the US, this study examined the extent to which social projection, communication exposure, and an interaction between the two, influenced individuals' perceptions about two subordinate types of social norms surrounding digital piracy: injunctive norms and descriptive norms. In line with the social projection model, individuals made social estimates about others' piracy attitudes and behaviors anchoring on their own personal attitudes and behavior. However, frequent communication exposure reduced the degree to which they relied on this egocentric thought process. In addition, the two-way interaction was contingent on another condition (perceiver's own piracy behavior) indicating that communication exposure had differing implications for pirates and non-pirates. Theoretical and practical implications are discussed.},
	journal = {Computers in Human Behavior},
	author = {Cho, Hichang and Chung, Siyoung and Filippova, Anna},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Descriptive norms, Digital piracy, Illegal downloading, Injunctive norms, Social norms perceptions, Social projection},
	pages = {506--515},
	file = {PDF:/home/miguel/Zotero/storage/5ZY6MLMU/2015_CHO.pdf:application/pdf},
}

@article{Otterbacher2015,
	title = {Crowdsourcing stereotypes: {Linguistic} bias in metadata generated via {GWAP}},
	volume = {2015-April},
	doi = {10.1145/2702123.2702151},
	abstract = {Games with a Purpose (GWAP) is a popular approach for metadata creation, enabling institutions to collect descriptions of digital artifacts on a mass scale. Creating metadata is challenging not only because one must recognize the artifact; the description must then be encoded into natural language. Language behaviors are influenced by many social factors, particularly when we are asked to describe other people. We consider labels for images of people generated via the ESP Game. While ESP has been shown to produce relevant labels, critics claim they are obvious and stereotypical. Based on theories of linguistic biases, we examine whether there are systematic differences in the ways players describe images of men versus women. Our first analysis considers images of people generally, and reveals a tendency for women to be described with subjective adjectives. A second analysis compares images depicting men and women within each of six occupational roles. Images of women receive more labels related to appearance, whereas those depicting men receive more occupation-related labels. Our work exposes the presence of gender-based stereotypes through linguistic biases, illustrates the forms in which they manifest, and raises important implications for those who design systems or train algorithms using data produced via GWAP.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Otterbacher, Jahna},
	year = {2015},
	note = {ISBN: 9781450331456},
	keywords = {Crowdsourcing, Games with a purpose, Gender, Linguistic bias, Metadata, Stereotypes, Subjective language},
	pages = {1955--1964},
	file = {PDF:/home/miguel/Zotero/storage/H59NSPUH/2015_OTTERBACHER.pdf:application/pdf},
}

@article{Angel2018,
	title = {Smart tools? {A} randomized controlled trial on the impact of three different media tools on personal finance},
	volume = {74},
	issn = {22148051},
	url = {https://doi.org/10.1016/j.socec.2018.04.002},
	doi = {10.1016/j.socec.2018.04.002},
	abstract = {By using a randomized controlled trial we test the impact of three treatments on financial literacy (knowledge, attitudes, reported behavior) among adolescents in Austria. Treatments comprise a documentary movie on debt, an internet exercise and a budgeting smartphone app. It is investigated if particularly the latter two instruments could serve as standalone alternatives (nudges) to conventional teaching interventions. Users of the budgeting app report to check their current account balance significantly more often than the control group. The web exercise, however, neither raises interest in personal finance issues nor significantly increases basic financial knowledge. The documentary movie did not affect attitudes towards saving or private credits. Even if it is only about raising awareness and interest in financial matters among adolescents, the ICT applications tested in this study thus do not lend strong support to policies solely relying on digital “stand-alone” solutions of similar design.},
	number = {April},
	journal = {Journal of Behavioral and Experimental Economics},
	author = {Angel, Stefan},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {104--111},
	file = {PDF:/home/miguel/Zotero/storage/SS77IW8N/2018_ANGEL.pdf:application/pdf},
}

@article{Shen2017,
	title = {Numerical {Nudging}: {Using} an {Accelerating} {Score} to {Enhance} {Performance}},
	volume = {28},
	issn = {14679280},
	doi = {10.1177/0956797617700497},
	abstract = {People often encounter inherently meaningless numbers, such as scores in health apps or video games, that increase as they take actions. This research explored how the pattern of change in such numbers influences performance. We found that the key factor is acceleration—namely, whether the number increases at an increasing velocity. Six experiments in both the lab and the field showed that people performed better on an ongoing task if they were presented with a number that increased at an increasing velocity than if they were not presented with such a number or if they were presented with a number that increased at a decreasing or constant velocity. This acceleration effect occurred regardless of the absolute magnitude or the absolute velocity of the number, and even when the number was not tied to any specific rewards. This research shows the potential of numerical nudging—using inherently meaningless numbers to strategically alter behaviors—and is especially relevant in the present age of digital devices.},
	number = {8},
	journal = {Psychological Science},
	author = {Shen, Luxi and Hsee, Christopher K.},
	year = {2017},
	keywords = {acceleration, evaluability, game, gamification, medium maximization, motivation, nudge, number comprehension, performance, score, token economy, velocity},
	pages = {1077--1086},
	file = {PDF:/home/miguel/Zotero/storage/5KHU36UW/2017_SHEN.pdf:application/pdf},
}

@article{Brandimarte2017,
	title = {Differential {Discounting} and {Present} {Impact} of {Past} {Information}},
	issn = {00963445},
	doi = {10.1037/xge0000372},
	abstract = {How does information about a person's past, accessed now, affect individuals' impressions of that person? In 2 survey experiments and 2 experiments with actual incentives, we compare whether, when evaluating a person, information about that person's past greedy or immoral behaviors is discounted similarly to information about her past generous or moral behaviors. We find that, no matter how far in the past a person behaved greedily or immorally, information about her negative behaviors is hardly discounted at all. In contrast, information about her past positive behaviors is discounted heavily: recent behaviors are much more influential than behaviors that occurred a long time ago. The lesser discounting of information about immoral and greedy behaviors is not caused by these behaviors being more influential, memorable, extreme, or attention-grabbing; rather, they are perceived as more diagnostic of a person's character than past moral or generous behaviors. The phenomenon of differential discounting of past information has particular relevance in the digital age, where information about people's past is easily retrieved. Our findings have significant implications for theories of impression formation and social information processing. (PsycINFO Database Record},
	journal = {Journal of Experimental Psychology: General},
	author = {Brandimarte, Laura and Vosgerau, Joachim and Acquisti, Alessandro},
	year = {2017},
	keywords = {Diagnosticity, Dictator game, Discounting of information, Experiments, Impression formation},
	file = {PDF:/home/miguel/Zotero/storage/N9JD4LH5/2018_BRANDIMARTE.pdf:application/pdf},
}

@article{Jayles2017,
	title = {How social information can improve estimation accuracy in human groups},
	volume = {114},
	issn = {10916490},
	doi = {10.1073/pnas.1703695114},
	abstract = {In our digital and connected societies, the development of social networks, online shopping, and reputation systems raises the questions of how individuals use social information and how it affects their decisions. We report experiments performed in France and Japan, in which subjects could update their estimates after having received information from other subjects. We measure and model the impact of this social information at individual and collective scales. We observe and justify that, when individuals have little prior knowledge about a quantity, the distribution of the logarithm of their estimates is close to a Cauchy distribution. We find that social influence helps the group improve its properly defined collective accuracy. We quantify the improvement of the group estimation when additional controlled and reliable information is provided, unbeknownst to the subjects. We show that subjects’ sensitivity to social influence permits us to define five robust behavioral traits and increases with the difference between personal and group estimates. We then use our data to build and calibrate a model of collective estimation to analyze the impact on the group performance of the quantity and quality of information received by individuals. The model quantitatively reproduces the distributions of estimates and the improvement of collective performance and accuracy observed in our experiments. Finally, our model predicts that providing a moderate amount of incorrect information to individuals can counterbalance the human cognitive bias to systematically underestimate quantities and thereby improve collective performance.},
	number = {47},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Jayles, Bertrand and Kim, Hye rin and Escobedo, Ramón and Cezera, Stéphane and Blanchet, Adrien and Kameda, Tatsuya and Sire, Clément and Theraulaz, Guy},
	year = {2017},
	keywords = {Collective intelligence, Computational modeling, Self-organization, Social influence, Wisdom of crowds},
	pages = {12620--12625},
	file = {PDF:/home/miguel/Zotero/storage/AQNKR37I/2017_SINGER.pdf:application/pdf},
}

@article{Schneider2018,
	title = {Nudging {Users} {Into} {Online} {Verification}: {The} {Case} of {Carsharing} {Platforms}},
	abstract = {With the emergence of new technologies, user transactions have become increasingly digital and thus anonymous. As a result, online platforms started offering new user identity verification mechanisms including online verification. However, convincing users to verify online represents a challenge as benefits must be carefully balanced with resulting risks. We combine Toulmin’s model of argumentation with Regulatory Focus Theory to develop communication arguments to nudge users into online verification. Two argument types consisting of both claim and claim-supporting data are employed: a promotion-focus argument conveying convenience and a prevention-focus argument aiming to reduce privacy concerns. In a controlled experiment, we find that both claims significantly increase online verification conversion rate if supported by data. The effect of a prevention-focus claim is stronger than that of a promotion-focus claim. However, if a prevention-focus claim is not supported by data, it actually decreases online verification rate compared to if no claim is shown.},
	journal = {ICIS 2017: Transforming Society with Digital Innovation},
	author = {Schneider, David and Grupp, Tillmann and Lins, Sebastian and Benlian, Alexander and Sunyaev, Ali},
	year = {2018},
	keywords = {Conversion rate, Digital nudging, Human behaviour in IS, Human computer interaction, Online identity verification, Regulatory Focus Theory, Toulmin’s model of argumentation},
	pages = {1--20},
	file = {PDF:/home/miguel/Zotero/storage/7NS39E4V/2017_SCHNEIDER.pdf:application/pdf},
}

@book{Addae2019,
	title = {Exploring user behavioral data for adaptive cybersecurity},
	volume = {29},
	isbn = {1125701909236},
	url = {https://doi.org/10.1007/s11257-019-09236-5},
	abstract = {This paper describes an exploratory investigation into the feasibility of predictive analytics of user behavioral data as a possible aid in developing effective user models for adaptive cybersecurity. Partial least squares structural equation modeling is applied to the domain of cybersecurity by collecting data on users’ attitude towards digital security, and analyzing how that influences their adoption and usage of technological security controls. Bayesian-network modeling is then applied to integrate the behavioral variables with simulated sensory data and/or logs from a web browsing session and other empirical data gathered to support personalized adaptive cybersecurity decision-making. Results from the empirical study show that predictive analytics is feasible in the context of behavioral cybersecurity, and can aid in the generation of useful heuristics for the design and development of adaptive cybersecurity mechanisms. Predictive analytics can also aid in encoding digital security behavioral knowledge that can support the adaptation and/or automation of operations in the domain of cybersecurity. The experimental results demonstrate the effectiveness of the techniques applied to extract input data for the Bayesian-based models for personalized adaptive cybersecurity assistance.},
	publisher = {Springer Netherlands},
	author = {Addae, Joyce H. and Sun, Xu and Towey, Dave and Radenkovic, Milena},
	year = {2019},
	doi = {10.1007/s11257-019-09236-5},
	note = {Publication Title: User Modeling and User-Adapted Interaction
Issue: 3
ISSN: 15731391},
	keywords = {Behavioral analytics, Adaptive assistance, Bayesian-inference, Cybersecurity, Predictive modeling},
	file = {PDF:/home/miguel/Zotero/storage/Z54TN2H2/2018_ADDAE.pdf:application/pdf},
}

@article{Meske2017,
	title = {The {DINU}-{Model} - {A} process model for the {Design} of {Nudges}},
	volume = {2017},
	url = {http://aisel.aisnet.org/ecis2017_rip%0Ahttp://aisel.aisnet.org/ecis2017_rip/11},
	abstract = {The sociotechnical paradigm legitimates our discipline and serves as core identity of IS. In this study, we want to focus on IS-induced human behavior by introducing a process model for nudging in IS. In behavioral economics, the concept of nudging has been proposed, which makes use of human cognitive processes and can direct people to an intended behavior. In computer science, the concept of persuasion has evolved with similar goals. Both concepts, nudging and persuasion, can contribute to IS research and may help to explain and steer user behavior in information systems. We aim for an integration of both concepts into one digital nudging process model, making it usable and accessible. We analyzed literature on nudging and persuasion and derived different steps, requirements, and nudging elements. The developed process model aims at enabling researchers and practitioners to design nudges in e.g. software systems but may also contribute to other areas like IT governance. Though the evaluation part of our study has not yet been completed, we present the current state of the process model enabling more research in this area.},
	journal = {Ecis},
	author = {Meske, Christian and Potthoff, Tobias},
	year = {2017},
	note = {ISBN: 978-0-9915567-0-0},
	keywords = {Digital Nudging, Behavior, Change, Nudge, Persuasion},
	pages = {2587--2597},
	file = {PDF:/home/miguel/Zotero/storage/MS89TJSQ/2017_MESKE.pdf:application/pdf},
}

@article{Aabel2018,
	title = {Digital {Cross}-{Channel} {Usability} {Heuristics}: {Improving} the {Digital} {Health} {Experience}},
	volume = {13},
	abstract = {The number of ways consumers of health information access digital content has grown rapidly in recent years. People seek information using various hardware devices (e.g., computer, tablet, phone), which can support multiple digital media platforms (e.g., browser, apps, texting/short message service [SMS], email, social media). Users expect to be able to access information from one or more of these digital channels at any time and from any place. For this reason, public health organizations must create a unified experience across all their digital channels to support the on-demand needs of those seeking health information. Though only in the past few years have many organizations started to think about all their channels together as a larger digital ecosystem. A cross-channel (CC) approach is needed to ensure that all channels are unified, seamless, and consistent. However, this approach is resource and time intensive, and because it's a relatively recent trend in the digital world, organizations often don't have the resources allocated for this need. Tools for developing, managing, and improving a CC experience are scarce. Most digital CC research has been focused on e-commerce, leaving a gap in the literature relating to CC experiences with health communication and behavioral health interventions. This paper proposes a process to identify and prioritize user task and channel relationships with a set of newly developed CC heuristics applied to these priority needs. This process is in the context of a digital public health program. Although this approach was developed from a public health perspective, it can be applicable to a wide range of public and commercial digital services or products.},
	number = {2},
	journal = {Journal of Usability Studies},
	author = {Aabel, Brad and Abeywarna, Dilini},
	year = {2018},
	keywords = {behavioral health, cross-channel, cross-device, eHealth, heuristic evaluation, multi-channel, omni-channel, public health, quit smoking, Smokefreegov 53, smoking cessation, usability, user experience, UX},
	pages = {52--72},
	file = {PDF:/home/miguel/Zotero/storage/CBHPARSU/2018_AABEL.pdf:application/pdf},
}

@article{Stryja2017,
	title = {A decision support system design to overcome resistance towards sustainable innovations},
	volume = {2017},
	abstract = {The concept of sustainability has been acknowledged as one of the central and most important issues of our time. However, technological innovations which provide a more sustainable way of living, for instance electric cars, are not always welcomed with open arms by consumers but often resisted at the beginning. As such, human resistance behavior can be explained as an interplay of different personality traits that favour the status quo. In this study, a decision support system design is introduced which bases on the concept of digital nudging that addresses innovation resistance on an individual's cognitive level by de-biasing innovation trial decision-making. An experimental pre-study is conducted to test the influence of different DSS modifications on the selection of electric cars in an online rental car booking scenario. First results show that DSS which set sustainable innovations as default option have a significantly positive effect on their trial probability while priming consumers towards electric car trial has no significant effect.},
	journal = {Proceedings of the 25th European Conference on Information Systems, ECIS 2017},
	author = {Stryja, Carola and Satzger, Gerhard and Dorner, Verena},
	year = {2017},
	note = {ISBN: 9780991556700},
	keywords = {Nudging, Choice architecture, Electric car, Experimental study, Product trial},
	pages = {2885--2895},
	file = {PDF:/home/miguel/Zotero/storage/FZA7W287/2017_STRYJA.pdf:application/pdf},
}

@article{Liu2018,
	title = {Temporal understanding of human mobility: {A} multi-time scale analysis},
	volume = {13},
	issn = {19326203},
	doi = {10.1371/journal.pone.0207697},
	abstract = {The recent availability of digital traces generated by cellphone calls has significantly increased the scientific understanding of human mobility. Until now, however, based on low time resolution measurements, previous works have ignored to study human mobility under various time scales due to sparse and irregular calls, particularly in the era of mobile Internet. In this paper, we introduced Mobile Flow Records, flow-level data access records of online activity of smartphone users, to explore human mobility. Mobile Flow Records collect high-resolution information of large populations. By exploiting this kind of data, we show the models and statistics of human mobility at a large-scale (3,542,235 individuals) and finer-granularity (7.5min). Next, we investigated statistical variations and biases of mobility models caused by different time scales (from 7.5min to 32h), and found that the time scale does influence the mobility model, which indicates a deep coupling of human mobility and time. We further show that mobility behaviors like transportation modes contribute to the diversity of human mobility, by exploring several novel and refined features (e.g., motion speed, duration, and trajectory distance). Particularly, we point out that 2-hour sampling adopted in previous works is insufficient to study detailed motion behaviors. Our work not only offers a macroscopic and microscopic view of spatial-temporal human mobility, but also applies previously unavailable features, both of which are beneficial to the studies on phenomena driven by human mobility.},
	number = {11},
	journal = {PLoS ONE},
	author = {Liu, Tongtong and Yang, Zheng and Zhao, Yi and Wu, Chenshu and Zhou, Zimu and Liu, Yunhao},
	year = {2018},
	note = {ISBN: 1111111111},
	pages = {1--15},
	file = {PDF:/home/miguel/Zotero/storage/2YS8G7J2/2018_LIU.pdf:application/pdf},
}

@article{SchneiderC.WeinmannM.andvomBrocke2018,
	title = {Digital {Nudging}–{Guiding} {Choices} by {Using} {Interface} {Design}},
	volume = {61},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3052192},
	abstract = {Decisions are influenced by the environment in which the choices are presented. In fact, no choice is made in a vacuum, as there is no neutral way to present choices. Presenting choices in certain ways, even unintentionally, can “nudge” people to change their behavior in predictable ways. “Nudging” is a concept from behavioral economics that describes how even minor changes to decision environments (e.g., setting defaults) can influence decision outcomes—typically without the decision-maker noticing this influence. The more decisions people make using digital devices, the more the software engineer becomes a choice architect who knowingly or unknowingly influences people’s decisions. Thus, we extend the nudging concept to the digital environment, defining “digital nudging” as the use of user-interface design elements to guide people’s behavior in digital choice environments, and present a digital nudge design process to help online choice architects take nudging principles into consideration when designing digital choice environments like Web sites and apps.},
	number = {7},
	journal = {Communications of the ACM},
	author = {Schneider, C., Weinmann, M., {and} vom Brocke, J.},
	year = {2018},
	pages = {67--73},
	file = {PDF:/home/miguel/Zotero/storage/DBJVG2CC/201X_SCHNEIDER.pdf:application/pdf},
}

@article{Ziegeldorf2016,
	title = {Comparison-based privacy: {Nudging} privacy in social media (position paper)},
	volume = {9481},
	issn = {16113349},
	doi = {10.1007/978-3-319-29883-2_15},
	abstract = {Social media continues to lead imprudent users into over-sharing, exposing them to various privacy threats. Recent research thus focusses on nudging the user into the ‘right’ direction. In this paper, we propose Comparison-based Privacy (CbP), a design paradigm for privacy nudges that overcomes the limitations and challenges of existing approaches. CbP is based on the observation that comparison is a natural human behavior. With CbP, we transfer this observation to decision-making processes in the digital world by enabling the user to compare herself along privacy-relevant metrics to user-selected comparison groups. In doing so, our approach provides a framework for the integration of existing nudges under a self-adaptive, user-centric norm of privacy. Thus, we expect CbP not only to provide technical improvements, but to also increase user acceptance of privacy nudges. We also show how CbP can be implemented and present preliminary results.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Ziegeldorf, Jan Henrik and Henze, Martin and Hummen, René and Wehrle, Klaus},
	year = {2016},
	note = {ISBN: 9783319298825},
	keywords = {Behavioral nudge, Privacy, Social media},
	pages = {226--234},
	file = {PDF:/home/miguel/Zotero/storage/2WRSAQR3/201X_ZIEGELDORF.pdf:application/pdf},
}

@article{Settanni2018,
	title = {Predicting {Individual} {Characteristics} from {Digital} {Traces} on {Social} {Media}: {A} {Meta}-{Analysis}},
	volume = {21},
	issn = {21522723},
	doi = {10.1089/cyber.2017.0384},
	abstract = {The increasing utilization of social media provides a vast and new source of user-generated ecological data (digital traces), which can be automatically collected for research purposes. The availability of these data sets, combined with the convergence between social and computer sciences, has led researchers to develop automated methods to extract digital traces from social media and use them to predict individual psychological characteristics and behaviors. In this article, we reviewed the literature on this topic and conducted a series of meta-analyses to determine the strength of associations between digital traces and specific individual characteristics; personality, psychological well-being, and intelligence. Potential moderator effects were analyzed with respect to type of social media platform, type of digital traces examined, and study quality. Our findings indicate that digital traces from social media can be studied to assess and predict theoretically distant psychosocial characteristics with remarkable accuracy. Analysis of moderators indicated that the collection of specific types of information (i.e., user demographics), and the inclusion of different types of digital traces, could help improve the accuracy of predictions.},
	number = {4},
	journal = {Cyberpsychology, Behavior, and Social Networking},
	author = {Settanni, Michele and Azucar, Danny and Marengo, Davide},
	year = {2018},
	keywords = {social media, data mining, digital traces, predictive modeling, psychological assessment, psychosocial characteristics},
	pages = {217--228},
	file = {PDF:/home/miguel/Zotero/storage/UMTY8D82/201X_SETTANI.pdf:application/pdf},
}

@article{Hummel2019,
	title = {How effective is nudging? {A} quantitative review on the effect sizes and limits of empirical nudging studies},
	volume = {80},
	issn = {22148051},
	url = {https://doi.org/10.1016/j.socec.2019.03.005},
	doi = {10.1016/j.socec.2019.03.005},
	abstract = {Changes in the choice architecture, so-called nudges, have been employed in a variety of contexts to alter people's behavior. Although nudging has gained a widespread popularity, the effect sizes of its influences vary considerably across studies. In addition, nudges have proven to be ineffective or even backfire in selected studies which raises the question whether, and under which conditions, nudges are effective. Therefore, we conduct a quantitative review on nudging with 100 primary publications including 317 effect sizes from different research areas. We derive four key results. (1) A morphological box on nudging based on eight dimensions, (2) an assessment of the effectiveness of different nudging interventions, (3) a categorization of the relative importance of the application context and the nudge category, and (4) a comparison of nudging and digital nudging. Thereby, we shed light on the (in)effectiveness of nudging and we show how the findings of the past can be used for future research. Practitioners, especially government officials, can use the results to review and adjust their policy making.},
	number = {February},
	journal = {Journal of Behavioral and Experimental Economics},
	author = {Hummel, Dennis and Maedche, Alexander},
	year = {2019},
	note = {Publisher: Elsevier},
	keywords = {Nudging, Digital nudging, Choice architecture, Behavioral economics, Quantitative review},
	pages = {47--58},
	file = {PDF:/home/miguel/Zotero/storage/P7JSN9F4/2019_HUMMEL.pdf:application/pdf},
}

@article{Lettieri2019,
	title = {Platform {Economy} and {Techno}-{Regulation}—{Experimenting} with {Reputation} and {Nudge}},
	volume = {11},
	issn = {1999-5903},
	doi = {10.3390/fi11070163},
	abstract = {In the cloud-based society, where the vast majority of social, economic and personal interactions is mediated by information communication technology (ICT), technology is no longer simply a subject of regulation but is becoming an integral part of the regulatory process. Techno-regulation, the “intentional influencing of individuals’ behavior by building norms into technological devices,” is inspiring new ways to support legal safeguards through hardware and software tools, technical solutions allowing the creation of legal relations, hampering breaches of law and even promoting norm compliance. This paper touches on these issues by focusing on Digital Labor Platforms, one of the most relevant phenomena in the gig economy. We present a research project exploring innovative techno-regulatory solutions to protect gig economy workers. The idea is to integrate, in the same strategy, legal principles, regulatory objectives and software solutions. Our attention focuses on two results of our activity—a techno-regulatory model relying on reputational mechanisms to affect the behavior of digital labor market operators and GigAdvisor, a cross-platform experimental application implementing the model.},
	number = {7},
	journal = {Future Internet},
	author = {Lettieri, Nicola and Guarino, Alfonso and Malandrino, Delfina and Zaccagnino, Rocco},
	year = {2019},
	keywords = {nudge, digital labour platforms, reputational systems, techno-regulation},
	pages = {163},
	file = {PDF:/home/miguel/Zotero/storage/U6LYC999/2019_LETTIERI.pdf:application/pdf},
}

@article{Purohit2019,
	title = {Functional digital nudges: {Identifying} optimal timing for effective behavior change},
	doi = {10.1145/3290607.3312876},
	abstract = {Digital nudges hold enormous potential to change behavior. Despite the appeal to consider timing as a critical factor responsible for the success of digital nudges, a comprehensive organizing framework to guide the design of digital nudges considering nudge moment is yet to be provided. In this paper, we advance the theoretical model to design digital nudges by incorporating three key components: (1) Identifying the optimal digital nudge moment (2) Inferring this optimal moment and (3) Delivering the digital nudge at that moment. We further discuss the existing work and open research avenues.},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	author = {Purohit, Aditya Kumar and Holzer, Adrian},
	year = {2019},
	note = {ISBN: 9781450359719},
	keywords = {Digital Nudge, Behavioral change interventions, Nudge moment, Timing},
	pages = {5--10},
	file = {PDF:/home/miguel/Zotero/storage/IRWERWHH/2019_PUROHIT.pdf:application/pdf},
}

@article{Koh2019,
	title = {Offline biases in online platforms: a study of diversity and homophily in {Airbnb}},
	volume = {8},
	issn = {21931127},
	doi = {10.1140/epjds/s13688-019-0189-5},
	abstract = {How diverse are sharing economy platforms? Are they fair marketplaces, where all participants operate on a level playing field, or are they large-scale online aggregators of offline human biases? Often portrayed as easy-to-access digital spaces whose participants receive equal opportunities, such platforms have recently come under fire due to reports of discriminatory behaviours among their users, and have been associated with gentrification phenomena that exacerbate preexisting inequalities along racial lines. In this paper, we focus on the Airbnb sharing economy platform, and analyse the diversity of its user base across five large cities. We find it to be predominantly young, female, and white. Notably, we find this to be true even in cities with a diverse racial composition. We then introduce a method based on the statistical analysis of networks to quantify behaviours of homophily, heterophily and avoidance between Airbnb hosts and guests. Depending on cities and property types, we do find signals of such behaviours relating both to race and gender. We use these findings to provide platform design recommendations, aimed at exposing and possibly reducing the biases we detect, in support of a more inclusive growth of sharing economy platforms.},
	number = {1},
	journal = {EPJ Data Science},
	author = {Koh, Victoria and Li, Weihua and Livan, Giacomo and Capra, Licia},
	year = {2019},
	note = {arXiv: 1811.06397},
	keywords = {Homophily, Online User Behavior, Sharing Economy, Social Networks, Statistical Validation},
	pages = {2--7},
	file = {PDF:/home/miguel/Zotero/storage/3ZD6PWTE/201X_KOH.pdf:application/pdf},
}

@article{Schofield2018,
	title = {Courting the visual image: {The} ability of digital graphics and interfaces to alter the memory and behaviour of the viewer},
	volume = {10901 LNCS},
	issn = {16113349},
	doi = {10.1007/978-3-319-91238-7_27},
	abstract = {An intrinsic connection exists between humans and the memories they create; they define who we are, where we came from and our accomplishments and failures. However, decades of research has shown how fragile human memory can be. Almost all human computer interfaces involve vision and most rely on vision as the primary means of passing information to the user [1]. It is worth considering that perhaps this specific form of media interaction requires special care and attention due to its inherently persuasive nature, and the undue reliance that the viewer may place on information presented through a (potentially photorealistic) visualisation medium. Their influence on human memory and behaviour cannot be underestimated. This paper will introduce research undertaken by the author over the past 25 years that has experimented with, and examined a range of visual based presentation technology into courtrooms all over the world. Courtrooms are environments where the decisions made (based on human memory and comprehension) can significantly affect the lives of others. This paper describes research undertaken to assess the effect of visual technology on users (in particular their memory and decision making abilities) and describes some of the issues raised by the experimental results. The work presented in this paper connects psychological research with human cognitive and perceptual processes and limitations, to allow the evaluation and optimisation of visual interfaces. The paper concludes with a discussion of the potential benefits and problems of designing interactive visual technology when considering the impact on human cognition.},
	number = {August},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Schofield, Damian},
	year = {2018},
	note = {ISBN: 9783319912370},
	keywords = {Computer graphics, Evidence, HCI, Psychology, Visual images},
	pages = {325--344},
	file = {PDF:/home/miguel/Zotero/storage/9CAVHFUV/2018_SCHOFIELD.pdf:application/pdf},
}

@article{Fitz2019,
	title = {Computers in {Human} {Behavior} {Batching} smartphone noti fi cations can improve well-being},
	volume = {101},
	number = {July},
	author = {Fitz, Nicholas and Ariely, Dan and Kushlev, Kostadin and Jagannathan, Ranjan and Lewis, Terrel and Paliwal, Devang},
	year = {2019},
	keywords = {batching noti fi cations},
	pages = {84--94},
	file = {PDF:/home/miguel/Zotero/storage/CXQBD4DU/2019_FITZ.pdf:application/pdf},
}

@article{Seltman2018,
	title = {Experimental design and {Analysis}},
	volume = {20},
	issn = {2316-770X},
	doi = {10.35699/2316-770x.2013.2692},
	abstract = {Resultado das disputas capitalistas por terra, as favelas sempre se instalaram em áreas relegadas pelo mercadoimobiliário formal, e em estreita relação com os cursos d’água. Tomando como contexto empírico a cidade de BeloHorizonte, este artigo discute a relação dialética entre água e favelas – fatores historicamente negligenciados e que apenasrecentemente ganharam alguma prioridade nas políticas públicas. As favelas, embora representativas da precariedade edeficiências urbanas dos espaços reservados às classes destituídas, oferecem interessantes possibilidades de investigação e reflexão acerca de um padrão de urbanização já excluído da cidade formal, ancorado numa relação desalienada entre gente e água pela reincorporação à vida cotidiana dos cursos d’água despoluídos, desde as pequenas cabeceiras até os fundos de vales urbanizados.},
	number = {2},
	journal = {Revista da Universidade Federal de Minas Gerais},
	author = {Seltman, Howard J},
	year = {2018},
	file = {PDF:/home/miguel/Zotero/storage/UVPI2FSB/2018_Experimental design.pdf:application/pdf},
}

@article{SURA2018c,
	title = {Gestión de {Tendencias} y {Riesgos}},
	url = {www.sura.com},
	author = {{SURA}},
	year = {2018},
	pages = {35},
	file = {PDF:/home/miguel/Zotero/storage/NZIRXDMI/201x_Gestión de Tendencias & Riesgos.pdf:application/pdf},
}

@book{Wulff2003,
	title = {A {First} {Course} in {Design} and {Analysis} of {Experiments}},
	volume = {57},
	isbn = {0-7167-3510-5},
	abstract = {Oehlert\&apos;s text is suitable for either a service course for non-statistics graduate students or for statistics majors. Unlike most texts for the one-term grad/upper level course on experimental design, Oehlert\&apos;s new book offers a superb balance of both analysis and design, presenting three practical themes to students: • when to use various designs • how to analyze the results • how to recognize various design options Also, unlike other older texts, the book is fully oriented toward the use of statistical software in analyzing experiments.},
	author = {Wulff, Shaun S},
	year = {2003},
	doi = {10.1198/tas.2003.s210},
	note = {Publication Title: The American Statistician
Issue: 1
ISSN: 0003-1305},
	file = {PDF:/home/miguel/Zotero/storage/KPXMUICJ/20XX_Experimental design.pdf:application/pdf},
}

@article{Yoon2018,
	title = {Using {Behavioral} {Analytics} to {Increase} {Exercise}: {A} {Randomized} {N}-of-1 {Study}},
	volume = {54},
	issn = {18732607},
	url = {http://dx.doi.org/10.1016/j.amepre.2017.12.011},
	doi = {10.1016/j.amepre.2017.12.011},
	abstract = {Introduction: This intervention study used mobile technologies to investigate whether those randomized to receive a personalized “activity fingerprint” (i.e., a one-time tailored message about personal predictors of exercise developed from 6 months of observational data) increased their physical activity levels relative to those not receiving the fingerprint. Study design: A 12-month randomized intervention study. Setting/participants: From 2014 to 2015, 79 intermittent exercisers had their daily physical activity assessed by accelerometry (Fitbit Flex) and daily stress experience, a potential predictor of exercise behavior, was assessed by smartphone. Intervention: Data collected during the first 6 months of observation were used to develop a person-specific “activity fingerprint” (i.e., N-of-1) that was subsequently sent via email on a single occasion to randomized participants. Main outcome measures: Pre–post changes in the percentage of days exercised were analyzed within and between control and intervention groups. Results: The control group significantly decreased their proportion of days exercised (10.5\% decrease, p{\textless}0.0001) following randomization. By contrast, the intervention group showed a nonsignificant decrease in the proportion of days exercised (4.0\% decrease, p=0.14). Relative to the decrease observed in the control group, receipt of the activity fingerprint significantly increased the likelihood of exercising in the intervention group (6.5\%, p=0.04). Conclusions: This N-of-1 intervention study demonstrates that a one-time brief message conveying personalized exercise predictors had a beneficial effect on exercise behavior among urban adults.},
	number = {4},
	journal = {American Journal of Preventive Medicine},
	author = {Yoon, Sunmoo and Schwartz, Joseph E. and Burg, Matthew M. and Kronish, Ian M. and Alcantara, Carmela and Julian, Jacob and Parsons, Faith and Davidson, Karina W. and Diaz, Keith M.},
	year = {2018},
	note = {Publisher: Elsevier Inc.},
	pages = {559--567},
	file = {2018_SUNMOO_USING BEHAVIORAL.pdf:/home/miguel/Zotero/storage/AY7VQQUF/2018_SUNMOO_USING BEHAVIORAL.pdf:application/pdf;2018_YOON.pdf:/home/miguel/Zotero/storage/EVXFGWTM/2018_YOON.pdf:application/pdf},
}

@article{Pinder2018,
	title = {Digital behaviour change interventions to break and form habits},
	volume = {25},
	issn = {15577325},
	doi = {10.1145/3196830},
	abstract = {Digital behaviour change interventions, particularly those using pervasive computing technology, hold great promise in supporting users to change their behaviour. However, most interventions fail to take habitual behaviour into account, limiting their potential impact. This failure is partly driven by a plethora of overlapping behaviour change theories and related strategies that do not consider the role of habits. We critically review the main theories and models used in the research to analyse their application to designing effective habitual behaviour change interventions. We highlight the potential for Dual Process Theory, modern habit theory, and Goal Setting Theory, which together model how users form and break habits, to drive effective digital interventions. We synthesise these theories into an explanatory framework, the Habit Alteration Model, and use it to outline the state of the art. We identify the opportunities and challenges of habit-focused interventions.},
	number = {3},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Pinder, Charlie and Vermeulen, Jo and Cowan, Benjamin R. and Beale, Russell},
	year = {2018},
	keywords = {Persuasive technology, Behaviour change technology, Digital behaviour change interventions, Habit breaking technology, Habit forming technology},
	file = {2018_PINDER ET AL_DIGITAL BEHAVIOUR CHANGE.pdf:/home/miguel/Zotero/storage/MM2E6F28/2018_PINDER ET AL_DIGITAL BEHAVIOUR CHANGE.pdf:application/pdf;2018_PINDER.pdf:/home/miguel/Zotero/storage/BRSUGYNW/2018_PINDER.pdf:application/pdf},
}

@article{Durbach2019,
	title = {Behavioural {Analytics}: {Exploring} judgments and choices in large data sets},
	volume = {70},
	issn = {14769360},
	url = {https://doi.org/10.1080/01605682.2018.1434400},
	doi = {10.1080/01605682.2018.1434400},
	abstract = {The ever-increasing availability of large data-sets that store users’ judgements (such as forecasts and preferences) and choices (such as acquisitions of goods and services) provides a fertile ground for Behavioural Operational Research (BOR). In this paper, we review the streams of Behavioural Decision Research that might be useful for BOR researchers and practitioners to analyse such behavioural data-sets. We then suggest ways that concepts from these streams can be employed in exploring behavioural data-sets for (i) detecting behavioural patterns, (ii) exploiting behavioural findings and (iii) improving judgements and decisions of consumers and citizens. We also illustrate how this taxonomy for behavioural analytics might be utilised in practice, in three real-world studies with behavioural data-sets generated by websites and online user activity.},
	number = {2},
	journal = {Journal of the Operational Research Society},
	author = {Durbach, Ian N. and Montibeller, Gilberto},
	year = {2019},
	note = {Publisher: Taylor \& Francis},
	keywords = {behavioural data mining, Behavioural operational research, decision analytics},
	pages = {255--268},
	file = {2019_DURBACH_BEHAVIORAL ANALYTICS.pdf:/home/miguel/Zotero/storage/CE3JRU7H/2019_DURBACH_BEHAVIORAL ANALYTICS.pdf:application/pdf;2019_DURBACH_OJO.pdf:/home/miguel/Zotero/storage/6ZPBJUNP/2019_DURBACH_OJO.pdf:application/pdf},
}

@article{Singh2018,
	title = {Big data mining of energy time series for behavioral analytics and energy consumption forecasting},
	volume = {11},
	issn = {19961073},
	doi = {10.3390/en11020452},
	abstract = {Responsible, efficient and environmentally aware energy consumption behavior is becoming a necessity for the reliable modern electricity grid. In this paper, we present an intelligent data mining model to analyze, forecast and visualize energy time series to uncover various temporal energy consumption patterns. These patterns define the appliance usage in terms of association with time such as hour of the day, period of the day, weekday, week, month and season of the year as well as appliance-appliance associations in a household, which are key factors to infer and analyze the impact of consumers' energy consumption behavior and energy forecasting trend. This is challenging since it is not trivial to determine the multiple relationships among different appliances usage from concurrent streams of data. Also, it is difficult to derive accurate relationships between interval-based events where multiple appliance usages persist for some duration. To overcome these challenges, we propose unsupervised data clustering and frequent pattern mining analysis on energy time series, and Bayesian network prediction for energy usage forecasting. We perform extensive experiments using real-world context-rich smart meter datasets. The accuracy results of identifying appliance usage patterns using the proposed model outperformed Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP) at each stage while attaining a combined accuracy of 81.82\%, 85.90\%, 89.58\% for 25\%, 50\% and 75\% of the training data size respectively. Moreover, we achieved energy consumption forecast accuracies of 81.89\% for short-term (hourly) and 75.88\%, 79.23\%, 74.74\%, and 72.81\% for the long-term; i.e., day, week, month, and season respectively.},
	number = {2},
	journal = {Energies},
	author = {Singh, Shailendra and Yassine, Abdulsalam},
	year = {2018},
	keywords = {Behavioral analytics, Big data, Clustering analysis, Data mining, Energy forecasting, Energy time series, Smart meters},
	file = {2017_SINGH.pdf:/home/miguel/Zotero/storage/MUW58WNM/2017_SINGH.pdf:application/pdf;2017_SINGH.pdf:/home/miguel/Zotero/storage/MNDHZK7W/2017_SINGH.pdf:application/pdf;2018_SINGH_BIG DATA MINING OF ENERGY.pdf:/home/miguel/Zotero/storage/BYJGBFZD/2018_SINGH_BIG DATA MINING OF ENERGY.pdf:application/pdf},
}

@article{noauthor_crisp-dm_nodate,
	title = {{CRISP}-{DM} {CRISP}-{DM}},
	file = {PDF:/home/miguel/Zotero/storage/Q52MEZIB/CRISP_DM_CONTENIDO.pdf:application/pdf},
}

@article{noauthor_segmentacion_nodate,
	title = {Segmentación dinámica},
	file = {PDF:/home/miguel/Zotero/storage/FC53HFFB/Segmentación Dinámica.pdf:application/pdf},
}

@article{Febrero2008,
	title = {Outlier detection in functional data by depth measures, with application to identify abnormal {NOx} levels},
	volume = {19},
	issn = {11804009},
	doi = {10.1002/env.878},
	abstract = {This paper analyzes outlier detection for functional data by means of functional depths, which measures the centrality of a given curve within a group of trajectories providing center-outward orderings of the set of curves. We give some insights of the usefulness of looking for outliers in functional datasets and propose a method based in depths for the functional outlier detection. The performance of the proposed procedure is analyzed by several Monte Carlo experiments. Finally, we illustrate the procedure by finding outliers in a dataset of NOx (nitrogen oxides) emissions taken from a control station near an industrial area. Copyright © 2007 John Wiley \& Sons, Ltd.},
	number = {4},
	journal = {Environmetrics},
	author = {Febrero, Manuel and Galeano, Pedro and González-Manteiga, Wenceslao},
	year = {2008},
	keywords = {Depths, Functional median, Functional trimmed mean, Nitrogen oxides, Outliers, Smoothed bootstrap},
	pages = {331--345},
	file = {PDF:/home/miguel/Zotero/storage/4LA99ADC/2008_Febrero-Bande, Galeano, Gonzalez.pdf:application/pdf},
}

@article{Ieva2013,
	title = {Depth measures for multivariate functional data},
	volume = {42},
	issn = {03610926},
	doi = {10.1080/03610926.2012.746368},
	abstract = {In this article, we address the problem of mining and analyzing multivariate functional data. That is, data where each observation is a set of possibly correlated functions. Complex data of this kind is more and more common in many research fields, particularly in the biomedical context. In this work, we propose and apply a new concept of depth measure for multivariate functional data. With this new depth measure it is possible to generalize robust statistics, such as the median, to the multivariate functional framework, which in turn allows the application of outlier detection, boxplots construction, and nonparametric tests also in this more general framework. We present an application to Electrocardiographic (ECG) signals. Copyright © Taylor \& Francis Group, LLC.},
	number = {7},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Ieva, Francesca and Paganoni, Anna M.},
	year = {2013},
	keywords = {Depth measures, ECG signals, Multivariate functional data, Rank tests},
	pages = {1265--1276},
	file = {PDF:/home/miguel/Zotero/storage/IQJYHG82/2013_Ieva Paganoni.pdf:application/pdf},
}

@article{Ieva2017,
	title = {Component-wise outlier detection methods for robustifying multivariate functional samples},
	issn = {09325026},
	doi = {10.1007/s00362-017-0953-1},
	abstract = {We propose a new method for detecting outliers in multivariate functional data. We exploit the joint use of two different depth measures, and generalize the outliergram to the multivariate functional framework, aiming at detecting and discarding both shape and magnitude outliers. The main application consists in robustifying the reference samples of data, composed by G different known groups to be used, for example, in classification procedures in order to make them more robust. We asses by means of a simulation study the method’s performance in comparison with different outlier detection methods. Finally we consider a real dataset: we classify data minimizing a suitable distance from the center of reference groups. We compare performance of supervised classification on test sets training the algorithm on original dataset and on the robustified one, respectively.},
	journal = {Statistical Papers},
	author = {Ieva, Francesca and Paganoni, Anna Maria},
	year = {2017},
	note = {Publisher: Springer Berlin Heidelberg},
	pages = {1--20},
	file = {PDF:/home/miguel/Zotero/storage/B9M5IURW/2017_Ieva Paganoni.pdf:application/pdf},
}

@article{Schaer2019,
	title = {Application of digital nudging in customer journeys – {A} systematic literature review},
	abstract = {More and more decisions are made on screens. Digital nudging attempts to guide users’ decisions on these screens. One relevant application area of digital nudging are customer journeys. Emerging research on this topic mainly analyses digital nudging on companies’ owned conversion screens (i.e. websites). However, in a multi-channel, multi-owner customer journeys (i.e. own website and search engine or social media) there is increasing need to guide users through various digital touchpoints along all stages of the customer journey. This paper examines existing research on the application of digital nudging throughout customer journeys. The literature review reveals that nudging has been considered in customer journey-related literature, but so far with little explicit cross-referencing to nudging and behavioral economics research. The scientific contribution of this paper comprises a synthesis of existing research, identification of research gaps and a research agenda to study the application of digital nudging along the customer journey.},
	journal = {25th Americas Conference on Information Systems, AMCIS 2019},
	author = {Schaer, Armando and Stanoevska-Slabeva, Katarina},
	year = {2019},
	keywords = {Digital nudging, Application of nudging, Customer journey, Literature review, Nudging pipeline},
	pages = {1--10},
	file = {PDF:/home/miguel/Zotero/storage/ZVFM9N6T/2019_DIGITAL NUDGING.pdf:application/pdf},
}

@article{Alzate2017,
	title = {Formal institutions of metropolitan governance in {Colombia}: a {Rational} {Choice} analysis},
	url = {https://www.redalyc.org/articulo.oa?id=67555408007},
	abstract = {This article analyses the necessary conditions for cooperation of neighboring municipalities in the integration of metropolitan areas in Colombia. This is based on a review, under the analytical framework of rational choice, of the main theories concerning metropolitan government, and of the current regulations related to the metropolitan jurisdiction in the country. It has been concluded that the creation of a metropolitan area in Colombia depends on the existence of extraordinary benefits for the participating municipalities, and that the strengthening of metropolitan governance depends either on the maintenance of such benefits or on the strengthening of social accountability mechanisms that could modify policy makers' incentives.},
	journal = {Revista Opera},
	author = {Roldán Alzate, Luis Miguel},
	year = {2017},
	note = {Place: CO},
}

@article{Li2012,
	title = {{DD}-classifier: {Nonparametric} classification procedure based on {DD}-plot},
	volume = {107},
	issn = {01621459},
	doi = {10.1080/01621459.2012.688462},
	abstract = {Using the DD-plot (depth vs. depth plot), we introduce a new nonparametric classification algorithm and call it DD-classifier. The algorithm is completely nonparametric, and it requires no prior knowledge of the underlying distributions or the form of the separating curve. Thus, it can be applied to a wide range of classification problems. The algorithm is completely data driven and its classification outcome can be easily visualized in a two-dimensional plot regardless of the dimension of the data. Moreover, it has the advantage of bypassing the estimation of underlying parameters such as means and scales, which is often required by the existing classification procedures. We study the asymptotic properties of the DD-classifier and its misclassification rate. Specifically, we show that DD-classifier is asymptotically equivalent to the Bayes rule under suitable conditions, and it can achieve Bayes error for a family broader than elliptical distributions. The performance of the classifier is also examined using simulated and real datasets. Overall, the DD-classifier performs well across a broad range of settings, and compares favorably with existing classifiers. It can also be robust against outliers or contamination. © 2012 American Statistical Association.},
	number = {498},
	journal = {Journal of the American Statistical Association},
	author = {Li, Jun and Cuesta-Albertos, Juan A. and Liu, Regina Y.},
	year = {2012},
	keywords = {Classification, Data depth, DD-classifier, DD-plot, Maximum depth classifier, Misclassification rates, Nonparametric, Robustness},
	pages = {737--753},
	file = {PDF:/home/miguel/Zotero/storage/JLKH8FIF/DD Classifier Nonparametric Classification Procedure Based on DD Plot.pdf:application/pdf},
}

@article{Liu1999,
	title = {Multivariate analysis by data depth: {Descriptive} statistics, graphics and inference},
	volume = {27},
	issn = {00905364},
	doi = {10.2307/120138},
	abstract = {A data depth can be used to measure the "depth" or "outlyingness" of a given multivariate sample with respect to its underlying distribution. This leads to a natural center-outward ordering of the sample points. Based on this ordering, quantitative and graphical methods are introduced for analyzing multivariate distributional characteristics such as location, scale, bias, skewness and kurtosis, as well as for comparing inference methods. All graphs are one-dimensional curves in the plane and can be easily visualized and interpreted. A "sunburst plot" is presented as a bivariate generalization of the box-plot. DD-(depth versus depth) plots are proposed and examined as graphical inference tools. Some new diagnostic tools for checking multivariate normality are introduced. One of them monitors the exact rate of growth of the maximum deviation from the mean, while the others examine the ratio of the overall dispersion to the dispersion of a certain central region. The affine invariance property of a data depth also leads to appropriate invariance properties for the proposed statistics and methods.},
	number = {3},
	journal = {Annals of Statistics},
	author = {Liu, Regina Y. and Parelius, Jesse M. and Singh, Kesar},
	year = {1999},
	keywords = {Data depth, Bias, DD-plots, Depth ordering, Depth-L-statistics, Kurtosis, Location, Multivariate descriptive statistics, Multivariate normality, Multivariate ordering, Scale, Skewness, Sunburst plots},
	pages = {783--858},
	file = {PDF:/home/miguel/Zotero/storage/GAGBQEYY/regina.pdf:application/pdf},
}

@article{Zuo2000,
	title = {General {Notions} of {Statistical} {Depth} {Function}},
	volume = {28},
	number = {2},
	journal = {Statistics},
	author = {Zuo, Yijun and Serfling, Robert},
	year = {2000},
	pages = {461--482},
	file = {PDF:/home/miguel/Zotero/storage/TDFG56YM/DepthMultivariate.pdf:application/pdf},
}

@article{noauthor_another_nodate,
	title = {Another approach to polychotomous classification},
	file = {PDF:/home/miguel/Zotero/storage/GSCLBULQ/poly.pdf:application/pdf},
}

@incollection{Jiang2017,
	title = {Two-sample test for multivariate functional data},
	isbn = {978-3-319-55845-5},
	url = {http://link.springer.com/10.1007/978-3-319-55846-2},
	booktitle = {Functional {Statistics} and {Related} {Fields}},
	author = {Jiang, Qing and Meintanis, Simos G. and Zhu, Lixing},
	year = {2017},
	doi = {10.1007/978-3-319-55846-2},
	keywords = {empirical characteristic function, functional data, sample problem, two},
	pages = {145--154},
}

@article{Flores2018a,
	title = {Homogeneity test for functional data},
	volume = {45},
	issn = {13600532},
	doi = {10.1080/02664763.2017.1319470},
	abstract = {In the context of functional data analysis, we propose new sample tests for homogeneity. Based on some well-known depth measures, we construct four different statistics in order to measure distance between the two samples. A simulation study is performed to check the efficiency of the tests when confronted with shape and magnitude perturbation. Finally, we apply these tools to measure the homogeneity in some samples of real data, and we obtain good results using this new method.},
	number = {5},
	journal = {Journal of Applied Statistics},
	author = {Flores, Ramón and Lillo, Rosa and Romo, Juan},
	year = {2018},
	note = {arXiv: 1507.01835},
	keywords = {distance, FDA, Functional depth, homogeneity},
	pages = {868--883},
	file = {PDF:/home/miguel/Zotero/storage/IRHV4EL3/Homogeneity test for functional data.pdf:application/pdf},
}

@article{Nagy2016,
	title = {Statistical {Depth} for {Functional} {Data}},
	number = {October},
	author = {Nagy, Stanislav},
	year = {2016},
	keywords = {phd, thesis, dissertation, doctoraat, kuleuven, St},
}

@article{Calle-saldarriaga2020,
	title = {Homogeneity {Test} for {Functional} {Data} based on {Data}-{Depth} {Plots} .},
	author = {Calle-saldarriaga, Alejandro and Laniado, Henry and Zuluaga, Francisco},
	year = {2020},
	keywords = {acalles, address, affiliation, alejandro calle-saldarriaga, bootstrap-t, carrera 49, co, colombia, contact, department of mathe-, e-mail, eafit, edu, hypothesis testing, ın, matical sciences, medell, n, non-parametric, robust, school of science, universidad eafit},
	pages = {1--25},
	file = {PDF:/home/miguel/Zotero/storage/WNP2D987/alejandro calle saldarriaga_document.pdf:application/pdf},
}

@article{Meneses-Bautista2017,
	title = {Pronóstico del tipo de cambio {USD}/{MXN} con redes neuronales de retropropagación {Forecasting} {USD}/{MXN} {Exchange} {Rate} with {Backpropagation} {Neural} {Networks}},
	volume = {139},
	abstract = {Resumen. El análisis de las series de tiempo permite caracterizar un fenómeno e incluso predecir, con un cierto grado de precisión, su com-portamiento a futuro. La posibilidad de anticipar los movimientos de los mercados resulta sumamente atractiva para los responsables de la toma de decisiones, tanto en la iniciativa privada como en el sector público. En esta investigación se aborda la regresión de series de tiempo del tipo de cambio dólar estadounidense/peso mexicano, empleando un modelo generado por redes neuronales artificiales de retropropagación. Los re-sultados obtenidos ratifican empíricamente las ventajas de la utilización de las redes neuronales en el análisis y pronóstico de series de tiempo financieras, producto de las capacidades de aproximación de funciones y generalización que presentan dichas redes. Palabras clave: pronóstico de tipos de cambio, redes neuronales arti-ficiales, series de tiempo. Abstract. The analysis of time series allows to characterize a phenomenon and even to predict, under a certain degree of precision, its future behavior. The possibility of anticipating market movements is extremely attractive for decision-makers, both in the private sector and in the public sector. This research addresses the regression of time series of the US dollar / Mexican peso exchange rate, using a model generated by backpropagation artificial neural networks. The results obtained ratify empirically the advantages of the use of neural networks in the analysis and forecast of financial time series, as a result of the capabilities of function approximation and generalization that these networks present.},
	author = {Meneses-Bautista, Francisco D and Alvarado, Matías},
	year = {2017},
	keywords = {artificial neural networks, exchange rates forecasting, time series},
	pages = {97--110},
	file = {PDF:/home/miguel/Zotero/storage/CP9AA3QD/m-api-224959a8-d5cb-7cb3-30a4-caca5d01efaf.pdf:application/pdf},
}

@article{Ince2019,
	title = {An {Artificial} {Neural} {Network}-{Based} {Approach} to the {Monetary} {Model} of {Exchange} {Rate}},
	volume = {53},
	issn = {15729974},
	doi = {10.1007/s10614-017-9765-6},
	abstract = {This paper aims to investigate the predictive accuracy of the flexible price monetary model of the exchange rate, estimated by an approach based on combining the vector autoregressive model and multilayer feedforward neural networks. The forecasting performance of this nonlinear, nonparametric model is analyzed comparatively with a monetary model estimated in a linear static framework; the monetary model estimated in a linear dynamic vector autoregressive framework; the monetary model estimated in a parametric nonlinear dynamic threshold vector autoregressive framework; and the naïve random walk model applied to six different exchange rates over three forecasting periods. The models are compared in terms of both the magnitude of their forecast errors and the economic value of their forecasts. The proposed model yielded promising outcomes by performing better than the random walk model in 16 out of 18 instances in terms of the root mean square error and 15 out of 18 instances in terms of mean return and Sharpe ratio. The model also performed better than linear models in 17 out of 18 instances for root mean square error and 14 out of 18 instances for mean returns and Sharpe ratio. The distinguishing feature of the proposed model versus the present models in the literature is its robustness to outperform the random walk model, regardless of whether the magnitude of forecast errors or the economic value of the forecasts is chosen as a performance measure.},
	number = {2},
	journal = {Computational Economics},
	author = {Ince, Huseyin and Cebeci, Ali Fehim and Imamoglu, Salih Zeki},
	year = {2019},
	note = {Publisher: Springer US},
	keywords = {Artificial neural networks, Exchange rate forecasting, Monetary model},
	pages = {817--831},
	file = {PDF:/home/miguel/Zotero/storage/EMUG34MD/2019_Ince.pdf:application/pdf},
}

@article{Henriquez2019,
	title = {A combined {Independent} {Component} {Analysis}–{Neural} {Network} model for forecasting exchange rate variation},
	volume = {83},
	issn = {15684946},
	url = {https://doi.org/10.1016/j.asoc.2019.105654},
	doi = {10.1016/j.asoc.2019.105654},
	abstract = {The currency market is one of the most efficient markets, making it very difficult to predict future prices. Several studies have sought to develop more accurate models to predict the future exchange rate by analyzing econometric models, developing artificial intelligence models and combining both through the creation of hybrid models. This paper proposes a hybrid model for forecasting the variations of five exchange rates related to the US Dollar: Euro, British Pound, Japanese Yen, Swiss Franc and Canadian Dollar. The proposed model uses Independent Component Analysis (ICA) to deconstruct the series into independent components as well as neural networks (NN) to predict each component. This method differentiates this study from previous works where ICA has been used to extract the noise of time series or used to obtain explanatory variables that are then used in forecasting. The proposed model is then compared to random walk, autoregressive and conditional variance models, neural networks, recurrent neural networks and long–short term memory neural networks. The hypothesis of this study supposes that first deconstructing the exchange rate series and then predicting it separately would produce better forecasts than traditional models. By using the mean squared error and mean absolute percentage error as a measures of performance and Model Confidence Sets to statistically test the superiority of the proposed model, our results showed that this model outperformed the other models examined and significantly improved the accuracy of forecasts. These findings support this model's use in future research and in decision-making related to investments.},
	journal = {Applied Soft Computing Journal},
	author = {Henríquez, Jonatan and Kristjanpoller, Werner},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Exchange rate, Hybrid model, Independent Component Analysis, Neural networks, Time series forecasting},
	pages = {105654},
	file = {PDF:/home/miguel/Zotero/storage/CSLI2IMP/2010_Henríquez.pdf:application/pdf},
}

@article{Wu2019,
	title = {Application of support vector neural network with variational mode decomposition for exchange rate forecasting},
	volume = {23},
	issn = {14337479},
	url = {https://doi.org/10.1007/s00500-018-3336-1},
	doi = {10.1007/s00500-018-3336-1},
	abstract = {A hybrid ensemble learning approach is proposed for exchange rate forecasting combining variational mode decomposition (VMD) and support vector neural network (SVNN). First, VMD is employed to decompose the original exchange rate time series into several components. Then, SVNN is adopted to forecast different component series. In the end, the forecasting results of all the components are combined using SVNN as ensemble learning method to obtain the ensemble results. Four major daily exchange rate datasets are selected for model evaluation and comparison. The empirical study demonstrates that the proposed VMD–SVNN ensemble learning approach outperforms other single forecasting models and other ensemble learning approaches in terms of both level forecasting accuracy and directional forecasting accuracy. This suggests that the VMD–SVNN ensemble learning approach is a highly promising approach for exchange rates forecasting with high volatility and irregularity.},
	number = {16},
	journal = {Soft Computing},
	author = {Wu, Yungao and Gao, Jianwei},
	year = {2019},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 0050001833},
	keywords = {Ensemble learning, Exchange rates forecasting, Support vector neural network, Variational mode decomposition},
	pages = {6995--7004},
	file = {PDF:/home/miguel/Zotero/storage/LQAUNU7J/2018_Yungao.pdf:application/pdf},
}

@article{Zhai2020,
	title = {A neural network enhanced volatility component model},
	volume = {7688},
	issn = {14697696},
	doi = {10.1080/14697688.2019.1711148},
	abstract = {Volatility prediction, a central issue in financial econometrics, attracts increasing attention in the data science literature as advances in computational methods enable us to develop models with great forecasting precision. In this paper, we draw upon both strands of the literature and develop a novel two-component volatility model. The realized volatility is decomposed by a nonparametric filter into long- and short-run components, which are modeled by an artificial neural network and an ARMA process, respectively. We use intraday data on four major exchange rates and a Chinese stock index to construct daily realized volatility and perform out-of-sample evaluation of volatility forecasts generated by our model and well-established alternatives. Empirical results show that our model outperforms alternative models across all statistical metrics and over different forecasting horizons. Furthermore, volatility forecasts from our model offer economic gain to a mean-variance utility investor with higher portfolio returns and Sharpe ratio.},
	journal = {Quantitative Finance},
	author = {Zhai, Jia and Cao, Yi and Liu, Xiaoquan},
	year = {2020},
	note = {Publisher: Routledge},
	keywords = {ARMA process, Exchange rates, Volatility prediction, Wavelet analysis},
	file = {PDF:/home/miguel/Zotero/storage/KCAD6BNG/2020_Zhai.pdf:application/pdf},
}

@article{Waheeb2019,
	title = {A new genetically optimized tensor product functional link neural network: an application to the daily exchange rate forecasting},
	volume = {12},
	issn = {18645917},
	url = {https://doi.org/10.1007/s12065-019-00261-2},
	doi = {10.1007/s12065-019-00261-2},
	abstract = {The training speed for multilayer neural networks is slow due to the multilayering. Therefore, removing the hidden layers, provided that the input layer is endowed with additional higher order units is suggested to avoid such problem. Tensor product functional link neural network (TPFLNN) is a single layer with higher order terms that extend the network’s structure by introducing supplementary inputs to the network (i.e., joint activations). Although the structure of the TPFLNN is simple, it suffers from weight combinatorial explosion problem when its order becomes excessively high. Furthermore, similarly to many neural network methods, selection of proper weights is one of the most challenging issues in the TPFLNN. Finding suitable weights could help to reduce the number of needed weights. Therefore, in this study, the genetic algorithm (GA) was used to find near-optimum weights for the TPFLNN. The proposed method is abbreviated as GA–TPFLNN. The GA–TPFLNN was used to forecast the daily exchange rate for the Euro/US Dollar, and Japanese Yen/US Dollar. Simulation results showed that the GA–TPFLNN produced more accurate forecasts as compared to the standard TPFLNN, GA, GA–TPFLNN with backpropagation, GA-functional expansion FLNN, multilayer perceptron, support vector regression, random forests for regression, and naive methods. The GA helps the TPFLNN to find low complexity network structure and/or near-optimum parameters which leads to this better result.},
	number = {4},
	journal = {Evolutionary Intelligence},
	author = {Waheeb, Waddah and Ghazali, Rozaida},
	year = {2019},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 0123456789},
	keywords = {Exchange rate, Forecasting, Functional link neural network, Genetic algorithm, Time series},
	pages = {593--608},
	file = {PDF:/home/miguel/Zotero/storage/J3G4G8WC/2019_Waheeb.pdf:application/pdf},
}

@article{Sun2020,
	title = {Interval forecasting of exchange rates: a new interval decomposition ensemble approach},
	volume = {ahead-of-p},
	issn = {0263-5577},
	doi = {10.1108/imds-03-2019-0194},
	number = {ahead-of-print},
	journal = {Industrial Management \& Data Systems},
	author = {Sun, Shaolong and Wang, Shouyang and Wei, Yunjie},
	year = {2020},
	keywords = {autoregressive model, bivariate empirical mode decomposition, conflict of interests, exchange rate forecasting, interval-valued data, neural networks, of interests regarding the, paper type research paper, publication of, the authors declare that, there is no conflict},
	file = {PDF:/home/miguel/Zotero/storage/2RCTL3X5/2019_Sun.pdf:application/pdf},
}

@article{Jalil2006,
	title = {Evaluación de pronósticos del tipo de cambio utilizando redes neuronales y funciones de pérdida asimétricas},
	author = {Jalil, Munir and Misas, Martha},
	year = {2006},
	keywords = {evaluación de pronóstico, redes neuronales artificiales, tipo de cambio},
	file = {PDF:/home/miguel/Zotero/storage/9UTLAQIT/2006_Jalil_Misas.pdf:application/pdf},
}

@article{Gharleghi2014,
	title = {Predicting exchange rates using a novel "cointegration based neuro-fuzzy system"},
	volume = {137},
	issn = {21107017},
	doi = {10.1016/j.inteco.2013.12.001},
	abstract = {The present study focuses upon the applications of currently available intelligence techniques to forecast exchange rates in short and long horizons. The predictability of exchange rate returns is investigated through the use of a novel cointegration-based neuro-fuzzy system, which is a combination of a cointegration technique; a Fuzzy Inference System; and Artificial Neural Networks. The Relative Price Monetary Model for exchange rate determination is used to determine the inputs, consisting of macroeconomic variables and the type of interactions amongst the variables, in order to develop the system. Considering exchange rate returns of three ASEAN countries (Malaysia, the Philippines and Singapore), our results reveal that the cointegration-based neuro-fuzzy system model consistently outperforms the Vector Error Correction Model by successfully forecasting exchange rate monthly returns with a high level of accuracy. © 2013 CEPII (Centre d'Etudes Prospectives et d'Informations Internationales), a center for research and expertise on the world economy.},
	journal = {International Economics},
	author = {Gharleghi, Behrooz and Hassan Shaari, Abu and Shafighi, Najla},
	year = {2014},
	keywords = {Exchange rate, Neural networks, Error correction model, Intelligence systems, Unit root},
	pages = {88--103},
	file = {PDF:/home/miguel/Zotero/storage/FAZ9A595/2018_Gharleghi.pdf:application/pdf},
}

@article{ZapataGarrido2008,
	title = {Predicción del tipo de cambio peso-dólar utilizando {Redes} {Neuronales} {Artificiales} (rna)},
	issn = {1657-6276},
	number = {24},
	journal = {Pensamiento \& Gestión},
	author = {Zapata Garrido, Luis Alberto and Díaz Mojica, Hugo Fabián},
	year = {2008},
	keywords = {Artificial prediction of the type of change, Neuronal Networks, Predicción del tipo de cambio, Redes Neuronales Artificiales},
	pages = {29--42},
	file = {PDF:/home/miguel/Zotero/storage/8MS2X5QX/2007_Zapata.pdf:application/pdf},
}

@article{Parisi2003,
	title = {Modelos de redes neuronales aplicados a la predicción del tipo de cambio del dólar observado en {Chile}},
	journal = {Estudios de Administración},
	author = {Parisi, Antonino and Parisi, Franco and Guerrero, José Luis},
	year = {2003},
	file = {PDF:/home/miguel/Zotero/storage/METUUKVQ/Modelo de Redes Neuronales.pdf:application/pdf},
}

@article{OquendoPatino2012,
	title = {Redes {Neuronales} {Artificiales} en las {Ciencias} {Económicas}},
	url = {https://www.rics.org/south-asia/upholding-professional-standards/standards-of-conduct/ethics/},
	author = {Oquendo Patiño, Viviana María},
	year = {2012},
	keywords = {así como a los, con el fin de, constituirse como una metodología, contrastar los resultados obtenidos, de redes neuronales artificiales, económicas, en que ésta puede, para la predicción en, pueden realizar estás implementaciones, se, se ajusta un modelo, series de tiempo, software en los que, y mostrar la manera},
	pages = {1--24},
	file = {PDF:/home/miguel/Zotero/storage/MUUGAZGD/2012_Oquendo.pdf:application/pdf},
}

@article{Rifai2019,
	title = {Optimized fuzzy backpropagation neural network using genetic algorithm for predicting indonesian stock exchange composite index},
	issn = {2407439X},
	doi = {10.23919/EECSI48112.2019.8977058},
	abstract = {Investment activities in the capital market have the possibility to generate profits and at the same time also cause losses. The composite stock price index as an indicator used to determine investment continues to change over time. Uncertainty of stock exchange composite index requires investors to be able to make predictions so as to produce maximum profits. The aim of this study is to forecast the composite stock price index. The input variables used are Indonesia interest rates, rupiah exchange rates, Dow Jones index, and world gold prices. All data obtained in the period from January 2008 to March 2019. Data are used to build the Fuzzy Backpropagation Neural Network (FBPNN), model. The weight of FBPNN model was optimized using Genetic Algorithm then used to forecast the composite stock price index. The forecasting result of the composite stock price index for April to June 2019 respectively were 5822.6, 5826.8, and 5767.3 with the MAPE value of 8.42\%. These results indicate that Indonesia interest rates, rupiah exchange rate, Dow Jones index, and the gold price are the proper indicators to predict the composite stock price index.},
	journal = {International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)},
	author = {Rifa’i, Anwar and Mahdiana, Deni},
	year = {2019},
	note = {ISBN: 9786020737287},
	keywords = {Genetic algorithm, Fuzzy backpropagation neural network, Stock exchange composite index},
	pages = {195--199},
	file = {PDF:/home/miguel/Zotero/storage/8FWTE34F/08977058-2.pdf:application/pdf},
}

@article{Srivastava2019,
	title = {Estimation of air pollution in {Delhi} using machine learning techniques},
	doi = {10.1109/GUCON.2018.8675022},
	abstract = {Urban air pollution prediction becomes an indispensable alternative to curb its detrimental consequences. Numerous machine learning techniques have been adopted to forecast the air quality. In this paper, we implemented different classification and regression techniques like Linear Regression, SDG Regression, Random Forest Regression, Decision Tree Regression, Support Vector Regression, Artificial Neural Networks, Gradient Boosting Regression and Adaptive Boosting Regression to forecast the Air Quality Index of major pollutants like PM2.5, PM10, CO, NO2, SO2 and O3. The techniques are then evaluated using Mean square error, Mean absolute error and R2, which show that Support Vector Regression and Artificial Neural Networks are best suited for predicting the air quality in New Delhi.},
	journal = {2018 International Conference on Computing, Power and Communication Technologies, GUCON 2018},
	author = {Srivastava, Chavi and Singh, Shyamli and Singh, Amit Prakash},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538644911},
	keywords = {Forecasting, Air pollution monitoring, Artificial intelligence, Machine learning, Predictive models, Regression},
	pages = {304--309},
	file = {PDF:/home/miguel/Zotero/storage/WPHDGEFQ/08675022.pdf:application/pdf},
}

@article{Monia2019,
	title = {Predictive {Analysis} of {Air} {Pollution} {Using} {Collaborative} {Filtering} {Prediction} {Algorithm}},
	doi = {10.1109/ICECCT.2019.8869266},
	abstract = {Growing trends in Air pollution is possessing threat to environment. Various Researchers have extended their work in predicting air pollution using various predictive analytics. In this paper, we are implementing a predictive model for monitoring air pollution level in different cities of India and publishing it as a web service.The algorithm being used is Collaborative Filtering Prediction Algorithm. A comparison has also been carried out in different predictive analytics mainly using Machine Learning techniques such as regression and Deep Learning Technique and Collaborative filtering technique.},
	journal = {Proceedings of 2019 3rd IEEE International Conference on Electrical, Computer and Communication Technologies, ICECCT 2019},
	author = {{Monia} and Gupta, Akanksha and Sharma, Sameiksha},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538681572},
	keywords = {Collaborative filtering, Deep learning, Heuristics, Pearson Coefficient},
	pages = {1--8},
	file = {PDF:/home/miguel/Zotero/storage/LZUMI8AX/08869266.pdf:application/pdf},
}

@article{Sinnott2019,
	title = {Prediction of {Air} {Pollution} through {Machine} {Learning} {Approaches} on the {Cloud}},
	doi = {10.1109/BDCAT.2018.00015},
	abstract = {Prediction of pollution is an increasingly important problem. It can impact individuals and their health, e.g. asthma patients can be greatly affected by air pollution. Traditional air pollution prediction methods have limitations. Machine learning provides one approach that can offer new opportunities for prediction of air pollution. There are however many different machine learning approaches and identifying the best one for the problem at hand is often challenging. In this paper air pollution data, specifically particulate matter of less than 2.5 micrometers (PM2.5) was collected from a variety of web-based resources and following, data cleansing analysed with different machine learning models including linear regression, Artificial Neural Networks and Long Short Term Memory recurrent neural networks. We consider the accuracy and the ability of these different models to predict unhealthy levels of pollution. The advantages and disadvantages of these models are also discussed.},
	journal = {Proceedings - 5th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies, BDCAT 2018},
	author = {Sinnott, Richard O. and Guan, Ziyue},
	year = {2019},
	note = {ISBN: 9781538655023},
	keywords = {air pollution, cloud computing, data analytics, PM2.5, traffic},
	pages = {51--60},
	file = {PDF:/home/miguel/Zotero/storage/4N38C3P3/08606635.pdf:application/pdf},
}

@article{Hu2017,
	title = {{HazeEst}: {Machine} {Learning} {Based} {Metropolitan} {Air} {Pollution} {Estimation} from {Fixed} and {Mobile} {Sensors}},
	volume = {17},
	issn = {1530437X},
	doi = {10.1109/JSEN.2017.2690975},
	abstract = {Metropolitan air pollution is a growing concern in both developing and developed countries. Fixed-station monitors, typically operated by governments, offer accurate but sparse data, and are increasingly being augmented by lower fidelity but denser measurements taken by mobile sensors carried by concerned citizens and researchers. In this paper, we introduce HazeEst - a machine learning model that combines sparse fixed-station data with dense mobile sensor data to estimate the air pollution surface for any given hour on any given day in Sydney. We assess our system using seven regression models and tenfold cross validation. The results show that estimation accuracy of support vector regression (SVR) is similar to decision tree regression and random forest regression, and higher than extreme gradient boosting, multi-layer perceptrons, linear regression, and adaptive boosting regression. The air pollution estimates from our models are validated via field trials, and results show that SVR not only yields high spatial resolution estimates that correspond well with the pollution surface obtained from fixed and mobile sensor monitoring systems, but also indicates boundaries of polluted area better than other regression models. Our results can be visualized using a Web-based application customized for metropolitan Sydney. We believe that the continuous estimates provided by our system can better inform air pollution exposure and its impact on human health.},
	number = {11},
	journal = {IEEE Sensors Journal},
	author = {Hu, Ke and Rahman, Ashfaqur and Bhrugubanda, Hari and Sivaraman, Vijay},
	year = {2017},
	keywords = {machine learning, Air pollution monitoring, support vector regression, web application, wireless sensor network},
	pages = {3517--3525},
	file = {PDF:/home/miguel/Zotero/storage/Q9XERHIN/07892954.pdf:application/pdf},
}

@article{Xi2015,
	title = {A comprehensive evaluation of air pollution prediction improvement by a machine learning method},
	doi = {10.1109/SOLI.2015.7367615},
	abstract = {Urban air pollution prediction is one of the most important tasks in the treatment of urban air pollution. Due to the disadvantage that source list updated not in time for WRF-Chem which is a numeric model, the prediction result may be not good enough. In this paper, we take full advantages of forecast on pollution, weather, chemical component from WRF-Chem model as input features, design a comprehensive evaluation framework to improve the prediction performance. Experiments are implemented with different features groups and classification algorithms in machine learning method for 74 cities in China, to find the best model for each city. From experiments, for different city, the best result can be obtained by different group of feature selection and model selection. Experimental results indicate that the more feature we used, the more possibility to enhance the accuracy. For method aspect, the result from combined model is better than the unique model.},
	journal = {10th IEEE Int. Conf. on Service Operations and Logistics, and Informatics, SOLI 2015 - In conjunction with ICT4ALL 2015},
	author = {Xi, Xia and Wei, Zhao and Xiaoguang, Rui and Yijie, Wang and Xinxin, Bai and Wenjun, Yin and Jin, Don},
	year = {2015},
	note = {Publisher: IEEE
ISBN: 9781467384803},
	keywords = {machine learning, air pollutation prediction, air quality index prediction, combined method},
	pages = {176--181},
	file = {PDF:/home/miguel/Zotero/storage/QTUDI6GR/07367615.pdf:application/pdf},
}

@article{Eldakhly2018,
	title = {A {Novel} {Approach} of {Weighted} {Support} {Vector} {Machine} with {Applied} {Chance} {Theory} for {Forecasting} {Air} {Pollution} {Phenomenon} in {Egypt}},
	volume = {17},
	issn = {14690268},
	doi = {10.1142/S1469026818500013},
	abstract = {The particulate matter air pollutant of diameter less than 10 micrometers (PM10), a category of pollutants including solid and liquid particles, can be a health hazard for several reasons: it can harm lung tissues and throat, aggravate asthma and increase respiratory illness. Accurate prediction models of PM10 concentrations are essential for proper management, control, and making public warning strategies. Therefore, machine learning techniques have the capability to develop methods or tools that can be used to discover unseen patterns from given data to solve a particular task or problem. The chance theory has advanced concepts pertinent to treat cases where both randomness and fuzziness play simultaneous roles at one time. The main objective is to study the modification of a single machine learning algorithm - support vector machine (SVM) - applying the chance weight of the target variable, based on the chance theory, to the corresponding dataset point to be superior to the ensemble machine learning algorithms. The results of this study are outperforming of the SVM algorithms when modifying and combining with the right theory/technique, especially the chance theory over other modern ensemble learning algorithms.},
	number = {1},
	journal = {International Journal of Computational Intelligence and Applications},
	author = {Eldakhly, Nabil Mohamed and Aboul-Ela, Magdy and Abdalla, Areeg},
	year = {2018},
	keywords = {chance theory, ensemble machine learning algorithms, PM 1 0 pollutant, Single machine learning algorithm, weighted learning algorithms},
	pages = {1--29},
	file = {PDF:/home/miguel/Zotero/storage/3EERERUF/s1469026818500013.pdf:application/pdf},
}

@article{Bellinger2017,
	title = {A systematic review of data mining and machine learning for air pollution epidemiology},
	volume = {17},
	issn = {14712458},
	doi = {10.1186/s12889-017-4914-3},
	abstract = {Background: Data measuring airborne pollutants, public health and environmental factors are increasingly being stored and merged. These big datasets offer great potential, but also challenge traditional epidemiological methods. This has motivated the exploration of alternative methods to make predictions, find patterns and extract information. To this end, data mining and machine learning algorithms are increasingly being applied to air pollution epidemiology. Methods: We conducted a systematic literature review on the application of data mining and machine learning methods in air pollution epidemiology. We carried out our search process in PubMed, the MEDLINE database and Google Scholar. Research articles applying data mining and machine learning methods to air pollution epidemiology were queried and reviewed. Results: Our search queries resulted in 400 research articles. Our fine-grained analysis employed our inclusion/exclusion criteria to reduce the results to 47 articles, which we separate into three primary areas of interest: 1) source apportionment; 2) forecasting/prediction of air pollution/quality or exposure; and 3) generating hypotheses. Early applications had a preference for artificial neural networks. In more recent work, decision trees, support vector machines, k-means clustering and the APRIORI algorithm have been widely applied. Our survey shows that the majority of the research has been conducted in Europe, China and the USA, and that data mining is becoming an increasingly common tool in environmental health. For potential new directions, we have identified that deep learning and geo-spacial pattern mining are two burgeoning areas of data mining that have good potential for future applications in air pollution epidemiology. Conclusions: We carried out a systematic review identifying the current trends, challenges and new directions to explore in the application of data mining methods to air pollution epidemiology. This work shows that data mining is increasingly being applied in air pollution epidemiology. The potential to support air pollution epidemiology continues to grow with advancements in data mining related to temporal and geo-spacial mining, and deep learning. This is further supported by new sensors and storage mediums that enable larger, better quality data. This suggests that many more fruitful applications can be expected in the future.},
	number = {1},
	journal = {BMC Public Health},
	author = {Bellinger, Colin and Mohomed Jabbar, Mohomed Shazan and Zaïane, Osmar and Osornio-Vargas, Alvaro},
	year = {2017},
	note = {Publisher: BMC Public Health},
	keywords = {Big data, Data mining, Machine learning, Air pollution, Association mining, Epidemiology, Exposure},
	pages = {1--19},
	file = {PDF:/home/miguel/Zotero/storage/B5E4GS99/s12889-017-4914-3.pdf:application/pdf},
}

@article{OMS2005,
	title = {Actualización mundial 2005},
	volume = {5},
	abstract = {Se considera que el aire limpio es un requisito básico de la salud y el bienestar humanos. Sin embargo, su contaminación sigue representando una amenaza importante para la salud en todo el mundo. Según una evaluación de la OMS de la carga de enfermedad debida a la contaminación del aire, son más de dos millones las muertes prematuras que se pueden atribuir cada año a los efectos de la contaminación del aire en espacios abiertos urbanos y en espacios cerrados (produ- cida por la quema de combustibles sólidos). Más de la mitad de esta carga de enfermedad recae en las poblaciones de los países en desarrollo.},
	number = {1},
	journal = {Guías de calidad del aire de la OMS relativas al material particulado, el ozono, el dióxido de nitrógeno y el dióxido de azufre Actualización},
	author = {{OMS}},
	year = {2005},
	pages = {1--21},
	file = {PDF:/home/miguel/Zotero/storage/GYXW75W5/OMS_Guias de calidad del aire.pdf:application/pdf},
}

@article{Papaleonidas2013,
	title = {Neurocomputing techniques to dynamically forecast spatiotemporal air pollution data},
	volume = {4},
	issn = {18686478},
	doi = {10.1007/s12530-013-9078-5},
	abstract = {Real time monitoring, forecasting and modeling air pollutants' concentrations in major urban centers is one of the top priorities of all local and national authorities globally. This paper studies and analyzes the parameters related to the problem, aiming in the design and development of an effective machine learning model and its corresponding system, capable of forecasting dangerous levels of ozone (O3) concentrations in the city center of Athens and more specifically in the "Athinas" air quality monitoring station. This is a multi parametric case, so an effort has been made to combine a vast number of data vectors from several operational nearby measurements' stations. The final result was the design and construction of a group of artificial neural networks capable of estimating O3 concentrations in real time mode and also having the capacity of forecasting the same values for future time intervals of 1, 2, 3 and 6 h, respectively. © 2013 Springer-Verlag Berlin Heidelberg.},
	number = {4},
	journal = {Evolving Systems},
	author = {Papaleonidas, Antonios and Iliadis, Lazaros},
	year = {2013},
	keywords = {Artificial neural networks, Machine learning, Multi parametric ANN, Ozone estimation and forecasting, Pollution of the atmosphere},
	pages = {221--233},
	file = {PDF:/home/miguel/Zotero/storage/THFPTJZH/Papaleonidas-Iliadis2013_Article_NeurocomputingTechniquesToDyna.pdf:application/pdf},
}

@article{Martinez-Espana2018,
	title = {Air-pollution prediction in smart cities through machine learning methods: {A} case of study in {Murcia}, {Spain}},
	volume = {24},
	issn = {09486968},
	abstract = {Air-pollution is one of the main threats for developed societies. According to the World Health Organization (WHO), pollution is the main cause of deaths among children aged under five. Smart cities are called to play a decisive role to improve such pollution by first collecting, in real-time, different parameters such as SO2, NOx, O3, NH3, CO, PM10, just to mention a few, and then performing the subsequent data analysis and prediction. However, some machine learning techniques may be more well-suited than others to predict pollution-like variables. In this paper several machine learning methods are analyzed to predict the ozone level (O3) in the Region of Murcia (Spain). O3 is one of the main hazards to health when it reaches certain levels. Indeed, having accurate air-quality prediction models is a previous step to take mitigation activities that may benefit people with respiratory disease like Asthma, Bronchitis or Pneumonia in intelligent cities. Moreover, here it is identified the most-significant variables to monitor the air-quality in cities. Our results indicate an adjustment for the proposed O3 prediction models from 90\% and a root mean square error less than 11 µ/m3 for the cities of the Region of Murcia involved in the study.},
	number = {3},
	journal = {Journal of Universal Computer Science},
	author = {Martínez-España, Raquel and Bueno-Crespo, Andrés and Timón, Isabel and Soto, Jesús and Muñoz, Andrés and Cecilia, José M.},
	year = {2018},
	keywords = {Machine learning, Air-pollution monitoring, Hierarchical clustering, Ozone, Random forest, Smart cities},
	pages = {261--276},
	file = {PDF:/home/miguel/Zotero/storage/FNH5HGJM/Martinez et al 2018.pdf:application/pdf},
}

@article{Delavar2019,
	title = {A novel method for improving air pollution prediction based on machine learning approaches: {A} case study applied to the capital city of {Tehran}},
	volume = {8},
	issn = {22209964},
	doi = {10.3390/ijgi8020099},
	abstract = {Environmental pollution has mainly been attributed to urbanization and industrial developments across the globe. Air pollution has been marked as one of the major problems of metropolitan areas around the world, especially in Tehran, the capital of Iran, where its administrators and residents have long been struggling with air pollution damage such as the health issues of its citizens. As far as the study area of this research is concerned, a considerable proportion of Tehran air pollution is attributed to PM10 and PM2.5 pollutants. Therefore, the present study was conducted to determine the prediction models to determine air pollutions based on PM10 and PM2.5 pollution concentrations in Tehran. To predict the air-pollution, the data related to day of week, month of year, topography, meteorology, and pollutant rate of two nearest neighbors as the input parameters and machine learning methods were used. These methods include a regression support vector machine, geographically weighted regression, artificial neural network and auto-regressive nonlinear neural network with an external input as the machine learning method for the air pollution prediction. A prediction model was then proposed to improve the afore-mentioned methods, by which the error percentage has been reduced and improved by 57\%, 47\%, 47\% and 94\%, respectively. The most reliable algorithm for the prediction of air pollution was autoregressive nonlinear neural network with external input using the proposed prediction model, where its one-day prediction error reached 1.79 µg/m3. Finally, using genetic algorithm, data for day of week, month of year, topography, wind direction, maximum temperature and pollutant rate of the two nearest neighbors were identified as the most effective parameters in the prediction of air pollution.},
	number = {2},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Delavar, Mahmoud Reza and Gholami, Amin and Shiran, Gholam Reza and Rashidi, Yousef and Nakhaeizadeh, Gholam Reza and Fedra, Kurt and Afshar, Smaeil Hatefi},
	year = {2019},
	keywords = {Genetic algorithm, Machine learning, Air pollution, Artificial neural network, Auto-regressive nonlinear neural, Geographically weighted regression, Interpolation, Prediction, Regression SVM},
	file = {PDF:/home/miguel/Zotero/storage/J8RAF476/ijgi-08-00099-v3.pdf:application/pdf},
}

@article{noauthor_cm0868_nodate,
	title = {{CM0868} {GUÍA} {DEL} {PROYECTO} {Trabajo} 2 : {Ejecución} del plan inicial},
	file = {PDF:/home/miguel/Zotero/storage/PSAQ93XX/proyecto.pdf:application/pdf},
}

@article{Qi2008,
	title = {Trend {Time} – {Series} {Modeling} and {Forecasting} {With} {Neural} {Networks}},
	volume = {19},
	number = {5},
	author = {Qi, Min and Zhang, G Peter},
	year = {2008},
	pages = {808--816},
	file = {PDF:/home/miguel/Zotero/storage/NSKY3Q7U/IEEE_TNN_2008.pdf:application/pdf},
}

@article{Krzysko2020,
	title = {Robust multivariate functional discriminant coordinates},
	volume = {49},
	issn = {15324141},
	url = {https://doi.org/10.1080/03610918.2019.1580731},
	doi = {10.1080/03610918.2019.1580731},
	abstract = {In this paper, we consider the discriminant coordinates for multivariate functional data and their application to classification. We present more general construction of the multivariate functional discriminant coordinates than that known in the literature. The construction is based on basis expansion of functional data. To overcome non-robustness of classical estimators, we also propose robust estimation methods of unknown parameters. The constructed classification rules for multivariate functional data based on the linear discriminant analysis and standard and robust discriminant coordinates are compared on simulation study and on real data set. The results indicate possible usefulness of proposed methods in practice.},
	number = {3},
	journal = {Communications in Statistics: Simulation and Computation},
	author = {Krzyśko, Mirosław and Smaga, Łukasz},
	year = {2020},
	note = {Publisher: Taylor \& Francis},
	keywords = {Discriminant coordinates, Linear discriminant analysis, Multivariate functional data analysis, Robust location and scatter estimation},
	pages = {717--733},
	file = {PDF:/home/miguel/Zotero/storage/NNP93Z7Z/Robust multivariate functional discriminant coordinates.pdf:application/pdf},
}

@article{Gorecki2020,
	title = {Independence test and canonical correlation analysis based on the alignment between kernel matrices for multivariate functional data},
	volume = {53},
	issn = {15737462},
	doi = {10.1007/s10462-018-9666-7},
	abstract = {In the case of vector data, Gretton et al. (Algorithmic learning theory. Springer, Berlin, pp 63–77, 2005) defined Hilbert–Schmidt independence criterion, and next Cortes et al. (J Mach Learn Res 13:795–828, 2012) introduced concept of the centered kernel target alignment (KTA). In this paper we generalize these measures of dependence to the case of multivariate functional data. In addition, based on these measures between two kernel matrices (we use the Gaussian kernel), we constructed independence test and nonlinear canonical variables for multivariate functional data. We show that it is enough to work only on the coefficients of a series expansion of the underlying processes. In order to provide a comprehensive comparison, we conducted a set of experiments, testing effectiveness on two real examples and artificial data. Our experiments show that using functional variants of the proposed measures, we obtain much better results in recognizing nonlinear dependence.},
	number = {1},
	journal = {Artificial Intelligence Review},
	author = {Górecki, Tomasz and Krzyśko, Mirosław and Wołyński, Waldemar},
	year = {2020},
	keywords = {Multivariate functional data, Canonical correlation analysis, Correlation analysis, Functional data analysis},
	pages = {475--499},
	file = {PDF:/home/miguel/Zotero/storage/45INA4GP/Górecki2020_Article_IndependenceTestAndCanonicalCo.pdf:application/pdf},
}

@article{Dai2019,
	title = {Directional outlyingness for multivariate functional data},
	volume = {131},
	issn = {01679473},
	url = {https://doi.org/10.1016/j.csda.2018.03.017},
	doi = {10.1016/j.csda.2018.03.017},
	abstract = {The direction of outlyingness is crucial to describing the centrality of multivariate functional data. Motivated by this idea, classical depth is generalized to directional outlyingness for functional data. Theoretical properties of functional directional outlyingness are investigated and the total outlyingness can be naturally decomposed into two parts: magnitude outlyingness and shape outlyingness which represent the centrality of a curve for magnitude and shape, respectively. This decomposition serves as a visualization tool for the centrality of curves. Furthermore, an outlier detection procedure is proposed based on functional directional outlyingness. This criterion applies to both univariate and multivariate curves and simulation studies show that it outperforms competing methods. Weather and electrocardiogram data demonstrate the practical application of our proposed framework.},
	journal = {Computational Statistics and Data Analysis},
	author = {Dai, Wenlin and Genton, Marc G.},
	year = {2019},
	note = {arXiv: 1612.04615
Publisher: Elsevier B.V.},
	keywords = {Centrality visualization, Directional outlyingness, Multivariate function data, Outlier detection, Outlyingness decomposition},
	pages = {50--65},
	file = {PDF:/home/miguel/Zotero/storage/4V8UMIB4/Dai_2018.pdf:application/pdf},
}

@article{Dai2018,
	title = {Multivariate {Functional} {Data} {Visualization} and {Outlier} {Detection}},
	volume = {27},
	issn = {15372715},
	url = {https://doi.org/10.1080/10618600.2018.1473781},
	doi = {10.1080/10618600.2018.1473781},
	abstract = {This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate nonoutlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data. Supplementary material for this article is available online.},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Dai, Wenlin and Genton, Marc G.},
	year = {2018},
	note = {arXiv: 1703.06419
Publisher: Taylor \& Francis},
	keywords = {Directional outlyingness, Outlier detection, Data visualization, Functional data, Graphical tool, Magnitude and shape},
	pages = {923--934},
	file = {PDF:/home/miguel/Zotero/storage/62I8G2GK/Dai_201803.pdf:application/pdf},
}

@article{Hubert2015a,
	title = {Multivariate functional outlier detection},
	volume = {24},
	issn = {1613981X},
	url = {http://dx.doi.org/10.1007/s10260-015-0297-8},
	doi = {10.1007/s10260-015-0297-8},
	abstract = {Functional data are occurring more and more often in practice, and various statistical techniques have been developed to analyze them. In this paper we consider multivariate functional data, where for each curve and each time point a \$\$p\$\$p-dimensional vector of measurements is observed. For functional data the study of outlier detection has started only recently, and was mostly limited to univariate curves \$\$(p=1)\$\$(p=1). In this paper we set up a taxonomy of functional outliers, and construct new numerical and graphical techniques for the detection of outliers in multivariate functional data, with univariate curves included as a special case. Our tools include statistical depth functions and distance measures derived from them. The methods we study are affine invariant in \$\$p\$\$p-dimensional space, and do not assume elliptical or any other symmetry.},
	number = {2},
	journal = {Statistical Methods and Applications},
	author = {Hubert, Mia and Rousseeuw, Peter J. and Segaert, Pieter},
	year = {2015},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Robustness, Functional data, Depth, Diagnostic plot, Graphical display, Heatmap},
	pages = {177--202},
	file = {PDF:/home/miguel/Zotero/storage/LB98GEB2/Hubert2015_Article_MultivariateFunctionalOutlierD.pdf:application/pdf},
}

@article{Kuhnt2016,
	title = {An angle-based multivariate functional pseudo-depth for shape outlier detection},
	volume = {146},
	issn = {10957243},
	url = {http://dx.doi.org/10.1016/j.jmva.2015.10.016},
	doi = {10.1016/j.jmva.2015.10.016},
	abstract = {A measure especially designed for detecting shape outliers in functional data is presented. It is based on the tangential angles of the intersections of the centred data and can be interpreted like a data depth. Due to its theoretical properties we call it functional tangential angle (FUNTA) pseudo-depth. Furthermore we introduce a robustification (rFUNTA). The existence of intersection angles is ensured through the centring. Assuming that shape outliers in functional data follow a different pattern, the distribution of intersection angles differs. Furthermore we formulate a population version of FUNTA in the context of Gaussian processes. We determine sample breakdown points of FUNTA and compare its performance with respect to outlier detection in simulation studies and a real data example.},
	journal = {Journal of Multivariate Analysis},
	author = {Kuhnt, Sonja and Rehage, André},
	year = {2016},
	note = {Publisher: Elsevier Inc.},
	keywords = {Data depth, Functional data, Bootstrap, Robust estimate, Shape outlier detection},
	pages = {325--340},
	file = {PDF:/home/miguel/Zotero/storage/47AJ9Q6H/Kuhnt_.pdf:application/pdf},
}

@article{Lopez-Pintado2014,
	title = {Simplicial band depth for multivariate functional data},
	volume = {8},
	issn = {18625355},
	doi = {10.1007/s11634-014-0166-6},
	abstract = {We propose notions of simplicial band depth for multivariate functional data that extend the univariate functional band depth. The proposed simplicial band depths provide simple and natural criteria to measure the centrality of a trajectory within a sample of curves. Based on these depths, a sample of multivariate curves can be ordered from the center outward and order statistics can be defined. Properties of the proposed depths, such as invariance and consistency, can be established. A simulation study shows the robustness of this new definition of depth and the advantages of using a multivariate depth versus the marginal depths for detecting outliers. Real data examples from growth curves and signature data are used to illustrate the performance and usefulness of the proposed depths. © 2014 Springer-Verlag Berlin Heidelberg.},
	number = {3},
	journal = {Advances in Data Analysis and Classification},
	author = {López-Pintado, Sara and Sun, Ying and Lin, Juan K. and Genton, Marc G.},
	year = {2014},
	keywords = {Band depth, Functional and image data, Functional boxplot, Modified band depth, Multivariate, Simplicial},
	pages = {321--338},
	file = {PDF:/home/miguel/Zotero/storage/Q2AY2W7X/Lopez-Pintado, Sun, 2014.pdf:application/pdf},
}

@article{Tarabelloni2015,
	title = {Use of {Depth} {Measure} for {Multivariate} {Functional} {Data} in {Disease} {Prediction}: {An} {Application} to {Electrocardiograph} {Signals}},
	volume = {11},
	issn = {15574679},
	doi = {10.1515/ijb-2014-0041},
	abstract = {In this paper we develop statistical methods to compare two independent samples of multivariate functional data that differ in terms of covariance operators. In particular we generalize the concept of depth measure to this kind of data, exploiting the role of the covariance operators in weighting the components that define the depth. Two simulation studies are carried out to validate the robustness of the proposed methods and to test their effectiveness in some settings of interest. We present an application to Electrocardiographic (ECG) signals aimed at comparing physiological subjects and patients affected by Left Bundle Branch Block. The proposed depth measures computed on data are then used to perform a nonparametric comparison test among these two populations. They are also introduced into a generalized regression model aimed at classifying the ECG signals.},
	number = {2},
	journal = {International Journal of Biostatistics},
	author = {Tarabelloni, Nicholas and Ieva, Francesca and Biasi, Rachele and Maria Paganoni, Anna},
	year = {2015},
	pmid = {26110484},
	keywords = {ECG signals, covariance operators, depth measures, generalized linear models, multivariate functional data},
	pages = {189--201},
	file = {PDF:/home/miguel/Zotero/storage/3WRBFCM5/Tarabelloni_2015.pdf:application/pdf},
}

@article{Febrero-Bande2012a,
	title = {Statistical computing in functional data analysis: {The} {R} package fda.usc},
	volume = {51},
	issn = {15487660},
	doi = {10.18637/jss.v051.i04},
	abstract = {This paper is devoted to the R package fda.usc which includes some utilities for functional data analysis. This package carries out exploratory and descriptive analysis of functional data analyzing its most important features such as depth measurements or functional outliers detection, among others. The R package fda.usc also includes functions to compute functional regression models, with a scalar response and a functional explanatory data via non-parametric functional regression, basis representation or functional principal components analysis. There are natural extensions such as functional linear models and semi-functional partial linear models, which allow non-functional covariates and factors and make predictions. The functions of this package complement and incorporate the two main references of functional data analysis: The R package fda and the functions implemented by Ferraty and Vieu (2006).},
	number = {4},
	journal = {Journal of Statistical Software},
	author = {Febrero-Bande, Manuel and de la Fuente, Manuel Oviedo},
	year = {2012},
	keywords = {Depth measures, Functional data regression, Non-parametric kernel estimation, Outlier, Representation of functional data},
	file = {PDF:/home/miguel/Zotero/storage/IWDMRYP8/v51i04.pdf:application/pdf},
}

@article{Gorecki2018,
	title = {Selected statistical methods of data analysis for multivariate functional data},
	volume = {59},
	issn = {09325026},
	doi = {10.1007/s00362-016-0757-8},
	abstract = {Data in the form of a continuous vector function on a given interval are referred to as multivariate functional data. These data are treated as realizations of multivariate random processes. The paper is devoted to three statistical dimension reduction techniques for multivariate data. For the first one, principal components analysis, the authors present a review of a recent paper (Jacques and Preda in, Comput Stat Data Anal, 71:92–106, 2014). For two others one, canonical variables and discriminant coordinates, the authors extend existing works for univariate functional data to multivariate. These methods for multivariate functional data are presented, illustrated and discussed in the context of analyzing real data sets. Each of these techniques is applied on real data set.},
	number = {1},
	journal = {Statistical Papers},
	author = {Górecki, Tomasz and Krzyśko, Mirosław and Waszak, Łukasz and Wołyński, Waldemar},
	year = {2018},
	keywords = {Multivariate functional data, Discriminant coordinates, Canonical correlation analysis, Functional data analysis, Principal component analysis},
	pages = {153--182},
	file = {PDF:/home/miguel/Zotero/storage/DREST7JJ/Górecki2018_Article_SelectedStatisticalMethodsOfDa.pdf:application/pdf},
}

@article{Virta2020,
	title = {Independent component analysis for multivariate functional data},
	volume = {176},
	issn = {10957243},
	url = {https://doi.org/10.1016/j.jmva.2019.104568},
	doi = {10.1016/j.jmva.2019.104568},
	abstract = {We extend two methods of independent component analysis, fourth order blind identification and joint approximate diagonalization of eigen-matrices, to vector-valued functional data. Multivariate functional data occur naturally and frequently in modern applications, and extending independent component analysis to this setting allows us to distill important information from this type of data, going a step further than the functional principal component analysis. To allow the inversion of the covariance operator we make the assumption that the dependency between the component functions lies in a finite-dimensional subspace. In this subspace we define fourth cross-cumulant operators and use them to construct the two novel, Fisher consistent methods for solving the independent component problem for vector-valued functions. Both simulations and an application on a hand gesture data set show the usefulness and advantages of the proposed methods over functional principal component analysis.},
	journal = {Journal of Multivariate Analysis},
	author = {Virta, Joni and Li, Bing and Nordhausen, Klaus and Oja, Hannu},
	year = {2020},
	note = {arXiv: 1712.07641
Publisher: Elsevier Inc.},
	keywords = {Covariance operator, Dimension reduction, Fourth order blind identification, Functional principal component analysis, Hilbert space, Joint approximate diagonalization of eigenmatrices},
	pages = {104568},
	file = {PDF:/home/miguel/Zotero/storage/9LGC3Y2I/1-s2.0-S0047259X19300223-main.pdf:application/pdf},
}

@article{Ferger2000,
	title = {Optimal {Tests} for the {General} {Two}-{Sample} {Problem}},
	volume = {74},
	issn = {0047259X},
	doi = {10.1006/jmva.1999.1879},
	abstract = {In this paper we consider the classical problem of testing whether two samples of observations are from the same distribution. Since in many situations the data are multivariate or even of functional type, classical methodology is not applicable. In our approach we conceive a difference in distribution as the occurrence of a change-point problem, where the change-point is known in advance. This point of view enables us to construct new tests which are distribution-free under the null hypothesis for general sample spaces. The power function of the tests is studied under local and global alternatives. Finally some optimality results are provided. © 2000 Academic Press.},
	number = {1},
	journal = {Journal of Multivariate Analysis},
	author = {Ferger, Dietmar},
	year = {2000},
	keywords = {Two-sample test, contiguous alternatives, local po},
	pages = {1--35},
	file = {PDF:/home/miguel/Zotero/storage/7WLBEX87/1-s2.0-S0047259X99918791-main.pdf:application/pdf},
}

@article{Hall2007,
	title = {Two-sample tests in functional data analysis starting from discrete data},
	volume = {17},
	issn = {10170405},
	abstract = {One of the ways in which functional data analysis differs from other areas of statistics is in the extent to which data are pre-processed prior to analysis. Functional data are invariably recorded discretely, although they are generally substantially smoothed as a prelude even to viewing by statisticians, let alone further analysis. This has a potential to interfere with the performance of two-sample statistical tests, since the use of different tuning parameters for the smoothing step, or different observation times or subsample sizes (i.e., numbers of observations per curve), can mask the differences between distributions that a test is trying to locate. In this paper, and in the context of two-sample tests, we take up this issue. Ways of pre-processing the data, so as to minimise the effects of smoothing, are suggested. We show theoretically and numerically that, by employing exactly the same tuning parameter (e.g. bandwidth) to produce each curve from its raw data, significant loss of power can be avoided. Provided a common tuning parameter is used, it is often satisfactory to choose that parameter along conventional lines, as though the target was estimation of the continuous functions themselves, rather than testing hypotheses about them. Moreover, in this case, using a second-order smoother (such as a local-linear method), the subsample sizes can be almost as small as the square root of sample sizes before the effects of smoothing have any first-order impact on the results of a two-sample test.},
	number = {4},
	journal = {Statistica Sinica},
	author = {Hall, Peter and Van Keilegom, Ingrid},
	year = {2007},
	keywords = {Bootstrap, Bandwidth, Cramér-von mises test, Curve estimation, Hypothesis testing, Kernel, Local-linear methods, Local-polynomial methods, Nonparametric regression, Smoothing},
	pages = {1511--1531},
	file = {PDF:/home/miguel/Zotero/storage/FZVCD8LS/79c9c66f8b64aa5a5650431f710f5c115208.pdf:application/pdf},
}

@article{Jiang2019,
	title = {Asymptotics, finite-sample comparisons and applications for two-sample tests with functional data},
	volume = {170},
	issn = {10957243},
	url = {https://doi.org/10.1016/j.jmva.2018.09.002},
	doi = {10.1016/j.jmva.2018.09.002},
	abstract = {We consider two-sample tests for functional data with observations which may be uni- or multi-dimensional. The new methods are formulated as L2-type criteria based on empirical characteristic functions and are convenient from the computational point of view. Asymptotic properties are presented. Simulations and two real data applications are conducted in order to evaluate the performance of the proposed tests vis-à-vis other methods.},
	journal = {Journal of Multivariate Analysis},
	author = {Jiang, Qing and Hušková, Marie and Meintanis, Simos G. and Zhu, Lixing},
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Functional data, Empirical characteristic function, Two-sample problem},
	pages = {202--220},
	file = {PDF:/home/miguel/Zotero/storage/CUAB32BU/1-s2.0-S0047259X17307303-main.pdf:application/pdf},
}

@book{Palumbo2018,
	title = {Data {Science}: {Innovative} {Developments} in {Data} {Analysis} and {Clustering}},
	isbn = {978-3-319-55722-9},
	abstract = {This edited volume on the latest advances in data science covers
a wide range of topics in the context of data analysis and
classification. In particular, it includes contributions on
classification methods for high-dimensional data, clustering
methods, multivariate statistical methods, and various
applications. The book gathers a selection of peer-reviewed
contributions presented at the Fifteenth Conference of the
International Federation of Classification Societies (IFCS2015),
which was hosted by the Alma Mater Studiorum, University of
Bologna, from July 5 to 8, 2015.},
	author = {Palumbo, Francesco and Montanari, Angela and Vichi, Maurizio},
	year = {2018},
	doi = {10.30748/soi.2018.153.08},
	note = {Issue: 2(153)
ISSN: 16817710},
	file = {PDF:/home/miguel/Zotero/storage/MVTKY7K8/m-api-64642b5f-1d80-e143-05c6-a439d0b2d343.pdf:application/pdf},
}

@article{Zhang2015,
	title = {Two sample inference for the second-order property of temporally dependent functional data},
	volume = {21},
	issn = {13507265},
	doi = {10.3150/13-BEJ592},
	abstract = {Motivated by the need to statistically quantify the difference between two spatio-temporal datasets that arise in climate downscaling studies, we propose new tests to detect the differences of the covariance operators and their associated characteristics of two functional time series. Our two sample tests are constructed on the basis of functional principal component analysis and self-normalization, the latter of which is a new studentization technique recently developed for the inference of a univariate time series. Compared to the existing tests, our SN-based tests allow for weak dependence within each sample and it is robust to the dependence between the two samples in the case of equal sample sizes. Asymptotic properties of the SNbased test statistics are derived under both the null and local alternatives. Through extensive simulations, our SN-based tests are shown to outperform existing alternatives in size and their powers are found to be respectable. The tests are then applied to the gridded climate model outputs and interpolated observations to detect the difference in their spatial dynamics.},
	number = {2},
	journal = {Bernoulli},
	author = {Zhang, Xianyang and Shao, Xiaofeng},
	year = {2015},
	note = {arXiv: 1506.00847v1},
	keywords = {Time series, Functional data analysis, Climate downscaling, Long run variance matrix, Self-normalization, Two sample problem},
	pages = {909--929},
	file = {PDF:/home/miguel/Zotero/storage/JH6T5G4A/1506.00847.pdf:application/pdf},
}

@article{Febrero-Bande2007,
	title = {A functional analysis of {NOx} levels: location and scale estimation and outlier detection},
	volume = {22},
	issn = {09434062},
	doi = {10.1007/s00180-007-0053-0},
	abstract = {Five notions of data depth are considered. They are mostly designed for functional data but they can be also adapted to the standard multivariate case. The performance of these depth notions, when used as auxiliary tools in estimation and classification, is checked through a Monte Carlo study. © 2007 Springer-Verlag.},
	number = {3},
	journal = {Reports in statistics and operations research},
	author = {Febrero-Bande, Manuel and Galeano, Pedro and González-Manteiga, Wenceslao},
	year = {2007},
	note = {ISBN: 0018000700530},
	keywords = {Depth measures, Functional data, Projections method, Supervised classification},
	pages = {481--496},
	file = {PDF:/home/miguel/Zotero/storage/U2X8HLNA/2007CS-Ms-02-Rev2.pdf:application/pdf},
}

@article{Fraiman2001,
	title = {Trimmed means for functional data},
	volume = {10},
	issn = {11330686},
	doi = {10.1007/BF02595706},
	abstract = {In practice, the use of functional data is often preferable to that of large finite-dimensional vectors obtained by discrete approximations of functions. In this paper a new concept of data depth is introduced for functional data. The aim is to measure the centrality of a given curve within a group of curves. This concept is used to define ranks and trimmed means for functional data. Some theoretical and practical aspects are discussed and a simulation study is given. The results show a good performance of our method, in terms of efficiency and robustness, when compared with the mean. Finally, a real-data example based on the Nasdaq 100 index is discussed.},
	number = {2},
	journal = {Test},
	author = {Fraiman, Ricardo and Muniz, Graciela},
	year = {2001},
	keywords = {Data depth, Functional data, Trimmed means estimates},
	pages = {419--440},
	file = {PDF:/home/miguel/Zotero/storage/QDLB7MKP/Fraiman, Muniz, 2001.pdf:application/pdf},
}

@article{Gorecki2019,
	title = {{fdANOVA}: an {R} software package for analysis of variance for univariate and multivariate functional data},
	volume = {34},
	issn = {16139658},
	url = {https://doi.org/10.1007/s00180-018-0842-7},
	doi = {10.1007/s00180-018-0842-7},
	abstract = {Functional data, i.e., observations represented by curves or functions, frequently arise in various fields. The theory and practice of statistical methods for such data is referred to as functional data analysis (FDA) which is one of major research fields in statistics. The practical use of FDA methods is made possible thanks to availability of specialized and usually free software. In particular, a number of R packages is devoted to these methods. In the paper, we introduce a new R package fdANOVA which provides an access to a broad range of global analysis of variance methods for univariate and multivariate functional data. The implemented testing procedures mainly for homoscedastic case are briefly overviewed and illustrated by examples on a well known functional data set. To reduce the computation time, parallel implementation is developed and its efficiency is empirically evaluated. Since some of the implemented tests have not been compared in terms of size control and power yet, appropriate simulations are also conducted. Their results can help in choosing proper testing procedures in practice.},
	number = {2},
	journal = {Computational Statistics},
	author = {Górecki, Tomasz and Smaga, Łukasz},
	year = {2019},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Functional data, Analysis of variance, fdANOVA, R},
	pages = {571--597},
	file = {PDF:/home/miguel/Zotero/storage/2B2TVXKC/Górecki-Smaga2019_Article_FdANOVAAnRSoftwarePackageForAn.pdf:application/pdf},
}

@article{Hanusz2019,
	title = {Discriminant coordinates analysis for multivariate functional data},
	volume = {0},
	issn = {1532415X},
	url = {https://doi.org/10.1080/03610926.2019.1602650},
	doi = {10.1080/03610926.2019.1602650},
	abstract = {One of the basic statistical methods of dimensionality reduction is analysis of discriminant coordinates given by Fisher (1936) and Rao (1948). The space of discriminant coordinates is a space convenient for presenting multidimensional data originating from multiple groups and for the use of various classification methods (methods of discriminant analysis). In the present paper, we adapt the classical discriminant coordinates analysis to multivariate functional data. The theory has been applied to analysis of textural properties of apples of six varieties, measured over a period of 180 days, stored in two types of refrigeration chamber.},
	number = {0},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Hanusz, Zofia and Krzyśko, Mirosław and Nadulski, Rafał and Waszak, Łukasz},
	year = {2019},
	note = {Publisher: Taylor \& Francis},
	keywords = {Multivariate functional data, discriminant coordinates, multivariate analysis, textural properties of apples},
	pages = {1--14},
	file = {PDF:/home/miguel/Zotero/storage/W3ZF67LN/Discriminant coordinates analysis for multivariate functional data.pdf:application/pdf},
}

@article{Tsukada2019,
	title = {High dimensional two-sample test based on the inter-point distance},
	volume = {34},
	issn = {16139658},
	doi = {10.1007/s00180-017-0777-4},
	abstract = {The multivariate two-sample problem has been extensively investigated, and various methods have been proposed. However, most two-sample tests perform poorly when applied to high-dimensional data, and many of them are not applicable when the dimension of the data exceeds the sample size. We reconsider two previously reported tests (Baringhaus and Franz in Stat Sin 20:1333–1361, 2010; Biswas and Ghosh in J Multivar Anal 123:160–171, 2014), and propose two new criteria. Simulations demonstrate that the power of the proposed test is stable for high-dimensional data and large samples, and the power of our test is equivalent to that of the test by Biswas and Ghosh when the covariance matrices are different. We also investigate the theoretical properties of our test when the dimension tends to infinity and the sample size is fixed, and when the dimension is fixed and the sample size tends to infinity. In these cases, the proposed test is asymptotically distribution-free and consistent.},
	number = {2},
	journal = {Computational Statistics},
	author = {Tsukada, Shin ichi},
	year = {2019},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 0018001707774},
	keywords = {High-dimensional data, Inter-point distance, Nonparametric test, Two-sample},
	pages = {599--615},
	file = {PDF:/home/miguel/Zotero/storage/T4WPX5U7/Tsukada2019_Article_HighDimensionalTwo-sampleTestB.pdf:application/pdf},
}

@article{Pomann,
	title = {Two {Sample} {Hypothesis} {Testing} for {Functional} {Data}},
	abstract = {A nonparametric testing procedure is proposed for testing the hypothesis that two samples of curves observed at discrete grids and with noise have the same underlying distribution. Our method relies on representing the curves using a common orthogonal basis expansion. The approach reduces the dimension of the testing problem in a way that enables the application of traditional non-parametric univariate testing procedures. This procedure is computationally inexpensive, can be easily implemented, and accommodates different sampling designs across the samples. Numerical studies confirm the size and power prop-erties of the test in many realistic scenarios, and furthermore show that the proposed test is more powerful than alternative testing procedures. The pro-posed methodology is illustrated on a state-of-the art diffusion tensor imaging study, where the objective is to compare white matter tract profiles in healthy individuals and multiple sclerosis patients.},
	number = {1986},
	author = {Pomann, Gina-Maria and Staicu, Ana-Maria and Ghosh, Sujit},
	keywords = {Functional data, Functional principal component analysis, Hypothesis testing, Anderson Darling test, Diffusion tensor imaging, Multiple Sclerosis},
	file = {PDF:/home/miguel/Zotero/storage/GKRGV92X/mimeo2656_Pomann.pdf:application/pdf},
}

@article{Berrendero2011,
	title = {Principal components for multivariate functional data},
	volume = {55},
	issn = {01679473},
	doi = {10.1016/j.csda.2011.03.011},
	abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
	number = {9},
	journal = {Computational Statistics and Data Analysis},
	author = {Berrendero, J. R. and Justel, A. and Svarc, M.},
	year = {2011},
	keywords = {Dimension reduction, Eigenvalue functions, Explained variability},
	pages = {2619--2634},
	file = {PDF:/home/miguel/Zotero/storage/IUP3DQ8Y/65054_Principal_components_for_multivariate2011.pdf:application/pdf},
}

@article{Chakraborty2015,
	title = {A {Wilcoxon}-{Mann}-{Whitney}-type test for infinite-dimensional data},
	volume = {102},
	issn = {14643510},
	doi = {10.1093/biomet/asu072},
	abstract = {TheWilcoxon-Mann-Whitney test is a robust competitor of the t test in the univariate setting. For finitedimensional multivariate non-Gaussian data, several extensions of theWilcoxon-Mann-Whitney test have been shown to outperformHotelling's T 2 test. In this paper, we study aWilcoxon-Mann-Whitney-type test based on spatial ranks in infinite-dimensional spaces, we investigate its asymptotic properties and compare it with several existing tests. The proposed test is shown to be robust with respect to outliers and to have better power than some competitors for certain distributions with heavy tails. We study its performance using real and simulated data.},
	number = {1},
	journal = {Biometrika},
	author = {Chakraborty, Anirvan and Chaudhuri, Probal},
	year = {2015},
	note = {arXiv: 1403.0201},
	keywords = {Functional data, Two-sample problem, Contaminated data, Separable Hilbert space, Spatial rank, Standard Brownian motion, T process},
	pages = {239--246},
	file = {PDF:/home/miguel/Zotero/storage/39P6THF8/asu072.pdf:application/pdf},
}

@article{Cuesta-Albertos2007,
	title = {The random projection method in goodness of fit for functional data},
	volume = {51},
	issn = {01679473},
	doi = {10.1016/j.csda.2006.09.007},
	abstract = {The possibility of considering random projections to identify probability distributions belonging to parametric families is explored. The results are based on considerations involving invariance properties of the family of distributions as well as on the random way of choosing the projections. In particular, it is shown that if a one-dimensional (suitably) randomly chosen projection is Gaussian, then the distribution is Gaussian. In order to show the applicability of the methodology some goodness-of-fit tests based on these ideas are designed. These tests are computationally feasible through the bootstrap setup, even in the functional framework. Simulations providing power comparisons of these projections-based tests with other available tests of normality, as well as to test the Black-Scholes model for a stochastic process are presented. © 2006 Elsevier B.V. All rights reserved.},
	number = {10},
	journal = {Computational Statistics and Data Analysis},
	author = {Cuesta-Albertos, J. A. and del Barrio, E. and Fraiman, R. and Matrán, C.},
	year = {2007},
	keywords = {Black-Scholes, Families of distributions, Gaussian distributions, Goodness-of-fit tests, Random projections, Stochastic processes},
	pages = {4814--4831},
	file = {PDF:/home/miguel/Zotero/storage/EIQCP26Z/Randomprojections_Families.pdf:application/pdf},
}

@article{Cuevas2006,
	title = {On the use of the bootstrap for estimating functions with functional data},
	volume = {51},
	issn = {01679473},
	doi = {10.1016/j.csda.2005.10.012},
	abstract = {The bootstrap methodology for functional data and functional estimation target is considered. A Monte Carlo study analyzing the performance of the bootstrap confidence bands (obtained with different resampling methods) of several functional estimators is presented. Some of these estimators (e.g., the trimmed functional mean) rely on the use of depth notions for functional data and do not have received yet much attention in the literature. A real data example in cardiology research is also analyzed. In a more theoretical aspect, a brief discussion is given providing some insights on the asymptotic validity of the bootstrap methodology when functional data, as well as a functional parameter, are involved. © 2005 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Computational Statistics and Data Analysis},
	author = {Cuevas, Antonio and Febrero, Manuel and Fraiman, Ricardo},
	year = {2006},
	keywords = {Functional median, Smoothed bootstrap, Functional data, Bootstrap, Bootstrap validity, Trimmed functional mean},
	pages = {1063--1074},
	file = {PDF:/home/miguel/Zotero/storage/YC8LZP9H/Cuevas, Fraiman, 2006.pdf:application/pdf},
}

@article{Rousseeuw1999,
	title = {The depth function of a population distribution},
	volume = {49},
	issn = {00261335},
	doi = {10.1007/PL00020903},
	abstract = {Tukey (1975) introduced the notion of halfspace depth in a data analytic context, as a multivariate analog of rank relative to a finite data set. Here we focus on the depth function of an arbitrary probability distribution on ℝp, and even of a non-probability measure. The halfspace depth of any point θ in ℝp is the smallest measure of a closed halfspace that contains θ. We review the properties of halfspace depth, enriched with some new results. For various measures, uniform as well as non-uniform, we derive an expression for the depth function. We also compute the Tukey median, which is the θ in which the depth function attains its maximal value. Various interesting phenomena occur. For the uniform distribution on a triangle, a square or any regular polygon, the depth function has ridges that correspond to an 'inversion' of depth contours. And for a product of Cauchy distributions, the depth contours are squares. We also consider an application of the depth function to voting theory.},
	number = {3},
	journal = {Metrika},
	author = {Rousseeuw, Peter J. and Ruts, Ida},
	year = {1999},
	keywords = {Depth contour, Halfspace depth, Location depth, Tukey median, Voting},
	pages = {213--244},
	file = {PDF:/home/miguel/Zotero/storage/8SZMS6JJ/RousseeuwRuts_DepthFunctionOfPopulation_Metrika_1999.pdf:application/pdf},
}

@article{Cuesta-Albertos2008,
	title = {The random {Tukey} depth},
	volume = {52},
	issn = {01679473},
	doi = {10.1016/j.csda.2008.04.021},
	abstract = {The computation of the Tukey depth, also called halfspace depth, is very demanding, even in low dimensional spaces, because it requires that all possible one-dimensional projections be considered. A random depth which approximates the Tukey depth is proposed. It only takes into account a finite number of one-dimensional projections which are chosen at random. Thus, this random depth requires a reasonable computation time even in high dimensional spaces. Moreover, it is easily extended to cover the functional framework. Some simulations indicating how many projections should be considered depending on the kind of problem, sample size and dimension of the sample space among others are presented. It is noteworthy that the random depth, based on a very low number of projections, obtains results very similar to those obtained with the Tukey depth. © 2008 Elsevier B.V. All rights reserved.},
	number = {11},
	journal = {Computational Statistics and Data Analysis},
	author = {Cuesta-Albertos, J. A. and Nieto-Reyes, A.},
	year = {2008},
	note = {arXiv: 0707.0167},
	keywords = {s, functional data, 1980 subject classification, 62g07, 62g35, a, and phrases, homogeneity test, m, multidimen-, one-dimensional projections, primary 62h05, random tukey depth, secondary, sional data, supervised classification},
	pages = {4979--4988},
	file = {PDF:/home/miguel/Zotero/storage/8VJKNJBW/Random_tukey.pdf:application/pdf},
}

@article{Sguera2014,
	title = {Spatial depth-based classification for functional data},
	volume = {23},
	issn = {11330686},
	doi = {10.1007/s11749-014-0379-1},
	abstract = {We enlarge the number of available functional depths by introducing the kernelized functional spatial depth (KFSD). KFSD is a local-oriented and kernel-based version of the recently proposed functional spatial depth (FSD) that may be useful for studying functional samples that require an analysis at a local level. In addition, we consider supervised functional classification problems, focusing on cases in which the differences between groups are not extremely clear-cut or the data may contain outlying curves. We perform classification by means of some available robust methods that involve the use of a given functional depth, including FSD and KFSD, among others. We use the functional k-nearest neighbor classifier as a benchmark procedure. The results of a simulation study indicate that the KFSD-based classification approach leads to good results. Finally, we consider two real classification problems, obtaining results that are consistent with the findings observed with simulated curves.},
	number = {4},
	journal = {Test},
	author = {Sguera, Carlo and Galeano, Pedro and Lillo, Rosa},
	year = {2014},
	note = {arXiv: 1305.2957
ISBN: 1174901403},
	keywords = {Functional depths, Functional outliers, Functional spatial depth, Kernelized functional spatial depth, Supervised functional classification},
	pages = {725--750},
	file = {PDF:/home/miguel/Zotero/storage/VUZFLH4M/Spatial_Depth-Based_Classification_for_Functional_.pdf:application/pdf},
}

@article{Liang2020,
	title = {Modeling and {Regionalization} of {China}’s {PM2}.5 {Using} {Spatial}-{Functional} {Mixture} {Models}},
	volume = {0},
	issn = {1537274X},
	url = {https://doi.org/10.1080/01621459.2020.1764363},
	doi = {10.1080/01621459.2020.1764363},
	abstract = {Abstract–Severe air pollution affects billions of people around the world, particularly in developing countries such as China. Effective emission control policies rely primarily on a proper assessment of air pollutants and accurate spatial clustering outcomes. Unfortunately, emission patterns are difficult to observe as they are highly confounded by many meteorological and geographical factors. In this study, we propose a novel approach for modeling and clustering PM (Formula presented.) concentrations across China. We model observed concentrations from monitoring stations as spatially dependent functional data and assume latent emission processes originate from a functional mixture model with each component as a spatio-temporal process. Cluster memberships of monitoring stations are modeled as a Markov random field, in which confounding effects are controlled through energy functions. The superior performance of our approach is demonstrated using extensive simulation studies. Our method is effective in dividing China and the Beijing-Tianjin-Hebei region into several regions based on PM (Formula presented.) concentrations, suggesting that separate local emission control policies are needed. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
	number = {0},
	journal = {Journal of the American Statistical Association},
	author = {Liang, Decai and Zhang, Haozhe and Chang, Xiaohui and Huang, Hui},
	year = {2020},
	note = {Publisher: Taylor \& Francis},
	keywords = {Environmental policies, Latent emission process, Markov random field, Model-based clustering},
	pages = {1--70},
	file = {PDF:/home/miguel/Zotero/storage/8C5PBC7K/2020_Liang et al.pdf:application/pdf},
}

@article{Wang2020,
	title = {The functional spatio-temporal statistical model with application to {O3} pollution in {Beijing}, {China}},
	volume = {17},
	issn = {16604601},
	doi = {10.3390/ijerph17093172},
	abstract = {In recent years, with rapid industrialization and massive energy consumption, ground-level ozone (O3) has become one of the most severe air pollutants. In this paper, we propose a functional spatio-temporal statistical model to analyze air quality data. Firstly, since the pollutant data from the monitoring network usually have a strong spatial and temporal correlation, the spatio-temporal statistical model is a reasonable method to reveal spatial correlation structure and temporal dynamic mechanism in data. Secondly, effects from the covariates are introduced to explore the formation mechanism of ozone pollution. Thirdly, considering the obvious diurnal pattern of ozone data, we explore the diurnal cycle of O3 pollution using the functional data analysis approach. The spatio-temporal model shows great applicational potential by comparison with other models. With application to O3 pollution data of 36 stations in Beijing, China, we give explanations of the covariate effects on ozone pollution, such as other pollutants and meteorological variables, and meanwhile we discuss the diurnal cycle of ozone pollution.},
	number = {9},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Wang, Yaqiong and Xu, Ke and Li, Shaomin},
	year = {2020},
	pmid = {32370183},
	keywords = {Functional data analysis, O3 pollution, Spatio-temporal statistical model},
	file = {PDF:/home/miguel/Zotero/storage/39L9QM4L/2020.pdf:application/pdf},
}

@article{Torres2020,
	title = {A functional data analysis approach for the detection of air pollution episodes and outliers: {A} case study in {Dublin}, {Ireland}},
	volume = {8},
	issn = {22277390},
	doi = {10.3390/math8020225},
	abstract = {Ground level concentrations of nitrogen oxide (NOx) can act as an indicator of air quality in the urban environment. In cities with relatively good air quality, and where NOx concentrations rarely exceed legal limits, adverse health effects on the population may still occur. Therefore, detecting small deviations in air quality and deriving methods of controlling air pollution are challenging. This study presents different data analytical methods which can be used to monitor and effectively evaluate policies or measures to reduce nitrogen oxide (NOx) emissions through the detection of pollution episodes and the removal of outliers. This method helps to identify the sources of pollution more effectively, and enhances the value of monitoring data and exceedances of limit values. It will detect outliers, changes and trend deviations in NO2 concentrations at ground level, and consists of four main steps: classical statistical description techniques, statistical process control techniques, functional analysis and a functional control process. To demonstrate the effectiveness of the outlier detection methodology proposed, it was applied to a complete one-year NO2 dataset for a sub-urban site in Dublin, Ireland in 2013. The findings demonstrate how the functional data approach improves the classical techniques for detecting outliers, and in addition, how this new methodology can facilitate a more thorough approach to defining effect air pollution control measures.},
	number = {2},
	journal = {Mathematics},
	author = {Torres, Javier Martínez and Pérez, Jorge Pastor and Val, Joaquín Sancho and McNabola, Aonghus and Comesaña, Miguel Martínez and Gallagher, John},
	year = {2020},
	keywords = {Air pollution, Functional data analysis, Outlier, Non-normal data, Statistical process control},
	file = {PDF:/home/miguel/Zotero/storage/N3QUX9SF/2020_Martinez.pdf:application/pdf},
}

@article{Melendez2020,
	title = {Imputación de valores perdidos y detección de valores atípicos en datos funcionales: una aplicación con datos de {PM10}},
	volume = {19},
	issn = {16574583},
	doi = {10.18273/revuin.v19n2-2020001},
	abstract = {Los datos recopilados en el monitoreo de la contaminación del aire, como PM10, se obtienen en estaciones automatizadas que generalmente contenían valores faltantes debido a fallas de la máquina, mantenimiento de rutina o errores humanos. Los conjuntos de datos incompletos pueden causar sesgo de información, por lo tanto, es importante encontrar la mejor manera de estimar estos valores faltantes para garantizar la calidad de los datos analizados. En este trabajo se evaluaron los datos de partículas PM10consideradas en el tiempo como un objeto funcional, para este caso se utilizó la base de datos de la red de monitoreo ambiental de la Corporación Ambiental de La Guajira (Corpoguajira). En este estudio hemos implementado la metodología de Jeng-Min Chiou,(2014) para imputar datos funcionales. La detección de valores atípicos de contaminantes es muy importante para el monitoreo y control de la calidad del aire. Además, hemos implementado el método de imputación de datos faltantes y detección de valores atípicos para datos funcionales. Consideramos las concentraciones de partículas PM10en las estaciones de monitoreo ambiental sobre el área de influencia de la mina de carbón a cielo abierto durante 2012. Para imputar datos faltantes funcionales, se basó en la aplicación de herramientas como el análisis de componentes principales funcional (ACPF) y los procedimientos gráficos para detectar curvas de valores atípicos como el bagplot funcional y el diagrama de caja funcional de la región de mayor densidad (HDR) por sus siglas en ingles. Los resultados indican que la estación de Barranca es una curva atípica y se observó que los intervalos imputados capturan la dinámica que se comparte con las otras trayectorias de las diferentes estaciones},
	number = {2},
	journal = {Revista UIS Ingenierías},
	author = {Meléndez, Rafael and Bolívar, Stevenso N and Rojano, Roberto},
	year = {2020},
	keywords = {análisis de componentes principales, datos funcionales, funcionales, valores atípicos funcionales},
	pages = {1--10},
	file = {PDF:/home/miguel/Zotero/storage/MNVU3FMI/2020_Melendez et al.pdf:application/pdf},
}

@article{Arribas-Gil2014,
	title = {Shape outlier detection and visualization for functional data: {The} outliergram},
	volume = {15},
	issn = {14684357},
	doi = {10.1093/biostatistics/kxu006},
	abstract = {We propose a new method to visualize and detect shape outliers in samples of curves. In functional data analysis, we observe curves defined over a given real interval and shape outliers may be defined as those curves that exhibit a different shape from the rest of the sample. Whereas magnitude outliers, that is, curves that lie outside the range of the majority of the data, are in general easy to identify, shape outliers are often masked among the rest of the curves and thus difficult to detect. In this article, we exploit the relationship between two measures of depth for functional data to help to visualize curves in terms of shape and to develop an algorithm for shape outlier detection. We illustrate the use of the visualization tool, the outliergram, through several examples and analyze the performance of the algorithm on a simulation study. Finally, we apply our method to assess cluster quality in a real set of time course microarray data.},
	number = {4},
	journal = {Biostatistics},
	author = {Arribas-Gil, Ana and Romo, Juan},
	year = {2014},
	pmid = {24622037},
	note = {arXiv: 1306.1718},
	keywords = {Depth for functional data, Outlier visualization, Robust estimation, Time course microarray data},
	pages = {603--619},
	file = {PDF:/home/miguel/Zotero/storage/W7D52XLG/Outliergram.pdf:application/pdf},
}

@article{Sun2011,
	title = {Functional boxplots},
	volume = {20},
	issn = {10618600},
	doi = {10.1198/jcgs.2011.09224},
	abstract = {This article proposes an informative exploratory tool, the functional boxplot, for visualizing functional data, as well as its generalization, the enhanced functional boxplot. Based on the center outward ordering induced by band depth for functional data, the descriptive statistics of a functional boxplot are: the envelope of the 50\% central region, the median curve, and the maximum non-outlying envelope. In addition, outliers can be detected in a functional boxplot by the 1.5 times the 50\% central region empirical rule, analogous to the rule for classical boxplots. The construction of a functional boxplot is illustrated on a series of sea surface temperatures related to the El Niño phenomenon and its outlier detection performance is explored by simulations. As applications, the functional boxplot and enhanced functional boxplot are demonstrated on children growth data and spatio-temporal U.S. precipitation data for nine climatic regions, respectively. This article has supplementary material online. © 2011 American Statistical Association.},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Sun, Ying and Genton, Marc G.},
	year = {2011},
	keywords = {Functional data, Depth, Growth data, Precipitation data, Space-time data, Visualization},
	pages = {316--334},
	file = {PDF:/home/miguel/Zotero/storage/5R24R49W/Functional_Boxplot.pdf:application/pdf},
}

@article{Ieva2019,
	title = {Roahd package: {Robust} analysis of high dimensional data},
	volume = {11},
	issn = {20734859},
	doi = {10.32614/RJ-2019-032},
	abstract = {The focus of this paper is on the open-source R package roahd (RObust Analysis of High dimensional Data), see Tarabelloni et al. (2017). roahd has been developed to gather recently proposed statistical methods that deal with the robust inferential analysis of univariate and multivariate functional data. In particular, efficient methods for outlier detection and related graphical tools, methods to represent and simulate functional data, as well as inferential tools for testing differences and dependency among families of curves will be discussed, and the associated functions of the package will be described in details.},
	number = {2},
	journal = {R Journal},
	author = {Ieva, Francesca and Paganoni, Anna Maria and Romo, Juan and Tarabelloni, Nicholas},
	year = {2019},
	pages = {291--307},
	file = {PDF:/home/miguel/Zotero/storage/IQDTTYBX/RJ-2019-032.pdf:application/pdf},
}

@article{Hyndman2010,
	title = {Rainbow plots, bagplots, and boxplots for functional data},
	volume = {19},
	issn = {10618600},
	doi = {10.1198/jcgs.2009.08158},
	abstract = {We propose new tools for visualizing large amounts of functional data in the form of smooth curves. The proposed tools include functional versions of the bagplot and boxplot, which make use of the first two robust principal component scores, Tukey's data depth and highest density regions. By-products of our graphical displays are outlier detection methods for functional data. We compare these new outlier detection methods with existing methods for detecting outliers in functional data, and show that our methods are better able to identify outliers. An R-package containing computer code and datasets is available in the online supplements. Copyright © 2010 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hyndman, Rob J. and Shang, Han Lin},
	year = {2010},
	keywords = {Outlier detection, Highest density regions, Kernel density estimation, Robust principal component analysis, Tukey's halfspace location depth},
	pages = {29--45},
	file = {PDF:/home/miguel/Zotero/storage/ND97F5V6/Hyndman, Shan, 2010.pdf:application/pdf},
}

@article{Martinez2014,
	title = {Air quality parameters outliers detection using functional data analysis in the {Langreo} urban area ({Northern} {Spain})},
	volume = {241},
	issn = {00963003},
	doi = {10.1016/j.amc.2014.05.004},
	abstract = {Polluted air of cities is a harmful factor to health that may eventually cause respiratory problems and cardiovascular disease. The monitoring and control of pollutants is an essential activity in order to protect the environment and the health by minimizing pollution levels through the detection of contaminants. Contaminants are emissions of substances to the atmosphere (mainly gases and particulate matter) whose values are greater than the limits allowed by the environmental legislation (they are anomalous values). Thus they are considered as vector samples where each component represents the gas concentration value in the air. In this sense, a model based on functional analysis has been implemented for the outliers detection in air quality samples in this research work. This model transforms the vectorial sample by creating a new functional sample in order to determine functional outliers by adjusting the concept of depth to the functional event. This method has been compared to classical outliers analysis from a vectorial point of view, emphasizing the power of use of such functional techniques over the traditional ones. The main aim of this research work is to compare the results corresponding to the classical and the functional methods and to obtain the most appropriate methodology to analyze this type of dataset in order to reach a better solution for the air quality control. © 2014 Elsevier Inc. All rights reserved.},
	number = {2},
	journal = {Applied Mathematics and Computation},
	author = {Martínez, J. and Saavedra, Á and García-Nieto, P. J. and Piñeiro, J. I. and Iglesias, C. and Taboada, J. and Sancho, J. and Pastor, J.},
	year = {2014},
	keywords = {Air quality control, Functional analysis, Gas pollution, Outliers detection},
	pages = {1--10},
	file = {PDF:/home/miguel/Zotero/storage/GNPF24R8/Martinez et al 2014.pdf:application/pdf},
}

@article{Paredes2019,
	title = {Mecanismos de pago en salud},
	number = {34},
	author = {Paredes, Daniela},
	year = {2019},
	pages = {2--3},
	file = {PDF:/home/miguel/Zotero/storage/EJFPXYDU/MECANISMO DE PAGO - CEREM.pdf:application/pdf},
}

@article{Torres2011,
	title = {Detection of outliers in gas emissions from urban areas using functional data analysis},
	volume = {186},
	issn = {03043894},
	doi = {10.1016/j.jhazmat.2010.10.091},
	abstract = {In this work a solution for the problem of the detection of outliers in gas emissions in urban areas that uses functional data analysis is described. Different methodologies for outlier identification have been applied in air pollution studies, with gas emissions considered as vectors whose components are gas concentration values for each observation made. In our methodology we consider gas emissions over time as curves, with outliers obtained by a comparison of curves instead of vectors. The methodology, which is based on the concept of functional depth, was applied to the detection of outliers in gas omissions in the city of Oviedo and results were compared with those obtained using a conventional method based on a comparison of vectors. Finally, the advantages of the functional method are reported. © 2010 Elsevier B.V.},
	number = {1},
	journal = {Journal of Hazardous Materials},
	author = {Torres, J. Martínez and Nieto, P. J.Garcia and Alejano, L. and Reyes, A. N.},
	year = {2011},
	pmid = {21112150},
	keywords = {Outliers, Functional depth, Air pollution, Functional data analysis, Gas emissions},
	pages = {144--149},
	file = {PDF:/home/miguel/Zotero/storage/KBHLIWHZ/Martínez 2010.pdf:application/pdf},
}

@article{Quintela-del-Rio2011,
	title = {Nonparametric functional data estimation applied to ozone data: {Prediction} and extreme value analysis},
	volume = {82},
	issn = {00456535},
	url = {http://dx.doi.org/10.1016/j.chemosphere.2010.11.025},
	doi = {10.1016/j.chemosphere.2010.11.025},
	abstract = {The study of extreme values and prediction of ozone data is an important topic of research when dealing with environmental problems. Classical extreme value theory is usually used in air-pollution studies. It consists in fitting a parametric generalised extreme value (GEV) distribution to a data set of extreme values, and using the estimated distribution to compute return levels and other quantities of interest. Here, we propose to estimate these values using nonparametric functional data methods. Functional data analysis is a relatively new statistical methodology that generally deals with data consisting of curves or multi-dimensional variables. In this paper, we use this technique, jointly with nonparametric curve estimation, to provide alternatives to the usual parametric statistical tools. The nonparametric estimators are applied to real samples of maximum ozone values obtained from several monitoring stations belonging to the Automatic Urban and Rural Network (AURN) in the UK. The results show that nonparametric estimators work satisfactorily, outperforming the behaviour of classical parametric estimators. Functional data analysis is also used to predict stratospheric ozone concentrations. We show an application, using the data set of mean monthly ozone concentrations in Arosa, Switzerland, and the results are compared with those obtained by classical time series (ARIMA) analysis. © 2010 Elsevier Ltd.},
	number = {6},
	journal = {Chemosphere},
	author = {Quintela-del-Río, Alejandro and Francisco-Fernández, Mario},
	year = {2011},
	pmid = {21144549},
	note = {Publisher: Elsevier Ltd},
	keywords = {Ozone, Prediction, Air-pollution, Extreme values, Nonparametric functional estimation},
	pages = {800--808},
	file = {PDF:/home/miguel/Zotero/storage/BZHAWZNV/Quintela del río 2011.pdf:application/pdf},
}

@article{Shaadan2012,
	title = {Assessing and comparing {PM10} pollutant behaviour using functional data approach},
	volume = {41},
	issn = {01266039},
	abstract = {This study highlights the advantage of functional data approach in assessing and comparing the PM 10 pollutant behaviour as an alternative statistical approach during and between the two extreme haze years (1997 and 2005) that have been reported in Selangor, state of Malaysia. The aim of the study was to improvise the current conventional methods used in air quality assessment so that any unforeseen implicit information can be revealed and the previous research findings can be justified. An analysis based on the daily diurnal curves in place of discrete point values was performed. The analysis results provided evidences of the influence of the change in the climate (due to the El-Nino event), the different levels of different emission sources and meteorological conditions on the severity of the PM 10 problem. By means of the cummulative exceedence index and the functional depth method, most of the monitoring stations for the year 2005 experienced the worst day of critical exceedences on the 10 th of August, while for the year 1997 it occurred between 13 th and 26 th September inclusively at different dates among the stations.},
	number = {11},
	journal = {Sains Malaysiana},
	author = {Shaadan, Norshahida and Deni, Sayang Mohd and Jemain, Abd Aziz},
	year = {2012},
	keywords = {Functional data, Air quality, Behaviour, Exceedences, PM 10 pollutant},
	pages = {1335--1344},
	file = {PDF:/home/miguel/Zotero/storage/CZT2W9F9/Shaadan 2012.pdf:application/pdf},
}

@article{Shaadan2015,
	title = {Anomaly detection and assessment of {PM10} functional data at several locations in the {Klang} {Valley}, {Malaysia}},
	volume = {6},
	issn = {13091042},
	url = {http://dx.doi.org/10.5094/APR.2015.040},
	doi = {10.5094/APR.2015.040},
	abstract = {In environmental data sets, the occurrence of a high concentration of an unusual pollutant, more formally known as an anomaly, may indicate air quality problems. Thus, a critical understanding of the behavior of anomalies is increasingly becoming very important for air pollution investigations. This study was conducted to detect anomalies in daily PM10 functional data, to investigate the patterns of behavior as well as to identify possible factors that determine PM10 anomalies at three selected air quality monitoring stations (Klang, Kuala Selangor and Petaling Jaya) in the Klang Valley, Malaysia. The statistical method employed to detect these anomalies consisted of a combination of the robust projection pursuit and the robust Mahalanobis distance methods using air quality data recorded from 2005 to 2010. Analysis of obtained anomalous PM10 profiles showed that data recorded during El Nino years (2005, 2006 and 2009) contained the highest frequency of anomalies. More frequent anomalies appeared during the southwest (SW) monsoon which occurs in the months of July and August as well as during the northeast (NE) monsoon in February. A lesser number of anomalies were also observed during weekends compared to weekdays. The weekend and monsoonal effect phenomena were shown to be significantly existent at all stations while wind speed was positively associated with extreme PM10 anomalies at the Klang and Petaling Jaya stations. In conclusion, anomalies detection was found useful for air pollution investigation in this study. The findings of this study imply that the location and background of a station, as well as wind speed, seasonal (monsoon) and weekdays-weekend variations play important role in influencing PM10 anomalies.},
	number = {2},
	journal = {Atmospheric Pollution Research},
	author = {Shaadan, Norshahida and Jemain, Abdul Aziz and Latif, Mohd Talib and Deni, Sayang Mohd},
	year = {2015},
	note = {Publisher: Elsevier},
	keywords = {Functional data, Air quality monitoring, Anomaly detection, PM10},
	pages = {365--375},
	file = {PDF:/home/miguel/Zotero/storage/D6VN25TZ/Shaadan 2015.pdf:application/pdf},
}

@article{Sanchez-Lasheras2020,
	title = {Detection of outliers in pollutant emissions from the {Soto} de {Ribera} coal-fired power plant using functional data analysis: a case study in northern {Spain}},
	volume = {27},
	issn = {16147499},
	doi = {10.1007/s11356-019-04435-4},
	abstract = {For more than a century, air pollution has been one of the most important environmental problems in cities. Pollution is a threat to human health and is responsible for many deaths every year all over the world. This paper deals with the topic of functional outlier detection. Functional analysis is a novel mathematical tool employed for the recognition of outliers. This methodology is applied here to the emissions of a coal-fired power plant. This research uses two different methods, called functional high-density region (HDR) boxplot and functional bagplot. Please note that functional bagplots were developed using bivariate bagplots as a starting point. Indeed, they are applied to the first two robust principal component scores. Both methodologies were applied for the detection of outliers in the time pollutant emission curves that were built using, as inputs, the discrete information available from an air quality monitoring data record station and the subsequent smoothing of this dataset for each pollutant. In this research, both new methodologies are tested to detect outliers in pollutant emissions performed over a long period of time in an urban area. These pollutant emissions have been treated in order to use them as vectors whose components are pollutant concentration values for each observation made. Note that although the recording of pollutant emissions is made in a discrete way, these methodologies use pollutants as curves, identifying the outliers by a comparison of curves rather than vectors. Then, the concept of outlier goes from being a point to a curve that employs the functional depth as the indicator of curve distance. In this study, it is applied to the detection of outliers in pollutant emissions from a coal-fired power plant located on the outskirts of the city of Oviedo, located in the north of Spain and capital of the Principality of Asturias. Also, strengths of the functional methods are explained.},
	number = {1},
	journal = {Environmental Science and Pollution Research},
	author = {Sánchez-Lasheras, Fernando and Ordóñez-Galán, Celestino and García-Nieto, Paulino José and García-Gonzalo, Esperanza},
	year = {2020},
	pmid = {30771125},
	keywords = {Air pollution, Functional data analysis, Outlier detection, Gas emissions, Functional bagplot, Functional high-density region (HDR) boxplot},
	pages = {8--20},
	file = {PDF:/home/miguel/Zotero/storage/ZCBDL2YF/Sánchez Lasheras 2019.pdf:application/pdf},
}

@article{Liu1990,
	title = {On a notion of data depth based on random simplices},
	volume = {18},
	number = {1},
	journal = {Annals of Statistics},
	author = {Liu, Regina Y.},
	year = {1990},
	pages = {1403--405--414},
	file = {PDF:/home/miguel/Zotero/storage/DQC38HIW/Liu_1990.pdf:application/pdf},
}

@article{Lopez-Pintado2011,
	title = {A half-region depth for functional data},
	volume = {55},
	issn = {01679473},
	url = {http://dx.doi.org/10.1016/j.csda.2010.10.024},
	doi = {10.1016/j.csda.2010.10.024},
	abstract = {A new definition of depth for functional observations is introduced based on the notion of "half-region" determined by a curve. The half-region depth provides a simple and natural criterion to measure the centrality of a function within a sample of curves. It has computational advantages relative to other concepts of depth previously proposed in the literature which makes it applicable to the analysis of high-dimensional data. Based on this depth a sample of curves can be ordered from the center-outward and order statistics can be defined. The properties of the half-region depth, such as consistency and uniform convergence, are established. A simulation study shows the robustness of this new definition of depth when the curves are contaminated. Finally, real data examples are analyzed. © 2010 Published by Elsevier B.V.},
	number = {4},
	journal = {Computational Statistics and Data Analysis},
	author = {López-Pintado, Sara and Romo, Juan},
	year = {2011},
	note = {Publisher: Elsevier B.V.},
	keywords = {Data depth, Functional data, High-dimensional data, Order statistics},
	pages = {1679--1695},
	file = {PDF:/home/miguel/Zotero/storage/HRLD757K/half-region_CSDA_2011_ps.pdf:application/pdf},
}

@article{Hubert2020,
	title = {Package ‘ {mrfDepth} ’},
	author = {Hubert, Mia and Rousseeuw, Peter and Raymaekers, Jakob},
	year = {2020},
	file = {PDF:/home/miguel/Zotero/storage/NBZXDXJ7/mrfDepth.pdf:application/pdf},
}

@article{Rousseeuw1999a,
	title = {The {Bagplot}: {A} {Bivariate} {Boxplot}},
	volume = {53},
	number = {4},
	journal = {Statistical Computing and Graphics},
	author = {Rousseeuw, Peter J. and Ruts, Ida and Tukey, John W.},
	year = {1999},
	pages = {382--387},
	file = {PDF:/home/miguel/Zotero/storage/ZMHKHX6P/Bagplot.pdf:application/pdf},
}

@article{Hubert2008,
	title = {An adjusted boxplot for skewed distributions},
	volume = {52},
	issn = {01679473},
	doi = {10.1016/j.csda.2007.11.008},
	abstract = {The boxplot is a very popular graphical tool for visualizing the distribution of continuous unimodal data. It shows information about the location, spread, skewness as well as the tails of the data. However, when the data are skewed, usually many points exceed the whiskers and are often erroneously declared as outliers. An adjustment of the boxplot is presented that includes a robust measure of skewness in the determination of the whiskers. This results in a more accurate representation of the data and of possible outliers. Consequently, this adjusted boxplot can also be used as a fast and automatic outlier detection tool without making any parametric assumption about the distribution of the bulk of the data. Several examples and simulation results show the advantages of this new procedure. © 2007 Elsevier B.V. All rights reserved.},
	number = {12},
	journal = {Computational Statistics and Data Analysis},
	author = {Hubert, M. and Vandervieren, E.},
	year = {2008},
	pages = {5186--5201},
	file = {PDF:/home/miguel/Zotero/storage/6Z8X2BR4/adjboxplot_revision.pdf:application/pdf},
}

@article{Pez-Pintado2009a,
	title = {On the concept of depth for functional data},
	volume = {104},
	issn = {01621459},
	doi = {10.1198/jasa.2009.0108},
	abstract = {The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the "centrality" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect "shape" outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls. © 2009 American Statistical Association.},
	number = {486},
	journal = {Journal of the American Statistical Association},
	author = {Pez-Pintado, Sara LÓ and Romo, Juan},
	year = {2009},
	keywords = {Data depth, Functional data, Rank test for functions},
	pages = {718--734},
	file = {PDF:/home/miguel/Zotero/storage/TIJNG5ZN/ws063012.pdf:application/pdf},
}

@article{Card2019,
	title = {Inner product and norm for {FD} fdata class objects {Functional} {Non}-{Linear} {Models} {RPubs} document {Functional} {Non}-{Supervised} {Classification}},
	number = {Cv},
	author = {Card, Reference and Math, Group},
	year = {2019},
	file = {PDF:/home/miguel/Zotero/storage/SPE4P7TV/RefCard_fda.usc_v1.pdf:application/pdf},
}

@article{Jentzsch2015,
	title = {True {Believers}, {Deserters}, and {Traitors}: {Who} {Leaves} {Insurgent} {Groups} and {Why}},
	volume = {59},
	issn = {15528766},
	doi = {10.1177/0022002715576750},
	abstract = {Anti-insurgent militias and states attempt to erode insurgent groups’ capacities and co-opt insurgent fighters by promising and providing benefits. They do so to create a perception that the insurgency is unraveling and to harness inside information to prosecute more effective counterinsurgency campaigns. Why do some insurgents defect to a paramilitary group and others exit the war by demobilizing, while still others remain loyal to their group? This article presents the first empirical analysis of these questions, connecting insurgents’ motivations for joining, wartime experiences, and organizational behavior with decisions to defect. A survey of ex-combatants in Colombia shows that individuals who joined for ideological reasons are less likely to defect overall but more likely to side-switch or demobilize when their group deviates from its ideological precepts. Among fighters who joined for economic reasons, political indoctrination works to decrease their chances of demobilization and defection to paramilitaries, while opportunities for looting decrease economically motivated combatants’ odds of defection.},
	number = {5},
	journal = {Journal of Conflict Resolution},
	author = {Oppenheim, Ben and Steele, Abbey and Vargas, Juan F. and Weintraub, Michael},
	year = {2015},
	note = {ISBN: 0022002715},
	keywords = {civil wars, Colombia, defection, demobilization, ideology, insurgency, internal armed conflict, militias, side switching},
	pages = {794--823},
	file = {PDF:/home/miguel/Zotero/storage/2F7MGWU4/0022002715576750.pdf:application/pdf},
}

@article{Crisp2002,
	title = {Institutional {Engineering} and the {Nature} of {Representation}: {Mapping} the {Effects} of {Electoral} {Reform} in {Colombia}},
	volume = {46},
	issn = {00925853},
	doi = {10.2307/3088430},
	abstract = {Representation can vary depending on whether legislators view constituents as best represented by aggregated, programmatic universal policies or by parochial, particularistic policies. In 1991 Colombia adopted a major institutional reform intended to change the "electoral connection" between voters and senators, encouraging members of the upper chamber to adopt a more national, programmatic vision. We explain variations in geographic patterns of electoral support in the post-reform era and show how the spatial pattern of votes for a senator influences his or her "hill style" in terms of bill-initiation priorities. Although reformers created the option of a more dispersed pattern of support, it is still possible for senate candidates to gain election with geographically concentrated constituencies. These senators have a higher probability of initiating bills with a pork-barrel propensity.},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Crisp, Brian and Ingall, Rachael E.},
	year = {2002},
	pages = {733},
	file = {PDF:/home/miguel/Zotero/storage/9IR7L5BL/3088430.pdf:application/pdf},
}

@article{Gutierrez2004,
	title = {Criminales y rebeldes: una discusión de la economía política del conflicto armado desde el caso colombiano.},
	volume = {0},
	issn = {0121-5167},
	url = {http://aprendeenlinea.udea.edu.co/revistas/index.php/estudiospoliticos/article/view/1362},
	abstract = {El artículo plantea que el conflicto colombiano parece ser un ejemplo típico de una “guerra codiciosa”, y exhibe lazos fuertes entre actividades económicas ilícitas y organizaciones insurgentes. A la vez, argumenta que la interpretación de la guerrilla como una simple expresión criminal ― tal como lo propone Paul Collier ― es errónea, incluso en un conflicto tan criminalizado como el colombiano. Para esto, se discuten algunas de las inconsistencias de las tesis de Collier, y las razones por las cuales son inaplicables al país.},
	number = {24},
	journal = {Estudios Políticos},
	author = {Gutiérrez, Francisco},
	year = {2004},
	pages = {37--71},
	file = {PDF:/home/miguel/Zotero/storage/HXPKRQEP/1362-Texto del artículo-61910-1-10-20131203.pdf:application/pdf},
}

@article{Salazar2011,
	title = {Convenios intermunicipales: {El} efecto de la {Policía} {Metropolitana} del área conurbada {Almatam}},
	volume = {XX},
	number = {II Semestre},
	journal = {Gestión y Política Pública},
	author = {Salazar, José de Jesús and Polendo, José and Ibarra, Jorge},
	year = {2011},
	note = {ISBN: 1332109500},
	keywords = {área conurbana Almatam., Convenios intermunicipales, Policía Metropolitana, seguridad pública},
	pages = {433--457},
	file = {PDF:/home/miguel/Zotero/storage/YD8WTPHU/13321095006.pdf:application/pdf},
}

@article{PinoMontoya2015,
	title = {Metodología {De} {Investigación} {En} {La} {Ciencia} {Política}: {La} {Mirada} {Empírico} {Analítica}},
	volume = {2},
	issn = {2382-3410},
	doi = {10.21501/23823410.1671},
	abstract = {En todo proceso de investigación social el investigador debe de optar por un enfoque que le permita explicarde forma adecuada el fenómeno particular que va a estudiar. Este artículo pretende exponer el paradigmaempírico como un enfoque acorde con los estudios realizados por las ciencias políticas. A través dela metodología se privilegió el enfoque cualitativo de investigación, el rastreo y el análisis documental conel fin de indagar sobre las perspectivas metodológicas utilizadas por las ciencias políticas. En los resultadosse encuentra que el paradigma empírico analítico como metodología de investigación es pertinente paraser utilizado en los estudios políticos. Finalmente en la conclusión las ciencias políticas, además de utilizarel paradigma empírico analítico de investigación debe utilizar otros enfoques investigativos que amplíen suvisión, para evitar el simplismo y llegar a una explicación más objetiva e integral.},
	number = {2},
	journal = {Revista Fundación Universitaria Luis Amigó},
	author = {Pino Montoya, José Wilmar},
	year = {2015},
	keywords = {ciencia, ciencia política, empírico analítico, experimento, revolución},
	pages = {185},
	file = {PDF:/home/miguel/Zotero/storage/IBYIDKYJ/PINO-MONTOYA 2015.pdf:application/pdf},
}

@article{Burgos2020,
	title = {Educación o resocialización: {Problemática} abordada desde la administración penitenciaria en {Colombia}.},
	volume = {25},
	issn = {2477-9555},
	number = {1},
	journal = {Utopía y Praxis Latinoamericana},
	author = {Burgos, Preciado and Amalia, Victoria},
	year = {2020},
	keywords = {centro carcelario, derechos fundamentales, Estado, fundamental rights, prison center, recidivism., reincidencia. / Resocialization, Resocialización, state},
	pages = {139--153},
	file = {PDF:/home/miguel/Zotero/storage/PWPIRHMM/27963600011.pdf:application/pdf},
}

@article{GomezGomez2001,
	title = {Economía y violencia en {Colombia}},
	issn = {1575-4227},
	abstract = {Gomez, C.M., (2001). Economía y Violencia en Colombia. En A. Martinez Ortiz (Ed.) Economía Crimen Conflicto. Bogotá, Colombia: Universidad Nacional de Colombia, 41–58.},
	number = {2},
	journal = {Quórum: revista de pensamiento iberoamericano},
	author = {Gómez Gómez, Carlos Mario},
	year = {2001},
	keywords = {CIENCIAS SOCIALES, Política, Political science, SOCIAL SCIENCES},
	pages = {159--173},
	file = {PDF:/home/miguel/Zotero/storage/JRLL7KJP/Dialnet-EconomiaYViolenciaEnColombia-201611.pdf:application/pdf},
}

@book{Rincon2007,
	title = {Curso elemental de probabilidad y estadística},
	isbn = {978-607-02-4092-8},
	abstract = {Una versión actualizada del presente texto se encuentra disponible en formato electrónico en la dirección http://www.matematicas.unam.mx/lars Prólogo El presente texto constituye el material completo del curso semestral de Proba-bilidad y Estadística, impartido por el autor a alumnos de la licenciatura en ciencias de la computación en la Facultad de Ciencias de la UNAM. Contiene el te-mario básico para un curso elemental e introductorio a algunos temas tradicionales de la probabilidad y la estadística, así como una colección de ejercicios. Algunos de estos ejercicios aparecen a lo largo del texto como parte de la lectura, y una colección más extensa aparece al final del libro incluyendo algunas soluciones o sugerencias para resolverlos. El texto está dirigido de manera general a alumnos de las distintas carreras de ingeniería, ciencias de la computación, y otras carreras científicas similares, cuyos programas de estudio contemplan un semestre introductorio a estos temas. Como es natural en este tipo de cursos, no se hacé enfasis en el rigor matemático de la demostración de los resultados, sino en el uso, interpretación y aplicación dé estos. Como prerequisitos para una lectura provechosa de este material, se requiere, en determinados momentos, tener cierta familiaridad con algunos conceptos elementa-les dé algebra y del cálculo diferencial e integral. El texto fue escrito en el sistema L A T E X, y la mayoría de las ilustraciones fueron elaboradas usando el paquete ps-tricks. El autor agradece cualquier comentario, sugerencia o corrección enviada al correo electrónico que aparece abajo.},
	author = {Rincón, Luis},
	year = {2007},
	note = {Publication Title: Notas de Matemáticas},
	file = {PDF:/home/miguel/Zotero/storage/QHHP4RN3/m-api-796e27d6-3c41-a92b-174d-7c04c1b3abd7.pdf:application/pdf},
}

@article{Restrepo2004,
	title = {The {Dynamics} of the {Colombian} {Civil} {Conflict}: {A} {New} {Data} {Set}},
	volume = {21},
	url = {http://personal.rhul.ac.uk/uhte/014/Research.htm},
	abstract = {We present a detailed, high-frequency data set on the civil conflict in Colombia during the period 1988-2002. We briefly introduce the Colombian case and the methodological issues that hinder data collection in civil wars, before presenting the pattern over time of conflict actions and intensity for all sides involved in the confrontation. We also describe the pattern of victimisation by group and the victimisation of civilians out of clashes.},
	number = {2},
	journal = {Homo Oeconomicus},
	author = {Restrepo, J and Spagat, M and Vargas, J},
	year = {2004},
	note = {ISBN: 3892650381},
	keywords = {Colombia, Conflict, Faculty of History and Social Science{\textbackslash}Economics, Guerrilla, Warfare},
	pages = {396--428},
	file = {PDF:/home/miguel/Zotero/storage/JQC6W4SG/The_Dynamics_of_the_Colombian_Civil_Conflict_A_New.pdf:application/pdf},
}

@book{AnduizaPerea2009,
	title = {Metodología de la ciencia política},
	author = {Anduiza Perea, Eva},
	year = {2009},
	file = {PDF:/home/miguel/Zotero/storage/EX3J4CXQ/Anduiza et al.pdf:application/pdf},
}

@book{Rincon2017,
	title = {Estadística {Descriptiva}},
	isbn = {978-85-7811-079-6},
	author = {Rincón, Luis},
	year = {2017},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISSN: 1098-6596},
	keywords = {icle},
	file = {PDF:/home/miguel/Zotero/storage/CA9NQUG7/RINCON_ESTADISTICA_DESCRIPTIVA.pdf:application/pdf},
}

@book{Rinc,
	title = {Una introducción a la estadística inferencial},
	isbn = {978-607-30-2432-7},
	author = {Rincón, Luis},
	year = {2019},
	file = {PDF:/home/miguel/Zotero/storage/JRFEJTWD/m-api-1e50588b-cc32-6dc1-466a-f4ba4df3b087.pdf:application/pdf},
}

@article{Fergusson2016,
	title = {The {Real} {Winner} ’ s {Curse}},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Fergusson, Leopoldo and Ruiz-guarin, Nelson A and Vargas, Juan F},
	year = {2020},
	keywords = {adriana camacho, angulo, catherine boone, clau-, democracy, elections, emilio depetris, ernesto dal-b, for their helpful comments, guadalupe dorna, juan dubra, laura bronner, marcela eslava, o, political inclusion, regression discontinuity, tim besley, violence, we thank juan carlos},
	pages = {1--65},
	file = {PDF:/home/miguel/Zotero/storage/TIQQHWTE/TheRealWinner’sCurse_preview.pdf:application/pdf},
}

@book{VariosAutores2014,
	address = {Madrid},
	title = {Diez {Textos} {Básicos} de {Ciencia} {Política}},
	publisher = {Ariel},
	author = {{Varios Autores}},
	year = {2014},
}

@article{Elster2005,
	title = {En favor de los mecanismos},
	volume = {20},
	issn = {0187-0173},
	abstract = {Some issues have also a distinctive title.},
	number = {57},
	journal = {Sociológica},
	author = {Elster, Jon},
	year = {2005},
	keywords = {anthropology, social science, sociology},
	pages = {239--273},
	annote = {No eisten leyes generales en las ciencias sociales. 

Las disciplinas no se limitan a la descripción y narración de los fenómenos.},
	file = {PDF:/home/miguel/Zotero/storage/IUB5UZQV/305024871010.pdf:application/pdf},
}

@article{MariaEncarnacaoBeltraoSposito2013,
	title = {Scanned by {CamScanner} ﯼﺭﺍﺰﻤﮐ},
	issn = {03601315},
	abstract = {ZnSn(OH)6 hierarchical cubes and Zn2SnO4 octahedra have been synthesized through a rapid, template-free, one-pot hydrothermal approach using zinc acetate, tin chloride and sodium hydroxide. ZnSn(OH)6 aggregates with cubic morphology and uniform size distribution have been successfully synthesized via aggregation-mediated crystallization. Through adjusting the hydrothermal parameters, Zn 2SnO4 octahedra were obtained at a higher temperature. The formation of Zn2SnO4 octahedra undergone a transformation from ZnSn(OH) 6 cubes. The as-synthesized products were characterized by powder X-ray diffraction (XRD), scanning electron microscopy (SEM) and differential scanning calorimetric analysis (DSC) and thermogravimetric analysis (TG). © (2011) Trans Tech Publications.},
	journal = {A psicanalise dos contos de fadas. Tradução Arlene Caetano},
	author = {Maria Encarnação Beltrão Sposito, Eda Maria Góes},
	year = {2013},
	pmid = {470195},
	note = {arXiv: NIHMS150003
ISBN: 978-85-232-0700-7},
	keywords = {Cidades, Ciências Sociais, Geografia Urbana, História Social},
	pages = {466},
	file = {PDF:/home/miguel/Zotero/storage/2YQKGWTX/Elster(2010).pdf:application/pdf},
}

@article{Package2020,
	title = {Package ‘ {MFPCA} ’},
	volume = {3},
	doi = {10.18637/jss.v093.i05>.URL},
	author = {Package, Type},
	year = {2020},
	file = {PDF:/home/miguel/Zotero/storage/C9NH8XS7/MFPCA.pdf:application/pdf},
}

@article{Chiou2014,
	title = {A functional data approach to missing value imputation and outlier detection for traffic flow data},
	volume = {2},
	issn = {21680582},
	url = {https://doi.org/10.1080/21680566.2014.892847},
	doi = {10.1080/21680566.2014.892847},
	abstract = {Missing values and outliers are frequently encountered in traffic monitoring data. We approach these problems by sampling the daily traffic flow rate trajectories from random functions and taking advantage of the data features using functional data analysis. We propose to impute missing values by using the conditional expectation approach to functional principal component analysis (FPCA). Our simulation study shows that the FPCA approach performs better than two commonly discussed methods in the literature, the probabilistic principal component analysis (PCA) and the Bayesian PCA, which have been shown to perform better than many conventional approaches. Based on the FPCA approach, the functional principal component scores can be applied to the functional bagplot and functional highest density region boxplot, which makes outlier detection possible for incomplete functional data. Our numerical results indicate that these two outlier detection approaches coupled with the proposed missing value imputation method can perform reasonably well. Although motivated by traffic flow data application, the proposed functional data methods for missing value imputation and outlier detection can be used in many applications with longitudinally recorded functional data.},
	number = {2},
	journal = {Transportmetrica B},
	author = {Chiou, Jeng Min and Zhang, Yi Chen and Chen, Wan Hui and Chang, Chiung Wen},
	year = {2014},
	keywords = {functional data, functional principal component analysis, intelligent transportation system, traffic flow rate, vehicle detector},
	pages = {106--129},
	file = {PDF:/home/miguel/Zotero/storage/4Q3C9LXC/A functional data approach to missing value imputation and outlier detection for traffic flow data.pdf:application/pdf},
}

@article{Junger2015,
	title = {Imputation of missing data in time series for air pollutants},
	volume = {102},
	issn = {18732844},
	url = {http://dx.doi.org/10.1016/j.atmosenv.2014.11.049},
	doi = {10.1016/j.atmosenv.2014.11.049},
	abstract = {Missing data are major concerns in epidemiological studies of the health effects of environmental air pollutants. This article presents an imputation-based method that is suitable for multivariate time series data, which uses the EM algorithm under the assumption of normal distribution. Different approaches are considered for filtering the temporal component. A simulation study was performed to assess validity and performance of proposed method in comparison with some frequently used methods. Simulations showed that when the amount of missing data was as low as 5\%, the complete data analysis yielded satisfactory results regardless of the generating mechanism of the missing data, whereas the validity began to degenerate when the proportion of missing values exceeded 10\%. The proposed imputation method exhibited good accuracy and precision in different settings with respect to the patterns of missing observations. Most of the imputations obtained valid results, even under missing not at random. The methods proposed in this study are implemented as a package called mtsdi for the statistical software system R.},
	journal = {Atmospheric Environment},
	author = {Junger, W. L. and Ponce de Leon, A.},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Time series, Air pollution, Data imputation, EM algorithm, Environmental epidemiology, Missing data, Particulate matter},
	pages = {96--104},
	file = {PDF:/home/miguel/Zotero/storage/DXNKYKQM/imputation.pdf:application/pdf},
}

@techreport{WorldHealthOrganization2006,
	title = {{WHO} {Air} quality guidelines for particulate matter, ozone, nitrogen dioxide and sulfur dioxide - {Global} {Update} 2005},
	author = {{World Health Organization}},
	year = {2006},
	pmid = {31407216},
	doi = {10.1007/s12011-019-01864-7},
	note = {ISSN: 15590720},
	file = {PDF:/home/miguel/Zotero/storage/Y6WS73QH/WHO_SDE_PHE_OEH_06.02_eng.pdf:application/pdf},
}

@misc{SIATA2021,
	title = {Información de calidad del aire},
	url = {https://siata.gov.co/descarga_siata/index.php/index2/calidad_aire/},
	author = {{SIATA}},
	year = {2021},
}

@techreport{Calidad2012,
	address = {Medellín},
	title = {Generalidades de la información {Red} de {Calidad} del {Aire} del {Valle} de {Aburrá}},
	url = {https://siata.gov.co/descarga_siata/index.php/info/aire/},
	institution = {Área Metropolitana del Valle de Aburrá},
	author = {{SIATA}},
	year = {2019},
	pages = {1--3},
	file = {Generalidades_Info_Aire.pdf:/home/miguel/Zotero/storage/D4NRHF3V/Generalidades_Info_Aire.pdf:application/pdf;Generalidades_Info_Aire.pdf:/home/miguel/Zotero/storage/MFBXH2W4/Generalidades_Info_Aire.pdf:application/pdf},
}

@article{Dai2020,
	title = {Functional outlier detection and taxonomy by sequential transformations},
	volume = {149},
	issn = {23318422},
	abstract = {Functional data analysis can be seriously impaired by abnormal observations, which can be classified as either magnitude or shape outliers based on their way of deviating from the bulk of data. Identifying magnitude outliers is relatively easy, while detecting shape outliers is much more challenging. We propose turning the shape outliers into magnitude outliers through data transformation and detecting them using the functional boxplot. Besides easing the detection procedure, applying several transformations sequentially provides a reasonable taxonomy for the flagged outliers. A joint functional ranking, which consists of several transformations, is also defined here. Simulation studies are carried out to evaluate the performance of the proposed method using different functional depth notions. Interesting results are obtained in several practical applications.},
	number = {11901573},
	journal = {Computational Statistics and Data Analysis},
	author = {Dai, Wenlin and Mrkvička, Tomáš and Sun, Ying and Genton, Marc G.},
	year = {2020},
	note = {arXiv: 1808.05414},
	keywords = {Multivariate functional data, Functional boxplot, Data transformation, Magnitude outliers, Shape outliers},
	file = {PDF:/home/miguel/Zotero/storage/VE4IN4SZ/main.pdf:application/pdf},
}

@article{Cuesta-Albertos2017,
	title = {The {DD} {G} -classifier in the functional setting},
	volume = {26},
	issn = {11330686},
	doi = {10.1007/s11749-016-0502-6},
	abstract = {The maximum depth classifier was the first attempt to use data depths instead of multivariate raw data in classification problems. Recently, the DD-classifier has addressed some of the serious limitations of this classifier but issues still remain. This paper aims to extend the DD-classifier as follows: first, by enabling it to handle more than two groups; second, by applying regular classification methods (such as kNN, linear or quadratic classifiers, recursive partitioning, etc) to DD-plots, which is particularly useful, because it gives insights based on the diagnostics of these methods; and third, by integrating various sources of information (data depths, multivariate functional data, etc) in the classification procedure in a unified way. This paper also proposes an enhanced revision of several functional data depths and it provides a simulation study and applications to some real data sets.},
	number = {1},
	journal = {Test},
	author = {Cuesta-Albertos, J. A. and Febrero-Bande, M. and Oviedo de la Fuente, M.},
	year = {2017},
	keywords = {DD-classifier, Functional data analysis, Functional depths},
	pages = {119--142},
	file = {PDF:/home/miguel/Zotero/storage/7CF4THDB/2017_test_cuesta_ddg.pdf:application/pdf},
}

@techreport{carlos_coordinacion_nodate,
	title = {Coordinación de la {Política} {Social}: criterios para avanzar},
	url = {www.indes.org},
	author = {Carlos, Isabel Licha and Molina, Gerardo},
	note = {ISBN: 2026232008},
	file = {PDF:/home/miguel/Zotero/storage/KWEND5WK/Coordinación-de-la-Política-Social-Criterios-para-Avanzar (1).pdf:application/pdf},
}

@article{cancelliere_are_2011,
	title = {Are workplace health promotion programs effective at improving presenteeism in workers? {A} systematic review and best evidence synthesis of the literature},
	volume = {11},
	issn = {14712458},
	doi = {10.1186/1471-2458-11-395},
	abstract = {Background: Presenteeism is highly prevalent and costly to employers. It is defined as being present at work, but limited in some aspect of job performance by a health problem. Workplace health promotion (WHP) is a common strategy used to enhance on-the-job productivity. The primary objective is to determine if WHP programs are effective in improving presenteeism. The secondary objectives are to identify characteristics of successful programs and potential risk factors for presenteeism. Methods: The Cochrane Library, Medline, and other electronic databases were searched from 1990 to 2010. Reference lists were examined, key journals were hand-searched and experts were contacted. Included studies were original research that contained data on at least 20 participants (≥ 18 years of age), and examined the impacts of WHP programs implemented at the workplace. The Effective Public Health Practice Project Tool for Quantitative Studies was used to rate studies. 'Strong' and 'moderate' studies were abstracted into evidence tables, and a best evidence synthesis was performed. Interventions were deemed successful if they improved the outcome of interest. Their program components were identified, as were possible risk factors contributing to presenteeism. Results: After 2,032 titles and abstracts were screened, 47 articles were reviewed, and 14 were accepted (4 strong and 10 moderate studies). These studies contained preliminary evidence for a positive effect of some WHP programs. Successful programs offered organizational leadership, health risk screening, individually tailored programs, and a supportive workplace culture. Potential risk factors contributing to presenteeism included being overweight, a poor diet, a lack of exercise, high stress, and poor relations with co-workers and management. Limitations: This review is limited to English publications. A large number of reviewed studies (70\%) were inadmissible due to issues of bias, thus limiting the amount of primary evidence. The uncertainties surrounding presenteeism measurement is of significant concern as a source of bias. Conclusions: The presenteeism literature is young and heterogeneous. There is preliminary evidence that some WHP programs can positively affect presenteeism and that certain risk factors are of importance. Future research would benefit from standard presenteeism metrics and studies conducted across a broad range of workplace settings. © 2011 Cancelliere et al.; licensee BioMed Central Ltd.},
	journal = {BMC Public Health},
	author = {Cancelliere, Carol and Cassidy, J. David and Ammendolia, Carlo and Côté, Pierre},
	year = {2011},
	pmid = {21615940},
	file = {PDF:/home/miguel/Zotero/storage/BK9LWP83/CANCELLIERE ET AL 2020.pdf:application/pdf},
}

@techreport{noauthor_nocion_nodate,
	title = {Noción de empresa {Septiembre} 2021},
	file = {PDF:/home/miguel/Zotero/storage/FHM938PE/ConceptoEmpresa_Integralidad_Septiembre_VF (002).pdf:application/pdf},
}

@techreport{noauthor_behavioral_2020,
	title = {Behavioral {Health} {Toolbox} for {Families} {Supporting} {Children} and {Teens} {During} the {COVID}-19 {Pandemic} {Behavioral} {Health} {Toolbox} for {Families}: {Supporting} {Children} and {Teens} {During} the {COVID}-19 {Pandemic}},
	abstract = {This toolbox provides tips on how to navigate some of the emotional responses that families may experience during the COVID-19 pandemic. The purpose of this toolbox is to provide general information about common emotional responses of children, teens, and families during disasters. Families, parents, caregivers, and educators can use this information to help children, teens, and families recover from disasters and grow stronger. Using this Toolbox This toolbox opens with an overview of the common emotional impacts of COVID-19. As outlined in the Contents section on the next page, the sections following provide more detailed information on these age groups: 1) Toddlers and preschool children 2) School-age children 3) Teens Each age-specific section includes information on common emotional responses, helping children heal and grow, and managing feelings and behaviors children may experience. Additionally, the document provides impacts of disasters on education and self-care recommendations for parents and caregivers. Additional resource materials are provided in the Additional Resources section.},
	year = {2020},
	file = {PDF:/home/miguel/Zotero/storage/3BG2BL53/BHG-COVID19-FamilyToolbox.pdf:application/pdf},
}

@article{noauthor_dialnet-lacodificacionenelmetododeinvestigaciondelagrounde-5248462_nodate,
	title = {Dialnet-{LaCodificacionEnElMetodoDeInvestigacionDeLaGrounde}-5248462},
	file = {PDF:/home/miguel/Zotero/storage/A2H5RHBI/Dialnet-LaCodificacionEnElMetodoDeInvestigacionDeLaGrounde-5248462.pdf:application/pdf},
}

@article{bohlen_combinations_2020,
	title = {Do combinations of behavior change techniques that occur frequently in interventions reflect underlying theory?},
	volume = {54},
	issn = {15324796},
	doi = {10.1093/abm/kaaa078},
	abstract = {Background Behavioral interventions typically include multiple behavior change techniques (BCTs). The theory informing the selection of BCTs for an intervention may be stated explicitly or remain unreported, thus impeding the identification of links between theory and behavior change outcomes. Purpose This study aimed to identify groups of BCTs commonly occurring together in behavior change interventions and examine whether behavior change theories underlying these groups could be identified. Methods The study involved three phases: (a) a factor analysis to identify groups of co-occurring BCTs from 277 behavior change intervention reports; (b) examining expert consensus (n = 25) about links between BCT groups and behavioral theories; (c) a comparison of the expert-linked theories with theories explicitly mentioned by authors of the 277 intervention reports. Results Five groups of co-occurring BCTs (range: 3-13 BCTs per group) were identified through factor analysis. Experts agreed on five links (≥80\% of experts), comprising three BCT groups and five behavior change theories. Four of the five BCT group-theory links agreed by experts were also stated by study authors in intervention reports using similar groups of BCTs. Conclusions It is possible to identify groups of BCTs frequently used together in interventions. Experts made shared inferences about behavior change theory underlying these BCT groups, suggesting that it may be possible to propose a theoretical basis for interventions where authors do not explicitly put forward a theory. These results advance our understanding of theory use in multicomponent interventions and build the evidence base for further understanding theory-based intervention development and evaluation.},
	number = {11},
	journal = {Annals of Behavioral Medicine},
	author = {Bohlen, Lauren Connell and DPhil, Susan Michie and de Bruin, Marijn and Rothman, Alexander J. and Kelly, Michael P. and Groarke, Hilary N.K. and Carey, Rachel N. and Hale, Joanna and Johnston, Marie},
	month = nov,
	year = {2020},
	pmid = {32959875},
	note = {Publisher: Oxford University Press},
	keywords = {Behavior change theory, Intervention design, Intervention evaluation, Multicomponent intervention},
	pages = {827--842},
	file = {PDF:/home/miguel/Zotero/storage/BVND5L5H/Do_Combinations_of_Behavior_Change_Techniques_That.pdf:application/pdf},
}

@article{sun_employee_2019,
	title = {Employee {Engagement}: {A} {Literature} {Review}},
	volume = {9},
	doi = {10.5296/ijhrs.v9i1.14167},
	abstract = {Employee engagement is an important issue in management theory and practice. However, there are still major differences in the concept, theory, influencing factors and outcomes of employee engagement, and there is still no authoritative standard. This paper attempts to review and summarize previous research results on employee engagement. Two kinds of definitions of employee engagement are identified: employee engagement as a multi-faceted construct (cognition, emotions and behaviors) and as a unitary construct (a positive state of mind, a dedicated willingness, the opposite of burnout). Three theoretical frameworks are used to explain the varying degrees of employee engagement: Needs-Satisfaction framework, Job Demands-Resources model and Social Exchange Theory. The influencing factors of employee engagement are divided into three categories: organizational factors (management style, job rewards, etc.), job factors (work environment, task characteristics, etc.) and individual factors (physical energies, self-consciousness, etc.). Employee engagement is found to have a positive relationship with individual performance (organizational commitment, positive behavior, etc.) and organizational performance (customer satisfaction, financial return, etc.) The research findings show that there are three shortcomings in previous studies: lack of research on demographic variables, personality differences and cross-cultural differences in employee engagement, lack of research on the mediating or moderating role of employee engagement, and lack of intervention mechanism for employee engagement.},
	number = {1},
	journal = {International Journal of Human Resource Studies},
	author = {Sun, Li and Bunchapattanasakda, Chanchai},
	month = jan,
	year = {2019},
	note = {Publisher: Macrothink Institute, Inc.},
	pages = {63},
	file = {PDF:/home/miguel/Zotero/storage/ARTLTMQ5/Employee_Engagement_A_Literature_Review.pdf:application/pdf},
}

@book{lerda_integracion_2003,
	title = {Integración, coherencia y coordinación de políticas públicas sectoriales : (reflexiones para el caso de las políticas fiscal y ambiental)},
	isbn = {92-1-322308-0},
	abstract = {"Noviembre de 2003." "LC/L.2026-P"--Title page verso. "Proyecto CEPAL/Sociedad Alemana de Cooperación Técnica (GTZ), 'Promoción del desarrollo económico en América Latina y el Caribe, por medio de la integración de propuestas de políticas ambientales y sociales.'"},
	publisher = {CEPAL, División de Desarrollo Sostenible y Asentamientos Humanos},
	author = {Lerda, Juan Carlos. and Acquatella, Jean. and Gómez, José Javier. and {United Nations. Economic Commission for Latin America and the Caribbean. Sustainable Development and Human Settlements Division.} and {Deutsche Gesellschaft für Technische Zusammenarbeit.}},
	year = {2003},
	file = {PDF:/home/miguel/Zotero/storage/RWYH26JX/LERDA2003.pdf:application/pdf},
}

@techreport{einfeld_nudge_nodate,
	title = {{NUDGE} {AND} {EVIDENCE} {BASED} {POLICY}: {FERTILE} {GROUND}?},
	abstract = {Nudging is an approach to public policy development that changes the decision making environment to encourage citizens to make a particular choice. The approach has been eagerly adopted by administrations around the world, with some governments establishing dedicated units, or Behavioural Insights Teams, to advance the use of Nudges. One reason proposed for Nudge's use is that it supports evidence based policy. Nudging has positioned itself firmly in evidence based policy rhetoric. For example, Behavioural Insight Teams have emphasised and encouraged the use of Randomised Control Trials as the best way to determine the effectiveness of a policy, arguing they can be simple to implement, cost effective and save money for government in the long term. There is little empirical understanding on whether Nudge's association with evidence based policy rhetoric has contributed to its popularity. This research seeks to understand how nudge is understood in relation to the evidence based movement, from the perspective of those designing, developing and implementing nudge policies. This paper finds policy makers perceive an interconnected relationship between Nudging and evidence based policy, with each providing fertile ground for the growth of the other. Implications for scholarship and practice are discussed.},
	author = {Einfeld, Colette},
	keywords = {Nudge, Behavioural Insights, Evidence Based Policy, Policy workers 3},
	file = {PDF:/home/miguel/Zotero/storage/X5TK6UHL/EINFELD 2017.pdf:application/pdf},
}

@article{patel_nudge_2018,
	title = {Nudge {Units} to {Improve} the {Delivery} of {Health} {Care}},
	volume = {378},
	issn = {0028-4793},
	doi = {10.1056/nejmp1712984},
	abstract = {The final common pathway for the application of nearly every advance in medicine is human behavior. No matter how effective a drug, how protective a vaccine, or how targeted a therapy may be, a clinician usually has to prescribe it, and a patient accept and use it as directed, for it to improve health. Clinicians' and patients' environments influence their decisions about taking these actions, and the seemingly subtle design of information and choices can have outsize effects on our behavior. When the "choice architecture" is designed to influence behavior in a predictable way but without restricting choice, it is often called a "nudge." Key information and important choices are constantly being presented in health care. 1 Consider the way in which a physician offers influenza vaccination, or the default settings in electronic health records (EHRs) for the duration of a new opioid prescription. Yet often, these frames or default options are selected haphazardly, without attention to shared goals of overcoming common barriers to vaccination or balancing pain relief against addiction risk. Or consider the conventional deployment of order-entry systems in EHRs. Their presentation of choices is often based on conventions or design intuitions, such as listing options alphabetically or by the service providing them. Little attention is paid to the potential effect of presenting choices strategically, and typically the relative effectiveness of alternative presentations hasn't been tested. Do we really want to list drugs for a given indication from A to Z, inadvertently guiding prescribers to choose a product that starts with a letter earlier in the alphabet when later options might be more effective, less expensive, or both? Other industries that went digital long ago have developed expertise in presenting choices in ways that strongly influence consumer behavior. For example, airlines require consumers to actively choose whether to purchase trip insurance before they can buy a plane ticket. Amazon displays additional, complementary items alongside the purchase you are about to make. Netflix changed default settings to automatically play the next episode in a TV series to encourage binge watching. Similar opportunities exist to direct clinicians and patients toward better health care in situations where there's consensus about desired behaviors.},
	number = {3},
	journal = {New England Journal of Medicine},
	author = {Patel, Mitesh S. and Volpp, Kevin G. and Asch, David A.},
	month = jan,
	year = {2018},
	pmid = {29342387},
	note = {Publisher: New England Journal of Medicine (NEJM/MMS)},
	pages = {214--216},
	file = {PDF:/home/miguel/Zotero/storage/HSA9VV8N/PATEL 2018.pdf:application/pdf},
}

@techreport{heyvaert_testing_nodate,
	title = {Testing the {Intervention} {Effect} in {Single}-{Case} {Experiments}: {A} {Monte} {Carlo} {Simulation} {Study}},
	author = {Heyvaert, Mieke and Moeyaert, Mariola and Verkempynck, Paul and Van Den Noortgate, Wim and Vervloet, Marlies and Ugille, Maaike and Onghena, Patrick},
	file = {PDF:/home/miguel/Zotero/storage/KIL5H9KL/JXE_Heyvaert et al.pdf:application/pdf},
}

@article{lally_promoting_2013,
	title = {Promoting habit formation},
	volume = {7},
	issn = {17437199},
	doi = {10.1080/17437199.2011.603640},
	abstract = {Habits are automatic behavioural responses to environmental cues, thought to develop through repetition of behaviour in consistent contexts. When habit is strong, deliberate intentions have been shown to have a reduced influence on behaviour. The habit concept may provide a mechanism for establishing new behaviours, and so healthy habit formation is a desired outcome for many interventions. Habits also however represent a potential challenge for changing ingrained unhealthy behaviours, which may be resistant to motivational shifts. This review aims to provide intervention developers with tools to help establish target behaviours as habits, based on theoretical and empirical insights. We discuss evidence-based techniques for forming new healthy habits and breaking existing unhealthy habits. To promote habit formation we focus on strategies to initiate a new behaviour, support context-dependent repetition of this behaviour, and facilitate the development of automaticity. We discuss techniques for disrupting existing unwanted habits, which relate to restructuring the personal environment and enabling alternative responses to situational cues. © 2013 Copyright Taylor and Francis Group, LLC.},
	number = {SUPPL1},
	journal = {Health Psychology Review},
	author = {Lally, Phillippa and Gardner, Benjamin},
	month = may,
	year = {2013},
	keywords = {automaticity, habit, health behaviour, intervention, social cognition},
	file = {PDF:/home/miguel/Zotero/storage/FD4MEERB/Promoting habit formation.pdf:application/pdf},
}

@techreport{noauthor_adolescentes_nodate,
	title = {Adolescentes {Estrategia} digital},
	file = {PDF:/home/miguel/Zotero/storage/J3TYS2UK/presentación-adolescentes-convertido-comprimido.pdf:application/pdf},
}

@techreport{dimant_meta-nudging_nodate,
	title = {Meta-{Nudging} {Honesty}: {Past}, {Present}, and {Future} of the {Research} {Frontier}},
	abstract = {Achieving successful behavior change via nudging is hard. This is particularly true when choice architects attempt to change behavior that is collectively harmful but individually beneficial. In this paper, we review the state-of-the-art of the behavior change literature to assess both robust evidence on the motives for lying and promising interventions to curb lying. Existing literature points to combining simple behavioral interventions (e.g., norm-nudging) with interventions that contain pecuniary consequences (e.g., norm enforcement via punishment). In this context, we also discuss the idea of 'meta-nudging': rather than pursuing the classical approach to nudge targeted behavior directly, one may instead want to nudge behavior indirectly by targeting those who are in positions of power and have the ability to enforce norm adherence of others. Research suggests that delegating the enforcement of norm prescriptions can be a promising approach to nudge honesty.},
	author = {Dimant, Eugen and Shalvi, Shaul},
	keywords = {Nudging, Behavior Change, Honesty, Lying},
	file = {PDF:/home/miguel/Zotero/storage/R47HVZ8W/SSRN-id4081493.pdf:application/pdf},
}

@article{hastings_theory_2020,
	title = {Theory and ontology in behavioural science},
	volume = {4},
	issn = {23973374},
	doi = {10.1038/s41562-020-0826-9},
	number = {3},
	journal = {Nature Human Behaviour},
	author = {Hastings, Janna and Michie, Susan and Johnston, Marie},
	month = mar,
	year = {2020},
	pmid = {32066979},
	note = {Publisher: Nature Research},
	pages = {226},
	file = {PDF:/home/miguel/Zotero/storage/993UHKK2/s41562-020-0826-9.pdf:application/pdf},
}

@techreport{noauthor_nudging_nodate,
	title = {Nudging: {A} {Very} {Short} {Guide}},
	url = {http://nrs.harvard.edu/urn-3:HUL.InstRepos:16205305},
	file = {PDF:/home/miguel/Zotero/storage/VRB5FDDG/SUNSTEIN 2014.pdf:application/pdf},
}

@book{noauthor_behavioural_nodate,
	title = {Behavioural {Insights} {Toolkit} : {Social} {Research} and {Evaluation} {Division}, {Department} for {Transport}.},
	isbn = {978-1-84864-130-3},
	abstract = {November 2011.},
	file = {PDF:/home/miguel/Zotero/storage/UQMCY83U/toolkit.pdf:application/pdf},
}

@techreport{fernanda_impacto_nodate,
	title = {{IMPACTO} {DE} {LA} {ECONOMÍA} {DEL} {COMPORTAMIENTO} {EN} {LA} {PRÁCTICA} {DE} {ACTIVIDAD} {FÍSICA} {COMO} {ESTRATEGIA} {DE} {PREVENCIÓN} {DEL} {RIESGO} {CARDIOVASCULAR}},
	author = {Fernanda, Luisa and Muñoz, Rodríguez and Orlando, Cristian and Gómez, Sánchez and Ochoa, Julián Arango},
	file = {PDF:/home/miguel/Zotero/storage/4EAJBIU8/TG - Luisa Rodriguez Muñoz - Cristian Sanchez Gomez.pdf:application/pdf},
}

@techreport{noauthor_behavioral_nodate,
	title = {Behavioral {Insights} {Toolkit}},
	file = {PDF:/home/miguel/Zotero/storage/FTX3J295/17rpirsbehavioralinsights.pdf:application/pdf},
}

@article{noauthor_ppt_nodate,
	title = {{PPT} def {Hábitos} 2},
	file = {PDF:/home/miguel/Zotero/storage/WXBA7LXD/PPT def Hábitos 2.pdf:application/pdf},
}

@incollection{gardner_habit_2019,
	title = {Habit {Formation} and {Behavior} {Change}},
	abstract = {and Keywords Within psychology, the term habit refers to a process whereby contexts prompt action au­ tomatically, through activation of mental context-action associations learned through pri­ or performances. Habitual behavior is regulated by an impulsive process, and so can be elicited with minimal cognitive effort, awareness, control, or intention. When an initially goal-directed behavior becomes habitual, action initiation transfers from conscious moti­ vational processes to context-cued impulse-driven mechanisms. Regulation of action be­ comes detached from motivational or volitional control. Upon encountering the associat­ ed context, the urge to enact the habitual behavior is spontaneously triggered and alter­ native behavioral responses become less cognitively accessible. By virtue of its cue-dependent automatic nature, theory proposes that habit strength will predict the likelihood of enactment of habitual behavior, and that strong habitual tenden­ cies will tend to dominate over motivational tendencies. Support for these effects has been found for many health-related behaviors, such as healthy eating, physical activity, and medication adherence. This has stimulated interest in habit formation as a behavior change mechanism: It has been argued that adding habit formation components into be­ havior change interventions should shield new behaviors against motivational lapses, making them more sustainable in the long-term. Interventions based on the habit-forma­ tion model differ from non-habit-based interventions in that they include elements that promote reliable context-dependent repetition of the target behavior, with the aim of es­ tablishing learned context-action associations that manifest in automatically cued behav­ ioral responses. Interventions may also seek to harness these processes to displace an ex­ isting "bad" habit with a "good" habit. Research around the application of habit formation to health behavior change interven­ tions is reviewed, drawn from two sources: extant theory and evidence regarding how habit forms, and previous interventions that have used habit formation principles and techniques to change behavior. Behavior change techniques that may facilitate movement through discrete phases in the habit formation trajectory are highlighted, and techniques that have been used in previous interventions are explored based on a habit formation framework. Although these interventions have mostly shown promising effects on behav­ ior, the unique impact on behavior of habit-focused components and the longevity of such effects are not yet known. As an intervention strategy, habit formation has been shown to},
	booktitle = {Oxford {Research} {Encyclopedia} of {Psychology}},
	publisher = {Oxford University Press},
	author = {Gardner, Benjamin and Rebar, Amanda L.},
	month = apr,
	year = {2019},
	doi = {10.1093/acrefore/9780190236557.013.129},
	file = {PDF:/home/miguel/Zotero/storage/KECLABMH/acrefore-9780190236557-e-129.pdf:application/pdf},
}

@article{ryan_exploring_2021,
	title = {Exploring the active ingredients of workplace physical and psychological wellbeing programs: a systematic review},
	volume = {11},
	issn = {16139860},
	doi = {10.1093/tbm/ibab003},
	abstract = {Previous reviews have established that workplace wellbeing initiatives are effective at promoting wellbeing, but less is known about which intervention characteristics or "active ingredients"underpin this effectiveness (i.e., behavior change techniques [BCTs]). This review aims to illuminate the connections between the types of BCTs and the level of intervention intensity with intervention effectiveness. A systematic search for peer-reviewed studies evaluating a workplace wellbeing initiative was undertaken across five databases: Medline, Scopus, PsycInfo, and CINAHL (Ovid Emcare). Eligible studies included those that evaluated the effect of a workplace wellbeing initiative on participants' physical wellbeing (e.g., physical activity and quality of life) and psychological wellbeing (e.g., mental health and stress), were published between 2009 and September 2019, and utilized a comparator (e.g., control group or prepost change). Studies were screened in independent duplicate to minimize bias. Effect sizes were calculated. Following removal of duplicates, 1,541 studies were identified and screened for eligibility. Of these, 23 studies reporting 28 comparisons were deemed to meet eligibility criteria. Just over 50\% of these studies reported evidence of either a strong or moderate effect across a physical and a psychological outcome, providing a positive indication that workplace wellbeing programs can promote physical and psychological wellbeing in workers. Interventions tended to employ multiple BCTs (mean range 8.1-9.4), however, no discernible patterns between the types or numbers of BCTs employed and intervention effectiveness was found. Further experimental work is required that compares and contrasts workplace wellbeing initiatives to enable a better understanding of how to develop and implement highly effective programs.},
	number = {5},
	journal = {Translational Behavioral Medicine},
	author = {Ryan, J. C. and Williams, G. and Wiggins, B. W. and Flitton, A. J. and McIntosh, J. T. and Carmen, M. J. and Cox, D. N.},
	month = may,
	year = {2021},
	pmid = {33677571},
	note = {Publisher: Oxford University Press},
	keywords = {Behavior change techniques, Health promotion, Physical wellbeing, Psychological wellbeing, Workplace wellbeing initiatives},
	pages = {1127--1141},
	file = {PDF:/home/miguel/Zotero/storage/L2CSIMJF/RYAN2020.pdf:application/pdf},
}

@techreport{noauthor_basic_nodate,
	title = {{THE} {BASIC} {TOOLKIT} {TOOLS} {AND} {ETHICS} {FOR} {APPLIED} {BEHAVIOURAL} {INSIGHTS}},
	url = {http://oe.cd/BASIC},
	file = {PDF:/home/miguel/Zotero/storage/HGX3X9CK/BASIC-Toolkit-web.pdf:application/pdf},
}

@article{noauthor_0375_nodate,
	title = {0375. {El} poder de los hábitos  {Por} qué hacemos lo que hacemos en la vida y en la empresa},
	file = {PDF:/home/miguel/Zotero/storage/IKZWLURN/0375. El poder de los hábitos  Por qué hacemos lo que hacemos en la vida y en la empresa.pdf:application/pdf},
}

@techreport{palmer_how_nodate,
	title = {How might we apply knowledge from the field of behavioral science to increase the chances new products and services will succeed? {Project} team},
	abstract = {Opportunity Client The innovation and design consultancy Doblin is committed to human-centeredness. Concept development projects always start with establishing empathy with stakeholders. They start with extensive user research. Later in the process, when the research data is collected and analyzed, Doblin concept development teams often noticed similar insights about people from one project to the next. They were identifying and re-identifying biases and heuristics codified and cataloged by the field of behavioral science. Doblin project manager and IIT Institute of Design adjunct professor of behavioral economics Ruth Schmidt was one of the Doblin employees with extensive knowledge of behavioral science. Naturally, she shared her knowledge with the project teams with which she worked, but formal knowledge of behavioral science wasn't universally known. Ruth wanted to create a resource to share these innate human tendencies, these biases and heuristics of behavioral science with all Doblin concept development teams. She wanted to create a tool that could assist them at critical stages in the process of developing new products and services, presenting just the right information when it was needed to make them more accommodating and ultimately, more successful. Ruth picked a team and successfully lobbied for them to work on the effort as an internal project for six weeks. Under her oversight, the Behavioral Design Toolkit was created.},
	author = {Palmer, Brewer and Keck, Doblin Paul and Schmidt, Ruth},
	file = {PDF:/home/miguel/Zotero/storage/RWMWNKQX/behavioraldesigntoolkit.pdf:application/pdf},
}

@book{michie_abc_nodate,
	title = {{ABC} of behaviour change theories},
	isbn = {978-1-912141-09-8},
	author = {Michie, Susan and West, Robert and Campbell, Rona and Brown, Jamie and Gainforth, Heather},
	file = {PDF:/home/miguel/Zotero/storage/RQHNLMQH/abc-of-behaviour-change-theories.pdf:application/pdf},
}

@techreport{dellavigna_berkeley_rcts_2020,
	title = {{RCTs} to {Scale}: {Comprehensive} {Evidence} from {Two} {Nudge} {Units} *},
	abstract = {Nudge interventions-behaviorally-motivated design changes with no financial incentives-have quickly expanded from academic studies to larger implementation in so-called Nudge Units in governments. This provides an opportunity to compare interventions in research studies, versus at scale. We assemble a unique data set of 126 RCTs covering over 23 million individuals, including all trials run by two of the largest Nudge Units in the United States. We compare these trials to a separate sample of nudge trials published in academic journals from two recent meta-analyses. In papers published in academic journals, the average impact of a nudge is very large-an 8.7 percentage point take-up effect, a 33.5\% increase over the average control. In the Nudge Unit trials, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.1\% increase. We consider five potential channels for this gap: statistical power, selective publication, academic involvement, differences in trial features and in nudge features. Publication bias in the academic journals, exacerbated by low statistical power, can account for the full difference in effect sizes. Academic involvement does not account for the difference. Different features of the nudges, such as in-person versus letter-based communication, likely reflecting institutional constraints, can partially explain the different effect sizes. We conjecture that larger sample sizes and institutional constraints, which play an important role in our setting, are relevant in other at-scale implementations. Finally, we compare these results to the predictions of academics and practitioners. Most forecasters overestimate the impact for the Nudge Unit interventions, though nudge practitioners are almost perfectly calibrated. * We are very grateful to the Office of Evaluation Sciences and Behavioral Insights Team North America for supporting this project and for countless suggestions and feedback. We thank},
	author = {DellaVigna Berkeley, Stefano UC and Elizabeth Linos, Nber and Abeler, Johannes and Andrews, Isaiah and Bandiera, Oriana and Benartzi, Shlomo and Beshears, John and Card, David and Enke, Benjamin and Green, Etan and Johnson, Eric and Kasy, Maximilian and Laibson, David and Loewenstein, George and Meager, Rachael and Milkman, Katherine and Rao, Gautam and Sacarny, Adam and Sunstein, Cass and Thaler, Richard and Vivalt, Eva and Zeckhauser and, Richard},
	year = {2020},
	file = {PDF:/home/miguel/Zotero/storage/Z9DNNZ7Y/DELLA VIGNA 2020.pdf:application/pdf},
}

@article{khadilkar_understanding_2020,
	title = {Understanding behavioural design: barriers and enablers},
	volume = {31},
	issn = {14661837},
	doi = {10.1080/09544828.2020.1836611},
	abstract = {Behavioural design has emerged as an important domain of design research and practice. However, there is a need to better distinguish behavioural design and subsequently map its unique features and challenges. In answer to this need, this work examines the evolving role of behaviour in design and contrasts this with eleven in-depth behavioural design cases. This has resulted in our identification of behavioural design as a new paradigm of design, with a number of unique characteristics. Based on this, we propose a model of behavioural design. Furthermore, we identify three critical barriers to behavioural design in practice, and suggest ten mitigating enablers. Together these findings provide implications for further development of theory, practice, and education in behavioural design.},
	number = {10},
	journal = {Journal of Engineering Design},
	author = {Khadilkar, Pramod Ratnakar and Cash, Philip},
	year = {2020},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Behavioural design, design process, design theory, user behaviour, user-centred design},
	pages = {508--529},
	file = {PDF:/home/miguel/Zotero/storage/JJ8N5WVR/Khadilkar_Cash_2020_Understandingbehaviouraldesignbarriersandenablers.pdf:application/pdf},
}

@book{schmied_behaviour_nodate,
	title = {Behaviour change toolkit : for international development practitioners : enabling people to practice positive behaviours},
	isbn = {978-80-87456-83-5},
	abstract = {Přeloženo? Název z obálky.},
	author = {Schmied, Petr.},
	file = {PDF:/home/miguel/Zotero/storage/7SRCKSWK/pin-2017-behaviour-change-toolkit_mail.pdf:application/pdf},
}

@techreport{noauthor_quieres_nodate,
	title = {¿{Quieres} conectarte con una vida saludable?},
	file = {PDF:/home/miguel/Zotero/storage/9MF7XQNW/Modelo Saludables  04-05-21  - V1 - PARA COMUNICACIONES.pdf:application/pdf},
}

@techreport{halpern_nudging_nodate,
	title = {Nudging by government: {Progress}, impact and lessons learnt},
	abstract = {"Nudge units" within governments, most notably in the United Kingdom and the United States, seek to encourage people to behave a certain way by using insights gained from behavioral science. The aim is to influence people's choices through policies that offer the right incentive or hurdle so that people choose the more economically beneficial options. Getting people to save for retirement, eat more healthful foods, and pay their taxes on time are some examples of institutionally desirable activities. The 10-fold rise in "nudge" projects undertaken since 2010-more than 20 countries have deployed or expressed interest in them-have revealed many lessons for policymakers. Chief among these lessons: the necessity of obtaining buy-in from key political leaders and other stakeholders, and the benefits of testing multiple intervention strategies at once. Although detailed cost-benefit analyses are not yet available, we estimate that behaviorally inspired interventions can help government agencies save hundreds of millions of dollars per year.},
	author = {Halpern, David and Sanders, Michael},
	file = {PDF:/home/miguel/Zotero/storage/FRPUNL6R/SANDERS 2016.pdf:application/pdf},
}

@article{noauthor_ppt_nodate-1,
	title = {{PPT} def {Hábitos} gestoras},
	file = {PDF:/home/miguel/Zotero/storage/DAKGMXYI/PPT def Hábitos gestoras.pdf:application/pdf},
}

@article{noauthor_disenocomportamiento_armayones1_nodate,
	title = {{DisenoComportamiento}\_Armayones1},
	file = {PDF:/home/miguel/Zotero/storage/4VZ22EJ8/DisenoComportamiento_Armayones1.pdf:application/pdf},
}

@article{noauthor_psicolol_tecnicas_modificacion_conducta_labrador_nodate,
	title = {{PSICOLOL}\_TECNICAS\_MODIFICACION\_CONDUCTA\_Labrador},
	file = {PDF:/home/miguel/Zotero/storage/HQNP537N/PSICOLOL_TECNICAS_MODIFICACION_CONDUCTA_Labrador.pdf:application/pdf},
}

@techreport{noauthor_fases_nodate,
	title = {{LAS} {FASES} {DEL} {CAMBIO}: {EL} {MODELO} {TRANSTEÓRICO} {DE} {PROCHASKA} {Y} {DICLEMENTE}},
	url = {https://tejedordehistorias.wordpress.com/2015/12/10/las-fases-del-cambio-el-modelo-},
	abstract = {https://tejedordehistorias.wordpress.com/2015/12/10/las-fases-del-cambio-el-modelo-transteorico-de-prochaska-y-diclemente/ 10 diciembre, 2015Jonatan Molina TorresEntrevista motivacional, Fases de cambio, Modelo transteórico de cambio, Prochaska y DiClemente Existen muchas definiciones de la psicología a lo largo de la historia, pero la mayoría de ellas comparte un elemento común que define la psicología como una ciencia que busca un cambio en las personas. Y no les falta razón. Generalmente el objetivo que persigue un psicólogo es que exista, tras la terapia, una variación de un determinado comportamiento de la persona o grupo que es definido como conducta problema u objetivo. Dependiendo de su naturaleza, lo que el psicólogo intentará será reducir la conducta (fumar menos cigarrillos para un fumador, tener menos rabietas para un niño con problemas de conducta,…) o aumentarla (hacer deporte en una persona con obesidad, entablar interacciones con personas en un fóbico social,…). Sin embargo, antes de centrarse en el cambio comportamental, el psicólogo debe hacer frente a un reto todavía más complejo: el cambio motivacional. Este paso debe anteceder siempre a la intervención propiamente dicha porque es más fácil que cambien aquellas personas que están dispuestas a cambiar. Es importante conocer en qué estado motivacional se encuentra el paciente para saber si es factible pedirle un cambio de comportamiento o si todavía se está en una fase prematura.},
	file = {PDF:/home/miguel/Zotero/storage/F9PAXYDQ/LAS-FASES-DEL-CAMBIO-EL-MODELO-TRANSTEÓRICO-DE-PROCHASKA-Y-DICLEMENTE.pdf:application/pdf},
}

@techreport{noauthor_diagnostico_nodate,
	title = {{DIAGNÓSTICO} {SITUACIONAL}-{FASES} {DEL} {CAMBIO}},
	file = {PDF:/home/miguel/Zotero/storage/77WV87FG/MODULO 5 - 5.1  DIAGNÓSTICO SITUACIONAL - FASES DEL CAMBIO.pdf:application/pdf},
}

@article{michie_behaviour_2011,
	title = {The behaviour change wheel: {A} new method for characterising and designing behaviour change interventions},
	volume = {6},
	issn = {17485908},
	doi = {10.1186/1748-5908-6-42},
	abstract = {Background: Improving the design and implementation of evidence-based practice depends on successful behaviour change interventions. This requires an appropriate method for characterising interventions and linking them to an analysis of the targeted behaviour. There exists a plethora of frameworks of behaviour change interventions, but it is not clear how well they serve this purpose. This paper evaluates these frameworks, and develops and evaluates a new framework aimed at overcoming their limitations.Methods: A systematic search of electronic databases and consultation with behaviour change experts were used to identify frameworks of behaviour change interventions. These were evaluated according to three criteria: comprehensiveness, coherence, and a clear link to an overarching model of behaviour. A new framework was developed to meet these criteria. The reliability with which it could be applied was examined in two domains of behaviour change: tobacco control and obesity.Results: Nineteen frameworks were identified covering nine intervention functions and seven policy categories that could enable those interventions. None of the frameworks reviewed covered the full range of intervention functions or policies, and only a minority met the criteria of coherence or linkage to a model of behaviour. At the centre of a proposed new framework is a 'behaviour system' involving three essential conditions: capability, opportunity, and motivation (what we term the 'COM-B system'). This forms the hub of a 'behaviour change wheel' (BCW) around which are positioned the nine intervention functions aimed at addressing deficits in one or more of these conditions; around this are placed seven categories of policy that could enable those interventions to occur. The BCW was used reliably to characterise interventions within the English Department of Health's 2010 tobacco control strategy and the National Institute of Health and Clinical Excellence's guidance on reducing obesity.Conclusions: Interventions and policies to change behaviour can be usefully characterised by means of a BCW comprising: a 'behaviour system' at the hub, encircled by intervention functions and then by policy categories. Research is needed to establish how far the BCW can lead to more efficient design of effective interventions. © 2011 Michie et al; licensee BioMed Central Ltd.},
	number = {1},
	journal = {Implementation Science},
	author = {Michie, Susan and van Stralen, Maartje M. and West, Robert},
	month = apr,
	year = {2011},
	pmid = {21513547},
	file = {PDF:/home/miguel/Zotero/storage/I277XNKI/Rueda.pdf:application/pdf},
}

@book{michie_behaviour_nodate,
	title = {The behaviour change wheel : a guide to designing interventions},
	isbn = {978-1-912141-08-1},
	abstract = {This is a practical guide to designing and evaluating behaviour change interventions and policies. It is based on the Behaviour Change Wheel, a synthesis of 19 behaviour change frameworks that draw on a wide range of disciplines and approaches. The guide is for policy makers, practitioners, intervention designers and researchers and introduces a systematic, theory-based method, key concepts and practical tasks.},
	author = {Michie, Susan and Atkins, Lou and West, Robert},
	file = {PDF:/home/miguel/Zotero/storage/MUWCCCXJ/the-behaviour-change-wheel.pdf:application/pdf},
}

@techreport{freire_apropia_nodate,
	title = {{APROPIA} {ARL} {SURA} {Modelo} {Educativo} y {Pedagógico} para {Clientes} {ARL} {SURA} "{La} {Educación} transforma personas, las personas transformamos el mundo"},
	author = {Freire, Paulo},
	file = {PDF:/home/miguel/Zotero/storage/UQZYD3BL/MODELO EDUCATIVO ARL.pdf:application/pdf},
}

@article{milkman_megastudies_2021,
	title = {Megastudies improve the impact of applied behavioural science},
	volume = {600},
	issn = {14764687},
	doi = {10.1038/s41586-021-04128-4},
	abstract = {Policy-makers are increasingly turning to behavioural science for insights about how to improve citizens’ decisions and outcomes1. Typically, different scientists test different intervention ideas in different samples using different outcomes over different time intervals2. The lack of comparability of such individual investigations limits their potential to inform policy. Here, to address this limitation and accelerate the pace of discovery, we introduce the megastudy—a massive field experiment in which the effects of many different interventions are compared in the same population on the same objectively measured outcome for the same duration. In a megastudy targeting physical exercise among 61,293 members of an American fitness chain, 30 scientists from 15 different US universities worked in small independent teams to design a total of 54 different four-week digital programmes (or interventions) encouraging exercise. We show that 45\% of these interventions significantly increased weekly gym visits by 9\% to 27\%; the top-performing intervention offered microrewards for returning to the gym after a missed workout. Only 8\% of interventions induced behaviour change that was significant and measurable after the four-week intervention. Conditioning on the 45\% of interventions that increased exercise during the intervention, we detected carry-over effects that were proportionally similar to those measured in previous research3–6. Forecasts by impartial judges failed to predict which interventions would be most effective, underscoring the value of testing many ideas at once and, therefore, the potential for megastudies to improve the evidentiary value of behavioural science.},
	number = {7889},
	journal = {Nature},
	author = {Milkman, Katherine L. and Gromet, Dena and Ho, Hung and Kay, Joseph S. and Lee, Timothy W. and Pandiloski, Pepi and Park, Yeji and Rai, Aneesh and Bazerman, Max and Beshears, John and Bonacorsi, Lauri and Camerer, Colin and Chang, Edward and Chapman, Gretchen and Cialdini, Robert and Dai, Hengchen and Eskreis-Winkler, Lauren and Fishbach, Ayelet and Gross, James J. and Horn, Samantha and Hubbard, Alexa and Jones, Steven J. and Karlan, Dean and Kautz, Tim and Kirgios, Erika and Klusowski, Joowon and Kristal, Ariella and Ladhania, Rahul and Loewenstein, George and Ludwig, Jens and Mellers, Barbara and Mullainathan, Sendhil and Saccardo, Silvia and Spiess, Jann and Suri, Gaurav and Talloen, Joachim H. and Taxer, Jamie and Trope, Yaacov and Ungar, Lyle and Volpp, Kevin G. and Whillans, Ashley and Zinman, Jonathan and Duckworth, Angela L.},
	month = dec,
	year = {2021},
	pmid = {34880497},
	note = {Publisher: Nature Research},
	pages = {478--483},
	file = {PDF:/home/miguel/Zotero/storage/PSTBI2ZD/NATURE_2021.pdf:application/pdf},
}

@article{bridle_systematic_2005,
	title = {Systematic review of the effectiveness of health behavior interventions based on the transtheoretical model},
	volume = {20},
	issn = {08870446},
	doi = {10.1080/08870440512331333997},
	abstract = {The Transtheoretical Model (TTM) has gained widespread popularity and acceptance, yet little is known about its effectiveness as a basis for health behavior intervention. A systematic review was conducted in order to evaluate the effectiveness of TTM interventions in facilitating health-related behavior change. Thirty-five electronic databases, catalogues, and internet resources were searched for relevant studies. In addition, the bibliographies of retrieved references were scanned for further relevant publications and authors were contacted for further information where necessary. Thirty-seven randomized controlled trials, targeting seven health-related behaviors, satisfied the inclusion criteria. Overall, the methodological quality of trials was variable, and there was limited evidence for the effectiveness of stage-based interventions as a basis for behavior change or for facilitating stage progression, irrespective of whether those interventions were compared with other types of intervention or with no intervention or usual care controls. The theoretical and practical implications of these findings are discussed. © 2005 Taylor \& Francis Group Ltd.},
	number = {3},
	journal = {Psychology and Health},
	author = {Bridle, C. and Riemsma, R. P. and Pattenden, J. and Sowden, A. J. and Mather, L. and Watt, I. S. and Walker, A.},
	month = jun,
	year = {2005},
	keywords = {Health behavior intervention, Systematic review, Transtheoretical model},
	pages = {283--301},
	file = {PDF:/home/miguel/Zotero/storage/MC5PMQNG/Bridle pdf.pdf:application/pdf},
}

@article{kulendrarajah_how_2020,
	title = {How effective are age' tools at changing patient behaviour? {A} rapid review},
	volume = {25},
	issn = {25154478},
	doi = {10.1136/bmjebm-2019-111244},
	abstract = {Background A common form of risk communication is to relay the relative risk (\%) of an adverse outcome based on surrogate markers associated with the outcome. A novel way of communicating risk is through effective age' of a person or specific organ. These tools can be used to change patient behaviour. Objective To determine the effect of effective age' tools on patient behaviour as compared with more traditional methods of risk communication. Study selection We performed a search of the PubMed database up to February 2019 for systematic reviews and randomised controlled trials (RCT) that answered our question. Interventions were effective age' tools, comparators were usual care or alternative risk communication tools. Primary outcomes were behavioural change measures. Findings We included 1 overview of systematic reviews (level 1 evidence), 2 systematic reviews (level 1 evidence) and 13 RCTs (level 2 evidence). Both systematic reviews concluded the evidence base was not conclusive enough to make specific recommendations. Age tools assessed in the 13 RCTs were: lung age' (n=5), heart age' (n=3), health age' (n=2), cardiovascular age' (n=1), body age' (n=1) and net present value' (n=1). 7/13 (54\%) RCTs demonstrated a clinical effect on behaviour change favouring the age' tool; 2/13 (15\%) demonstrated a null effect; 4/13 (31\%) favoured control. Conclusions Our findings indicate that systematic review evidence needs updating. The evidence from RCTs on the effect of using age metrics on patient behaviour is poor. There is a need for high-quality trials to decrease uncertainty in the available evidence.},
	number = {2},
	journal = {BMJ Evidence-Based Medicine},
	author = {Kulendrarajah, Bavidra and Grey, Adam and Nunan, David},
	month = apr,
	year = {2020},
	pmid = {31558486},
	note = {Publisher: BMJ Publishing Group},
	pages = {68--72},
	file = {PDF:/home/miguel/Zotero/storage/5IGV3K56/Kulendrarajah, Grey, Nunan 2019.pdf:application/pdf},
}

@article{efird_best_2018,
	title = {Best {Practices} for {Developing} and {Validating} {Scales} for {Health}, {Social}, and {Behavioral} {Research}: {A} {Primer}},
	volume = {1},
	url = {www.frontiersin.org},
	doi = {10.3389/fpubh.2018.00149},
	abstract = {Scale development and validation are critical to much of the work in the health, social, and behavioral sciences. However, the constellation of techniques required for scale development and evaluation can be onerous, jargon-filled, unfamiliar, and resource-intensive. Further, it is often not a part of graduate training. Therefore, our goal was to concisely review the process of scale development in as straightforward a manner as possible, both to facilitate the development of new, valid, and reliable scales, and to help improve existing ones. To do this, we have created a primer for best practices for scale development in measuring complex phenomena. This is not a systematic review, but rather the amalgamation of technical literature and lessons learned from our experiences spent creating or adapting a number of scales over the past several decades. We identified three phases that span nine steps. In the first phase, items are generated and the validity of their content is assessed. In the second phase, the scale is constructed. Steps in scale construction include pre-testing the questions, administering the survey, reducing the number of items, and understanding how many factors the scale captures. In the third phase, scale evaluation, the number of dimensions is tested, reliability is tested, and validity is assessed. We have also added examples of best practices to each step. In sum, this primer will equip both scientists and practitioners to understand the ontology and methodology of scale development and validation, thereby facilitating the advancement of our understanding of a range of health, social, and behavioral outcomes.},
	journal = {Frontiers in Public Health {\textbar} www.frontiersin.org},
	author = {Efird, Jimmy Thomas and Turrini, Aida and Boateng, Godfred O and Neilands, Torsten B and Frongillo, Edward A and Melgar-Quiñonez, Hugo R and Young, Sera L},
	year = {2018},
	keywords = {content validity, factor analysis, item reduction, psychometric evaluation, scale development, tests of dimensionality, tests of reliability, tests of validity},
	pages = {149},
	file = {PDF:/home/miguel/Zotero/storage/MSC8VPP5/full-text.pdf:application/pdf},
}

@article{cozzolino_food_2022,
	title = {The {Food} {Recognition} {Benchmark}: {Using} {Deep} {Learning} to {Recognize} {Food} in {Images}},
	volume = {1},
	url = {www.frontiersin.org},
	doi = {10.3389/fnut.2022.875143},
	abstract = {The automatic recognition of food on images has numerous interesting applications, including nutritional tracking in medical cohorts. The problem has received significant research attention, but an ongoing public benchmark on non-biased (i.e., not scraped from web) data to develop open and reproducible algorithms has been missing. Here, we report on the setup of such a benchmark using publicly available food images sourced through the mobile MyFoodRepo app used in research cohorts. Through four rounds, the benchmark released the MyFoodRepo-273 dataset constituting 24,119 images and a total of 39,325 segmented polygons categorized in 273 different classes. Models were evaluated on private tests sets from the same platform with 5,000 images and 7,865 annotations in the final round. Top-performing models on the 273 food categories reached a mean average precision of 0.568 (round 4) and a mean average recall of 0.885 (round 3), and were deployed in production use of the MyFoodRepo app. We present experimental validation of round 4 results, and discuss implications of the benchmark setup designed to increase the size and diversity of the dataset for future rounds.},
	journal = {Frontiers in Nutrition {\textbar} www.frontiersin.org},
	author = {Cozzolino, Daniel and Wataru, Shimoda and Paulo Lopez-Meyer, Japan and Salathé, Marcel and Mohanty, Sharada Prasanna and Singhal, Gaurav and Scuccimarra, Eric Antoine and Kebaili, Djilani and Héritier, Harris and Boulanger, Victor},
	year = {2022},
	keywords = {artificial intelligence (AI), benchmark, deep learning, food recognition, images},
	pages = {875143},
	file = {PDF:/home/miguel/Zotero/storage/R38J265D/full-text.pdf:application/pdf},
}

@article{tahir_healthcare_2021,
	title = {healthcare {A} {Comprehensive} {Survey} of {Image}-{Based} {Food} {Recognition} and {Volume} {Estimation} {Methods} for {Dietary} {Assessment}},
	url = {https://doi.org/10.3390/healthcare9121676},
	doi = {10.3390/healthcare9121676},
	abstract = {Dietary studies showed that dietary problems such as obesity are associated with other chronic diseases, including hypertension, irregular blood sugar levels, and increased risk of heart attacks. The primary cause of these problems is poor lifestyle choices and unhealthy dietary habits, which are manageable using interactive mHealth apps. However, traditional dietary monitoring systems using manual food logging suffer from imprecision, underreporting, time consumption, and low adherence. Recent dietary monitoring systems tackle these challenges by automatic assessment of dietary intake through machine learning methods. This survey discusses the best-performing methodologies that have been developed so far for automatic food recognition and volume estimation. Firstly, the paper presented the rationale of visual-based methods for food recognition. Then, the core of the study is the presentation, discussion, and evaluation of these methods based on popular food image databases. In this context, this study discusses the mobile applications that are implementing these methods for automatic food logging. Our findings indicate that around 66.7\% of surveyed studies use visual features from deep neural networks for food recognition. Similarly, all surveyed studies employed a variant of convolutional neural networks (CNN) for ingredient recognition due to recent research interest. Finally, this survey ends with a discussion of potential applications of food image analysis, existing research gaps, and open issues of this research area. Learning from unlabeled image datasets in an unsupervised manner, catastrophic forgetting during continual learning, and improving model transparency using explainable AI are potential areas of interest for future studies.},
	author = {Tahir, Ghalib Ahmed and Loo, Chu Kiong},
	year = {2021},
	keywords = {food recognition, automatic diet monitoring, feature extraction, food datasets, image analysis, interactive segmentation, volume estimation},
	file = {PDF:/home/miguel/Zotero/storage/SW5XDE4Y/full-text.pdf:application/pdf},
}

@techreport{albrecht_diagnostic_nodate,
	title = {Diagnostic instruments for behavioural addiction: an overview {Diagnostische} {Instrumente} der {Verhaltenssucht}: ein Überblick},
	abstract = {In non-substance-related addiction, the so-called behavioural addiction, no external psychotropic substances are consumed. The psychotropic},
	author = {Albrecht, Ulrike and Kirschner, Nina Ellen},
	file = {PDF:/home/miguel/Zotero/storage/KQPNTBPF/full-text.pdf:application/pdf},
}

@article{ab_ghani_subspace_2023,
	title = {Subspace {Clustering} in {High}-{Dimensional} {Data} {Streams}: {A} {Systematic} {Literature} {Review}},
	volume = {75},
	issn = {15462226},
	doi = {10.32604/cmc.2023.035987},
	abstract = {Clustering high dimensional data is challenging as data dimensionality increases the distance between data points, resulting in sparse regions that degrade clustering performance. Subspace clustering is a common approach for processing high-dimensional data by finding relevant features for each cluster in the data space. Subspace clustering methods extend traditional clustering to account for the constraints imposed by data streams. Data streams are not only high-dimensional, but also unbounded and evolving. This necessitates the development of subspace clustering algorithms that can handle high dimensionality and adapt to the unique characteristics of data streams. Although many articles have contributed to the literature review on data stream clustering, there is currently no specific review on subspace clustering algorithms in high-dimensional data streams. Therefore, this article aims to systematically review the existing literature on subspace clustering of data streams in high-dimensional streaming environments. The review follows a systematic methodological approach and includes 18 articles for the final analysis. The analysis focused on two research questions related to the general clustering process and dealing with the unbounded and evolving characteristics of data streams. The main findings relate to six elements: clustering process, cluster search, subspace search, synopsis structure, cluster maintenance, and evaluation measures. Most algorithms use a two-phase clustering approach consisting of an initialization stage, a refinement stage, a cluster maintenance stage, and a final clustering stage. The density-based top-down subspace clustering approach is more widely used than the others because it is able to distinguish true clusters and outliers using projected micro-clusters. Most algorithms implicitly adapt to the evolving nature of the data stream by using a time fading function that is sensitive to outliers. Future work can focus on the clustering framework, parameter optimization, subspace search techniques, memory-efficient synopsis structures, explicit cluster change detection, and intrinsic performance metrics. This article can serve as a guide for researchers interested in high-dimensional subspace clustering methods for data streams.},
	number = {2},
	journal = {Computers, Materials and Continua},
	author = {Ab Ghani, Nur Laila and Aziz, Izzatdin Abdul and AbdulKadir, Said Jadid},
	year = {2023},
	note = {Publisher: Tech Science Press},
	keywords = {Clustering, concept drift, data stream, evolving data stream, high dimensionality, projected clustering, stream clustering, subspace clustering},
	pages = {4649--4668},
	file = {PDF:/home/miguel/Zotero/storage/NHZE8XY3/TSP_CMC_35987.pdf:application/pdf},
}

@article{olteanu_meta-survey_2023,
	title = {Meta-survey on outlier and anomaly detection},
	volume = {555},
	issn = {18728286},
	doi = {10.1016/j.neucom.2023.126634},
	abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
	journal = {Neurocomputing},
	author = {Olteanu, Madalina and Rossi, Fabrice and Yger, Florian},
	month = oct,
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Outlier detection, Anomaly detection, Meta-survey},
	file = {PDF:/home/miguel/Zotero/storage/SK8BBDN7/1-s2.0-S0925231223007579-main.pdf:application/pdf},
}

@article{liu_efficient_2018,
	title = {Efficient {Outlier} {Detection} for {High}-{Dimensional} {Data}},
	volume = {48},
	issn = {21682232},
	doi = {10.1109/TSMC.2017.2718220},
	abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter \$\{k\}\$ of \$\{k\}\$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
	number = {12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Liu, Huawen and Li, Xuelong and Li, Jiuyong and Zhang, Shichao},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Dimension reduction, high-dimensional data, outlier detection, k nearest neighbors (kNN), low-rank approximation},
	pages = {2451--2461},
	file = {PDF:/home/miguel/Zotero/storage/JSK5FWQC/Efficient_Outlier_Detection_for_High-Dimensional_Data.pdf:application/pdf},
}

@article{aleman-gomez_depthgram_2022,
	title = {Depthgram: {Visualizing} outliers in high-dimensional functional data with application to {fMRI} data exploration},
	volume = {41},
	issn = {10970258},
	doi = {10.1002/sim.9342},
	abstract = {Functional magnetic resonance imaging (fMRI) is a non-invasive technique that facilitates the study of brain activity by measuring changes in blood flow. Brain activity signals can be recorded during the alternate performance of given tasks, that is, task fMRI (tfMRI), or during resting-state, that is, resting-state fMRI (rsfMRI), as a measure of baseline brain activity. This contributes to the understanding of how the human brain is organized in functionally distinct subdivisions. fMRI experiments from high-resolution scans provide hundred of thousands of longitudinal signals for each individual, corresponding to brain activity measurements over each voxel of the brain along the duration of the experiment. In this context, we propose novel visualization techniques for high-dimensional functional data relying on depth-based notions that enable computationally efficient 2-dim representations of fMRI data, which elucidate sample composition, outlier presence, and individual variability. We believe that this previous step is crucial to any inferential approach willing to identify neuroscientific patterns across individuals, tasks, and brain regions. We present the proposed technique via an extensive simulation study, and demonstrate its application on a motor and language tfMRI experiment.},
	number = {11},
	journal = {Statistics in Medicine},
	author = {Alemán-Gómez, Yasser and Arribas-Gil, Ana and Desco, Manuel and Elías, Antonio and Romo, Juan},
	month = may,
	year = {2022},
	pmid = {35118686},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {data visualization, dimensionality reduction, FMRI, functional depth, multidimensional outliers},
	pages = {2005--2024},
	file = {PDF:/home/miguel/Zotero/storage/ZCST33DF/Statistics in Medicine - 2022 - Alem%C3%A1n%E2%80%90G%C3%B3mez - Depthgram  Visualizing outliers in high%E2%80%90dimensional functional data with.pdf:application/pdf},
}

@techreport{du_dream_nodate,
	title = {Dream the {Impossible}: {Outlier} {Imagination} with {Diffusion} {Models}},
	url = {https://github.com/deeplearning-wisc/dream-ood.},
	abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
	author = {Du, Xuefeng and Sun, Yiyou and Zhu, Xiaojin and Li, Yixuan},
	file = {PDF:/home/miguel/Zotero/storage/I7DEMVLI/NeurIPS-2023-dream-the-impossible-outlier-imagination-with-diffusion-models-Paper-Conference.pdf:application/pdf},
}

@article{dang_nonparametric_2010,
	title = {Nonparametric depth-based multivariate outlier identifiers, and masking robustness properties},
	volume = {140},
	issn = {03783758},
	doi = {10.1016/j.jspi.2009.07.004},
	abstract = {In extending univariate outlier detection methods to higher dimension, various issues arise: limited visualization methods, inadequacy of marginal methods, lack of a natural order, limited parametric modeling, and, when using Mahalanobis distance, restriction to ellipsoidal contours. To address and overcome such limitations, we introduce nonparametric multivariate outlier identifiers based on multivariate depth functions, which can generate contours following the shape of the data set. Also, we study masking robustness, that is, robustness against misidentification of outliers as nonoutliers. In particular, we define a masking breakdown point (MBP), adapting to our setting certain ideas of Davies and Gather [1993. The identification of multiple outliers (with discussion). Journal of the American Statistical Association 88, 782-801] and Becker and Gather [1999. The masking breakdown point of multivariate outlier identification rules. Journal of the American Statistical Association 94, 947-955] based on the Mahalanobis distance outlyingness. We then compare four affine invariant outlier detection procedures, based on Mahalanobis distance, halfspace or Tukey depth, projection depth, and "Mahalanobis spatial" depth. For the goal of threshold type outlier detection, it is found that the Mahalanobis distance and projection procedures are distinctly superior in performance, each with very high MBP, while the halfspace approach is quite inferior. When a moderate MBP suffices, the Mahalanobis spatial procedure is competitive in view of its contours not constrained to be elliptical and its computational burden relatively mild. A small sampling experiment yields findings completely in accordance with the theoretical comparisons. While these four depth procedures are relatively comparable for the purpose of robust affine equivariant location estimation, the halfspace depth is not competitive with the others for the quite different goal of robust setting of an outlyingness threshold. © 2009 Elsevier B.V. All rights reserved.},
	number = {1},
	journal = {Journal of Statistical Planning and Inference},
	author = {Dang, Xin and Serfling, Robert},
	month = jan,
	year = {2010},
	keywords = {Nonparametric, Depth functions, Multivariate analysis, Outlier identification, Robust},
	pages = {198--213},
	file = {PDF:/home/miguel/Zotero/storage/UYHZBFZV/2010 Xin Dang - Nonparametric depth based multivariate outlier ide [retrieved_2024-11-17].pdf:application/pdf},
}

@article{ruff_unifying_2021,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	volume = {109},
	issn = {15582256},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gregoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Muller, Klaus Robert},
	month = may,
	year = {2021},
	note = {arXiv: 2009.11732
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {neural networks, deep learning, outlier detection, Anomaly detection (AD), explainable artificial intelligence, interpretability, kernel methods, novelty detection, one-class classification, out-of-distribution (OOD) detection, unsupervised learning.},
	pages = {756--795},
	file = {PDF:/home/miguel/Zotero/storage/2768IIUP/A_Unifying_Review_of_Deep_and_Shallow_Anomaly_Detection-2.pdf:application/pdf},
}

@techreport{chandola_anomaly_2009,
	title = {Anomaly {Detection} : {A} {Survey}},
	abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
	author = {Chandola, Varun},
	year = {2009},
	note = {Publication Title: ACM Computing Surveys},
	keywords = {Algorithms Additional Key Words and Phrases, Anomaly Detection, Categories and Subject Descriptors, Database Applications-Data Mining General Terms, H28 [Database Management], Outlier Detection},
	file = {PDF:/home/miguel/Zotero/storage/R3YNAM52/AnomalyDetection.pdf:application/pdf},
}

@article{chen_efficient_2025,
	title = {An efficient and distribution-free symmetry test for high-dimensional data based on energy statistics and random projections},
	volume = {206},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016794732400207X},
	doi = {10.1016/j.csda.2024.108123},
	journal = {Computational Statistics \& Data Analysis},
	author = {Chen, Bo and Chen, Feifei and Wang, Junxin and Qiu, Tao},
	month = jun,
	year = {2025},
	pages = {108123},
	file = {PDF:/home/miguel/Zotero/storage/4BMV4T4H/symmetry test.pdf:application/pdf},
}

@article{porreca_identifying_2024,
	title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized {Hill}’s numbers and their integral functions},
	issn = {15737845},
	doi = {10.1007/s11135-024-01876-z},
	abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
	journal = {Quality and Quantity},
	author = {Porreca, Annamaria and Maturo, Fabrizio},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers, Normalized Hill’s functions, Standardized Hill’s functions},
	file = {PDF:/home/miguel/Zotero/storage/4PV7R9ZM/s11135-024-01876-z.pdf:application/pdf},
}

@article{roldan-alzate_assessing_2022,
	title = {Assessing the effects of multivariate functional outlier identification and sample robustification on identifying critical {PM2}.5 air pollution episodes in {Medellín}, {Colombia}},
	volume = {29},
	issn = {15733009},
	doi = {10.1007/s10651-022-00544-5},
	abstract = {Identification of critical episodes of environmental pollution, both as a outlier identification problem and as a classification problem, is a usual application of multivariate functional data analysis. This article addresses the effects of robustifying multivariate functional samples on the identification of critical pollution episodes in Medellín, Colombia. To do so, it compares 18 depth-based outlier identification methods and highlights the best options in terms of precision through simulation. It then applies the two methods with the best performance to robustify a real dataset of air pollution (PM2.5 concentration) in the Metropolitan Area of Medellín, Colombia and compares the effects of robustifying the samples on the accuracy of supervised classification through the multivariate functional DD-classifier. Our results show that 10 out of 20 methods revised perform better in at least one kind outliers. Nevertheless, no clear positive effects of robustification were identified with the real dataset.},
	number = {4},
	journal = {Environmental and Ecological Statistics},
	author = {Roldán-Alzate, Luis Miguel and Zuluaga, Francisco},
	month = dec,
	year = {2022},
	note = {Publisher: Springer},
	keywords = {Air pollution, Multivariate functional outlier detection, Sequential transformations, α-trimming},
	pages = {801--825},
	file = {PDF:/home/miguel/Zotero/storage/CJBYMHYV/s10651-022-00544-5.pdf:application/pdf},
}

@techreport{bickel_springer_nodate,
	title = {Springer {Series} in {Statistics}},
	author = {Bickel, P and Diggle, P and Fienberg, S and Gather, U and Olkin, I and Zeger, S},
	file = {PDF:/home/miguel/Zotero/storage/BS948HA3/ferrati_vieu_fda.pdf:application/pdf},
}

@book{ramsay_functional_2005,
	address = {New York},
	title = {Functional {Data} {Analysis}},
	publisher = {Springer},
	author = {Ramsay, J.O. and Silverman, B.W.},
	year = {2005},
	file = {PDF:/home/miguel/Zotero/storage/P2TVEHDT/ramsay_silverman_downloaded_fda.pdf:application/pdf},
}

@book{ferraty_nonparametric_2006,
	title = {Nonparametric {Functional} {Data} {Analysis}},
	isbn = {978-0-387-30369-7},
	publisher = {Springer New York},
	author = {Ferraty, Frederic and Vieu, Philippe},
	year = {2006},
	doi = {10.1007/0-387-36620-2},
}

@article{amovin-assagba_outlier_2022,
	title = {Outlier detection in multivariate functional data through a contaminated mixture model},
	volume = {174},
	issn = {01679473},
	doi = {10.1016/j.csda.2022.107496},
	abstract = {In an industrial context, the activity of sensors is recorded at a high frequency. A challenge is to automatically detect abnormal measurement behavior. Considering the sensor measures as functional data, the problem can be formulated as the detection of outliers in a multivariate functional data set. Due to the heterogeneity of this data set, the proposed contaminated mixture model both clusters the multivariate functional data into homogeneous groups and detects outliers. The main advantage of this procedure over its competitors is that it does not require to specify the proportion of outliers. Model inference is performed through an Expectation-Conditional Maximization algorithm, and the BIC is used to select the number of clusters. Numerical experiments on simulated data demonstrate the high performance achieved by the inference algorithm. In particular, the proposed model outperforms the competitors. Its application on the real data which motivated this study allows to correctly detect abnormal behaviors.},
	journal = {Computational Statistics and Data Analysis},
	author = {Amovin-Assagba, Martial and Gannaz, Irène and Jacques, Julien},
	month = oct,
	year = {2022},
	note = {arXiv: 2106.07222
Publisher: Elsevier B.V.},
	keywords = {Multivariate functional data, Outlier detection, Model-based clustering, EM algorithm, Contaminated Gaussian mixture},
	file = {PDF:/home/miguel/Zotero/storage/8I46LXYN/1-s2.0-S0167947322000767-main.pdf:application/pdf},
}

@inproceedings{zhang_sparx_2022,
	title = {Sparx: {Distributed} {Outlier} {Detection} at {Scale}},
	isbn = {978-1-4503-9385-0},
	doi = {10.1145/3534678.3539076},
	abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Sean and Ursekar, Varun and Akoglu, Leman},
	month = aug,
	year = {2022},
	note = {arXiv: 2206.01281},
	keywords = {Apache Spark, data-parallel algorithms, distributed outlier detection},
	pages = {4530--4540},
	file = {PDF:/home/miguel/Zotero/storage/FWNZKEHP/Sparx- Distributed Outlier Detection at Scale.pdf:application/pdf},
}

@article{zhang_two-sample_2025,
	title = {Two-sample inference for sparse functional data},
	volume = {19},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-19/issue-1/Two-sample-inference-for-sparse-functional-data/10.1214/25-EJS2348.full},
	doi = {10.1214/25-EJS2348},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Zhang, Chi and Sang, Peijun and Qin, Yingli},
	month = jan,
	year = {2025},
	file = {PDF:/home/miguel/Zotero/storage/LJGZ3QLS/25-EJS2348.pdf:application/pdf},
}

@article{yao_functional_2005,
	title = {Functional data analysis for sparse longitudinal data},
	volume = {100},
	issn = {01621459},
	doi = {10.1198/016214504000001745},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
	number = {470},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans Georg and Wang, Jane Ling},
	month = jun,
	year = {2005},
	keywords = {Smoothing, Asymptotics, Conditioning, Confidence band, Measurement error, Principal components, Simultaneous inference},
	pages = {577--590},
	file = {PDF:/home/miguel/Zotero/storage/IW6QZWQP/2005 Functional Data Analysis for Sparse Longitudinal D [retrieved_2025-03-17].pdf:application/pdf},
}

@article{jimenez-varon_pointwise_2024,
	title = {Pointwise data depth for univariate and multivariate functional outlier detection},
	volume = {35},
	issn = {1099095X},
	doi = {10.1002/env.2851},
	abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
	number = {5},
	journal = {Environmetrics},
	author = {Jiménez-Varón, Cristian F. and Harrou, Fouzi and Sun, Ying},
	month = aug,
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {functional data, data depth, magnitude outliers, pairwise depth, pointwise depth, shape outliers, visualization},
	file = {PDF:/home/miguel/Zotero/storage/K3CZZX9Y/Environmetrics - 2024 - Jim%C3%A9nez%E2%80%90Var%C3%B3n - Pointwise data depth for univariate and multivariate functional outlier detection.pdf:application/pdf},
}

@article{zhang_sparse_2016,
	title = {From sparse to dense functional data and beyond},
	volume = {44},
	issn = {00905364},
	doi = {10.1214/16-AOS1446},
	abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
	number = {5},
	journal = {Annals of Statistics},
	author = {Zhang, Xiaoke and Wang, Jane Ling},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Asymptotic normality, L2 convergence, Local linear smoothing, Uniform convergence, Weighing schemes},
	pages = {2281--2321},
	file = {PDF:/home/miguel/Zotero/storage/8CYRDY8F/16-AOS1446.pdf:application/pdf},
}

@techreport{bivand_spatial_2015,
	title = {Spatial {Data} {Analysis} with {R}-{INLA} with {Some} {Extensions}},
	url = {http://www.jstatsoft.org/},
	abstract = {The integrated nested Laplace approximation (INLA) provides an interesting way of approximating the posterior marginals of a wide range of Bayesian hierarchical models. This approximation is based on conducting a Laplace approximation of certain functions and numerical integration is extensively used to integrate some of the models parameters out. The R-INLA package offers an interface to INLA, providing a suitable framework for data analysis. Although the INLA methodology can deal with a large number of models, only the most relevant have been implemented within R-INLA. However, many other important models are not available for R-INLA yet. In this paper we show how to fit a number of spatial models with R-INLA, including its interaction with other R packages for data analysis. Secondly, we describe a novel method to extend the number of latent models available for the model parameters. Our approach is based on conditioning on one or several model parameters and fit these conditioned models with R-INLA. Then these models are combined using Bayesian model averaging to provide the final approximations to the posterior marginals of the model. Finally, we show some examples of the application of this technique in spatial statistics. It is worth noting that our approach can be extended to a number of other fields, and not only spatial statistics.},
	author = {Bivand, Roger S and Gómez-Rubio, Virgilio and Rue, Håvard},
	year = {2015},
	note = {Publication Title: JSS Journal of Statistical Software
Volume: 63},
	keywords = {R, INLA, spatial statistics},
	file = {PDF:/home/miguel/Zotero/storage/A84S4MYK/v63i20.pdf:application/pdf},
}

@article{pigoli_distances_2014,
	title = {Distances and inference for covariance operators},
	volume = {101},
	issn = {14643510},
	doi = {10.1093/biomet/asu008},
	abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
	number = {2},
	journal = {Biometrika},
	author = {Pigoli, Davide and Aston, John A.D. and Dryden, Ian L. and Secchi, Piercesare},
	year = {2014},
	note = {Publisher: Oxford University Press},
	keywords = {Functional data analysis, Shape analysis, Distance metric, Procrustes analysis},
	pages = {409--422},
	file = {PDF:/home/miguel/Zotero/storage/GGGJBJPF/asu008.pdf:application/pdf},
}

@article{noauthor_notitle_nodate,
}

@article{wang_severe_2024,
	title = {Severe {Global} {Environmental} {Issues} {Caused} by {Canada}’s {Record}-{Breaking} {Wildfires} in 2023},
	volume = {41},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	issn = {0256-1530, 1861-9533},
	url = {https://link.springer.com/10.1007/s00376-023-3241-0},
	doi = {10.1007/s00376-023-3241-0},
	abstract = {Due to the record-breaking wildfires that occurred in Canada in 2023, unprecedented quantities of air pollutants and greenhouse gases were released into the atmosphere. The wildfires had emitted more than 1.3 Pg CO2 and 0.14 Pg CO2 equivalent of other greenhouse gases (GHG) including CH4 and N2O as of 31 August. The wildfire-related GHG emissions constituted more than doubled Canada’s planned cumulative anthropogenic emissions reductions in 10 years, which represents a significant challenge to climate mitigation efforts. The model simulations showed that the Canadian wildfires impacted not only the local air quality but also that of most areas in the northern hemisphere due to long-range transport, causing severe PM2.5 pollution in the northeastern United States and increasing daily mean PM2.5 concentration in northwestern China by up to 2 μg m–3. The observed maximum daily mean PM2.5 concentration in New York City reached 148.3 μg m–3, which was their worst air quality in more than 50 years, nearly 10 times that of the air quality guideline (i.e., 15 μg m–3) issued by the World Health Organization (WHO). Aside from the direct emissions from forest fires, the peat fires beneath the surface might smolder for several months or even longer and release substantial amounts of CO2. The substantial amounts of greenhouse gases from forest and peat fires might contribute to the positive feedback to the climate, potentially accelerating global warming. To better understand the comprehensive environmental effects of wildfires and their interactions with the climate system, more detailed research based on advanced observations and Earth System Models is essential.},
	language = {en},
	number = {4},
	urldate = {2025-09-11},
	journal = {Advances in Atmospheric Sciences},
	author = {Wang, Zhe and Wang, Zifa and Zou, Zhiyin and Chen, Xueshun and Wu, Huangjian and Wang, Wending and Su, Hang and Li, Fang and Xu, Wenru and Liu, Zhihua and Zhu, Jiaojun},
	month = apr,
	year = {2024},
	pages = {565--571},
	file = {PDF:/home/miguel/Zotero/storage/IGNB7LEG/Wang et al. - 2024 - Severe Global Environmental Issues Caused by Canada’s Record-Breaking Wildfires in 2023.pdf:application/pdf},
}

@article{qiang_zhang_long-range_2025,
	title = {Long-range {PM2}.5 pollution and health impacts from the 2023 {Canadian} wildfires},
	url = {https://www.nature.com/articles/s41586-025-09482-1},
	language = {en},
	journal = {Nature},
	author = {{Qiang Zhang} and {Wang, Yuexuanzi} and {Qingyang Xiao} and {Guannang Geng} and {Steven Davis} and {Xiaodong Liu} and {Jin Yang} and {Jiajun Liu} and {Wenyu Huang} and {Changpei He} and {Binhe Luo} and {Randall Martin} and {Michael Brauer} and {James Renderson} and {Kebin He}},
	year = {2025},
	pages = {1--26},
	file = {PDF:/home/miguel/Zotero/storage/IV8HUVW3/Zhang - Long-range PM2.5 pollution and health impacts from the 2023 Canadian wildfires.pdf:application/pdf},
}

@article{wang_review_nodate,
	title = {Review of functional data analysis},
	abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. They are both examples of “functional data”, which have become a commonly encountered type of data. Functional Data Analysis (FDA) encompasses the statistical methodology for such data. Broadly interpreted, FDA deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of FDA, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is Functional Principal Component Analysis (FPCA). FPCA is an important dimension reduction tool and in sparse data situations can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classiﬁcation of functional data. Beyond linear and single or multiple index methods we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models, and models that feature time warping, manifold learning, and empirical diﬀerential equations. The paper concludes with a brief discussion of future directions.},
	language = {en},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Muller, Hans-Georg},
	file = {PDF:/home/miguel/Zotero/storage/W63775J2/Wang et al. - Review of functional data analysis.pdf:application/pdf},
}

@article{shah_short-term_2024,
	title = {Short-{Term} {Hourly} {Ozone} {Concentration} {Forecasting} {Using} {Functional} {Data} {Approach}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2225-1146},
	url = {https://www.mdpi.com/2225-1146/12/2/12},
	doi = {10.3390/econometrics12020012},
	abstract = {Air pollution, especially ground-level ozone, poses severe threats to human health and ecosystems. Accurate forecasting of ozone concentrations is essential for reducing its adverse effects. This study aims to use the functional time series approach to model ozone concentrations, a method less explored in the literature, and compare it with traditional time series and machine learning models. To this end, the ozone concentration hourly time series is first filtered for yearly seasonality using smoothing splines that lead us to the stochastic (residual) component. The stochastic component is modeled and forecast using a functional autoregressive model (FAR), where each daily ozone concentration profile is considered a single functional datum. For comparison purposes, different traditional and machine learning techniques, such as autoregressive integrated moving average (ARIMA), vector autoregressive (VAR), neural network autoregressive (NNAR), random forest (RF), and support vector machine (SVM), are also used to model and forecast the stochastic component. Once the forecast from the yearly seasonality component and stochastic component are obtained, both are added to obtain the final forecast. For empirical investigation, data consisting of hourly ozone measurements from Los Angeles from 2013 to 2017 are used, and one-day-ahead out-of-sample forecasts are obtained for a complete year. Based on the evaluation metrics, such as R2, root mean squared error (RMSE), and mean absolute error (MAE), the forecasting results indicate that the FAR outperforms the competitors in most scenarios, with the SVM model performing the least favorably across all cases.},
	language = {en},
	number = {2},
	urldate = {2025-09-11},
	journal = {Econometrics},
	author = {Shah, Ismail and Gul, Naveed and Ali, Sajid and Houmani, Hassan},
	month = may,
	year = {2024},
	pages = {12},
	file = {PDF:/home/miguel/Zotero/storage/AYLTMDYN/Shah et al. - 2024 - Short-Term Hourly Ozone Concentration Forecasting Using Functional Data Approach.pdf:application/pdf},
}

@book{Ramsay2005,
	title = {Functional {Data} {Analysis}},
	isbn = {1-4419-0319-8},
	publisher = {Springer},
	author = {Ramsay, J.O and Silverman, B.W.},
	year = {2005},
	file = {PDF:/home/miguel/Zotero/storage/W95D5XYD/fda.pdf:application/pdf},
}

@article{li_variational_2019,
	title = {Variational autoencoder-based outlier detection for high-dimensional data},
	volume = {23},
	issn = {15714128},
	doi = {10.3233/IDA-184240},
	abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
	number = {5},
	journal = {Intelligent Data Analysis},
	author = {Li, Yongmou and Wang, Yijie and Ma, Xingkong},
	year = {2019},
	note = {Publisher: IOS Press},
	keywords = {high-dimensional data, outlier detection, Variational autoencoders},
	pages = {991--1002},
	file = {PDF:/home/miguel/Zotero/storage/2HGHWGWX/retrieve.pdf:application/pdf},
}

@article{mirzaie_state_2023,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/TUQ3HJIA/1-s2.0-S1574013723000217-main-2.pdf:application/pdf},
}

@article{souiden_survey_2022,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/J385EZWI/1-s2.0-S1574013722000107-main-2.pdf:application/pdf},
}

@techreport{xu_comparison_2018,
	title = {A {Comparison} of {Outlier} {Detection} {Techniques} for {High}-{Dimensional} {Data}},
	abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
	author = {Xu, Xiaodan and Liu, Huawen and Li, Li and Yao, Minghai},
	year = {2018},
	keywords = {data mining, high-dimensional data, outlier detection, evaluation measurement},
	file = {PDF:/home/miguel/Zotero/storage/7KIN9GF2/25892518.pdf:application/pdf},
}

@techreport{sikder_outlier_nodate,
	title = {Outlier {Detection} using {AI}: {A} {Survey}},
	author = {Sikder, Nazmul Kabir and Batarseh, Feras A},
	file = {PDF:/home/miguel/Zotero/storage/I79PPZH2/2112.00588v1.pdf:application/pdf},
}

@article{li_ms2od_2024,
	title = {{MS2OD}: outlier detection using minimum spanning tree and medoid selection},
	volume = {5},
	issn = {26322153},
	doi = {10.1088/2632-2153/ad2492},
	abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
	number = {1},
	journal = {Machine Learning: Science and Technology},
	author = {Li, Jia and Li, Jiangwei and Wang, Chenxu and Verbeek, Fons J. and Schultz, Tanja and Liu, Hui},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Physics},
	keywords = {machine learning, data mining, outlier detection, clustering, medical data, medoid selection, minimum spanning tree},
	file = {PDF:/home/miguel/Zotero/storage/FCH3Z3S2/Li_2024_Mach._Learn.%3A_Sci._Technol._5_015025.pdf:application/pdf},
}

@article{zhang_outlier_2024,
	title = {Outlier {Detection} {Using} {Three}-{Way} {Neighborhood} {Characteristic} {Regions} and {Corresponding} {Fusion} {Measurement}},
	volume = {36},
	issn = {15582191},
	doi = {10.1109/TKDE.2023.3312108},
	abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Xianyong and Yuan, Zhong and Miao, Duoqian},
	month = may,
	year = {2024},
	note = {Publisher: IEEE Computer Society},
	keywords = {Data mining, outlier detection, neighborhood rough sets, three-way decision, uncertainty measurement},
	pages = {2082--2095},
	file = {PDF:/home/miguel/Zotero/storage/5QP9GNU3/Outlier.pdf:application/pdf},
}

@article{todorov_r_2024,
	title = {The {R} {Package} {Ecosystem} for {Robust} {Statistics}},
	volume = {16},
	issn = {19390068},
	doi = {10.1002/wics.70007},
	abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
	number = {6},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Todorov, Valentin},
	month = nov,
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {robust, R, high dimensions, multivariate, outlier},
	file = {PDF:/home/miguel/Zotero/storage/8HJD5P9R/WIREs Computational Stats - 2024 - Todorov - The R Package Ecosystem for Robust Statistics.pdf:application/pdf},
}

@article{li_outlier_2020,
	title = {Outlier {Detection} {Using} {Structural} {Scores} in a {High}-{Dimensional} {Space}},
	volume = {50},
	issn = {21682275},
	doi = {10.1109/TCYB.2018.2876615},
	abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
	number = {5},
	journal = {IEEE Transactions on Cybernetics},
	author = {Li, Xiaojie and Lv, Jiancheng and Yi, Zhang},
	month = may,
	year = {2020},
	pmid = {30418896},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {outlier detection, Discrimination, outlier factor, structural scores},
	pages = {2302--2310},
	file = {PDF:/home/miguel/Zotero/storage/96ZTCAP2/Outlier_Detection_Using_Structural_Scores_in_a_High-Dimensional_Space.pdf:application/pdf},
}

@article{zhu_high-dimensional_2023,
	title = {A {High}-{Dimensional} {Outlier} {Detection} {Approach} {Based} on {Local} {Coulomb} {Force}},
	volume = {35},
	issn = {15582191},
	doi = {10.1109/TKDE.2022.3172167},
	abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Pengyun and Zhang, Chaowei and Li, Xiaofeng and Zhang, Jifu and Qin, Xiao},
	month = jun,
	year = {2023},
	note = {Publisher: IEEE Computer Society},
	keywords = {High-dimensional outlier detection, local outlier coulomb force, neighborhood outlier factor, outlier coulomb resultant force, similarity metric},
	pages = {5506--5520},
	file = {PDF:/home/miguel/Zotero/storage/9AFX6MDL/2023 Pengyun Zhu - A High Dimensional Outlier Detection Approach Base [retrieved_2025-01-01].pdf:application/pdf},
}

@article{zimek_survey_2012,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/FD7BBG4M/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data.pdf:application/pdf},
}

@techreport{sikder_outlier_nodate-1,
	title = {Outlier {Detection} using {AI}: {A} {Survey}},
	author = {Sikder, Nazmul Kabir and Batarseh, Feras A},
	file = {PDF:/home/miguel/Zotero/storage/3KGFV4E6/2112.00588v1.pdf:application/pdf},
}

@techreport{xu_comparison_2018-1,
	title = {A {Comparison} of {Outlier} {Detection} {Techniques} for {High}-{Dimensional} {Data}},
	abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
	author = {Xu, Xiaodan and Liu, Huawen and Li, Li and Yao, Minghai},
	year = {2018},
	keywords = {data mining, high-dimensional data, outlier detection, evaluation measurement},
	file = {PDF:/home/miguel/Zotero/storage/Y7ZRW5JA/25892518.pdf:application/pdf},
}

@techreport{salehi_unified_nodate,
	title = {A {Unified} {Survey} on {Anomaly}, {Novelty}, {Open}-{Set}, and {Out}-of-{Distribution} {Detection}: {Solutions} and {Future} {Challenges}},
	url = {https://github.com/taslimisina/osr-ood-ad-methods},
	abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
	author = {Salehi, Mohammadreza and Nl, Salehidehnavi@uva and Mirzaei, Hossein and Hendrycks, Dan and Li, Yixuan and Rohban, Mohammad Hossein and Sabokrou, Mohammad},
	file = {PDF:/home/miguel/Zotero/storage/RIUMNASH/234_A_Unified_Survey_on_Anomal.pdf:application/pdf},
}

@techreport{salehi_unified_nodate-1,
	title = {A {Unified} {Survey} on {Anomaly}, {Novelty}, {Open}-{Set}, and {Out}-of-{Distribution} {Detection}: {Solutions} and {Future} {Challenges}},
	url = {https://github.com/taslimisina/osr-ood-ad-methods},
	abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
	author = {Salehi, Mohammadreza and Nl, Salehidehnavi@uva and Mirzaei, Hossein and Hendrycks, Dan and Li, Yixuan and Rohban, Mohammad Hossein and Sabokrou, Mohammad},
	file = {PDF:/home/miguel/Zotero/storage/MMKWLC25/234_A_Unified_Survey_on_Anomal.pdf:application/pdf},
}

@article{zheng_deep_2022,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/5GF96AX5/ZHENG HYPERSPHERE.pdf:application/pdf},
}

@article{fonvieille_swimming_2023,
	title = {Swimming in an ocean of curves: {A} functional approach to understanding elephant seal habitat use in the {Argentine} {Basin}},
	volume = {218},
	issn = {00796611},
	doi = {10.1016/j.pocean.2023.103120},
	abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
	journal = {Progress in Oceanography},
	author = {Fonvieille, Nadège and Guinet, Christophe and Saraceno, Martin and Picard, Baptiste and Tournier, Martin and Goulet, Pauline and Campagna, Claudio and Campagna, Julieta and Nerini, David},
	month = nov,
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Model-based clustering, Brazil-Malvinas confluence, Functional Data Analysis, Habitat use, Southern elephant seals},
	file = {PDF:/home/miguel/Zotero/storage/U9KVE9DR/1-s2.0-S0079661123001635-main.pdf:application/pdf},
}

@inproceedings{suhermi_functional_2024,
	title = {Functional {Data} {Analysis} for {Household} {Appliance} {Energy} {Consumption} {Prediction}},
	doi = {10.1109/IC3INA64086.2024.10732718},
	abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
	booktitle = {International {Conference} on {Computer}, {Control}, {Informatics} and its {Applications}, {IC3INA}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Suhermi, Novri and Aisy, Rahida Rihhadatul},
	year = {2024},
	note = {Issue: 2024
ISSN: 29945925},
	keywords = {Prediction, Functional Data Analysis, Energy Consumption, Functional Regression},
	pages = {468--471},
	file = {PDF:/home/miguel/Zotero/storage/YMYP32XX/Functional_Data_Analysis_for_Household_Appliance_Energy_Consumption_Prediction.pdf:application/pdf},
}

@article{dietzel_shrinkage-based_2024,
	title = {Shrinkage-based {Bayesian} variable selection for species distribution modelling in complex environments: {An} application to urban biodiversity},
	volume = {81},
	issn = {15749541},
	doi = {10.1016/j.ecoinf.2024.102561},
	abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
	journal = {Ecological Informatics},
	author = {Dietzel, Andreas and Moretti, Marco and Cook, Lauren M.},
	month = jul,
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bayesian projection predictive variable selection, Blue-green infrastructure, Nature-based solutions, Shrinkage prior, Species distribution model, Urban biodiversity},
	file = {PDF:/home/miguel/Zotero/storage/I6UXFQF8/1-s2.0-S1574954124001031-main.pdf:application/pdf},
}

@article{maturo_environmental_2024,
	title = {Environmental {Loss} {Assessment} via {Functional} {Outlier} {Detection} of {Transformed} {Biodiversity} {Profiles}},
	issn = {15372693},
	doi = {10.1007/s13253-024-00648-4},
	abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Maturo, Fabrizio and Porreca, Annamaria},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers},
	file = {PDF:/home/miguel/Zotero/storage/28ZCAFK9/s13253-024-00648-4.pdf:application/pdf},
}

@article{pronello_penalized_2023,
	title = {Penalized model-based clustering of complex functional data},
	volume = {33},
	issn = {15731375},
	doi = {10.1007/s11222-023-10288-2},
	abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Pronello, Nicola and Ignaccolo, Rosaria and Ippoliti, Luigi and Fontanella, Sara},
	month = dec,
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Functional zoning, Manifold data, Mixture models, Shape analysis, Spatial clustering, Surface data},
	file = {PDF:/home/miguel/Zotero/storage/9CIW8YIP/s11222-023-10288-2.pdf:application/pdf},
}

@article{mirzaie_state_2023-1,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/WR9FM9ZJ/1-s2.0-S1574013723000217-main-2.pdf:application/pdf},
}

@book{mateu_geostatistical_2022,
	title = {Geostatistical functional data analysis},
	isbn = {978-1-119-38784-8},
	abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Mateu, Jorge. and Giraldo, Ramon.},
	year = {2022},
	file = {PDF:/home/miguel/Zotero/storage/9KIXBDRP/Geostatistical Functional Data Analysis - 2021 - Mateu.pdf:application/pdf},
}

@article{zimek_survey_2012-1,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/ZTXNGEL7/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data-2.pdf:application/pdf},
}

@article{zimek_survey_2012-2,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/K5NKHUSG/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data.pdf:application/pdf},
}

@techreport{sikder_outlier_nodate-2,
	title = {Outlier {Detection} using {AI}: {A} {Survey}},
	author = {Sikder, Nazmul Kabir and Batarseh, Feras A},
	file = {PDF:/home/miguel/Zotero/storage/XMBPIXG2/2112.00588v1.pdf:application/pdf},
}

@inproceedings{suhermi_functional_2024-1,
	title = {Functional {Data} {Analysis} for {Household} {Appliance} {Energy} {Consumption} {Prediction}},
	doi = {10.1109/IC3INA64086.2024.10732718},
	abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
	booktitle = {International {Conference} on {Computer}, {Control}, {Informatics} and its {Applications}, {IC3INA}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Suhermi, Novri and Aisy, Rahida Rihhadatul},
	year = {2024},
	note = {Issue: 2024
ISSN: 29945925},
	keywords = {Prediction, Functional Data Analysis, Energy Consumption, Functional Regression},
	pages = {468--471},
	file = {PDF:/home/miguel/Zotero/storage/6LDX748L/Functional_Data_Analysis_for_Household_Appliance_Energy_Consumption_Prediction.pdf:application/pdf},
}

@techreport{xu_comparison_2018-2,
	title = {A {Comparison} of {Outlier} {Detection} {Techniques} for {High}-{Dimensional} {Data}},
	abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
	author = {Xu, Xiaodan and Liu, Huawen and Li, Li and Yao, Minghai},
	year = {2018},
	keywords = {data mining, high-dimensional data, outlier detection, evaluation measurement},
	file = {PDF:/home/miguel/Zotero/storage/LHK85MDY/25892518.pdf:application/pdf},
}

@article{li_ms2od_2024-1,
	title = {{MS2OD}: outlier detection using minimum spanning tree and medoid selection},
	volume = {5},
	issn = {26322153},
	doi = {10.1088/2632-2153/ad2492},
	abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
	number = {1},
	journal = {Machine Learning: Science and Technology},
	author = {Li, Jia and Li, Jiangwei and Wang, Chenxu and Verbeek, Fons J. and Schultz, Tanja and Liu, Hui},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Physics},
	keywords = {machine learning, data mining, outlier detection, clustering, medical data, medoid selection, minimum spanning tree},
	file = {PDF:/home/miguel/Zotero/storage/KV37CYT8/Li_2024_Mach._Learn.%3A_Sci._Technol._5_015025.pdf:application/pdf},
}

@article{souiden_survey_2022-1,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/TBQFHKT6/1-s2.0-S1574013722000107-main.pdf:application/pdf},
}

@article{souiden_survey_2022-2,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/WL5B5EDF/1-s2.0-S1574013722000107-main-2.pdf:application/pdf},
}

@article{mirzaie_state_2023-2,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/G7D4BUPG/1-s2.0-S1574013723000217-main-2.pdf:application/pdf},
}

@article{li_variational_2019-1,
	title = {Variational autoencoder-based outlier detection for high-dimensional data},
	volume = {23},
	issn = {15714128},
	doi = {10.3233/IDA-184240},
	abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
	number = {5},
	journal = {Intelligent Data Analysis},
	author = {Li, Yongmou and Wang, Yijie and Ma, Xingkong},
	year = {2019},
	note = {Publisher: IOS Press},
	keywords = {high-dimensional data, outlier detection, Variational autoencoders},
	pages = {991--1002},
	file = {PDF:/home/miguel/Zotero/storage/AWI9D7BP/retrieve.pdf:application/pdf},
}

@article{zhang_outlier_2024-1,
	title = {Outlier {Detection} {Using} {Three}-{Way} {Neighborhood} {Characteristic} {Regions} and {Corresponding} {Fusion} {Measurement}},
	volume = {36},
	issn = {15582191},
	doi = {10.1109/TKDE.2023.3312108},
	abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Xianyong and Yuan, Zhong and Miao, Duoqian},
	month = may,
	year = {2024},
	note = {Publisher: IEEE Computer Society},
	keywords = {Data mining, outlier detection, neighborhood rough sets, three-way decision, uncertainty measurement},
	pages = {2082--2095},
	file = {PDF:/home/miguel/Zotero/storage/6XLX368Z/Outlier.pdf:application/pdf},
}

@article{mirzaie_state_2023-3,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/94RCSTUF/1-s2.0-S1574013723000217-main.pdf:application/pdf},
}

@article{pronello_penalized_2023-1,
	title = {Penalized model-based clustering of complex functional data},
	volume = {33},
	issn = {15731375},
	doi = {10.1007/s11222-023-10288-2},
	abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Pronello, Nicola and Ignaccolo, Rosaria and Ippoliti, Luigi and Fontanella, Sara},
	month = dec,
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Functional zoning, Manifold data, Mixture models, Shape analysis, Spatial clustering, Surface data},
	file = {PDF:/home/miguel/Zotero/storage/VB9M9ML8/s11222-023-10288-2.pdf:application/pdf},
}

@article{fonvieille_swimming_2023-1,
	title = {Swimming in an ocean of curves: {A} functional approach to understanding elephant seal habitat use in the {Argentine} {Basin}},
	volume = {218},
	issn = {00796611},
	doi = {10.1016/j.pocean.2023.103120},
	abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
	journal = {Progress in Oceanography},
	author = {Fonvieille, Nadège and Guinet, Christophe and Saraceno, Martin and Picard, Baptiste and Tournier, Martin and Goulet, Pauline and Campagna, Claudio and Campagna, Julieta and Nerini, David},
	month = nov,
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Model-based clustering, Brazil-Malvinas confluence, Functional Data Analysis, Habitat use, Southern elephant seals},
	file = {PDF:/home/miguel/Zotero/storage/5B9G65XA/1-s2.0-S0079661123001635-main.pdf:application/pdf},
}

@article{zheng_deep_2022-1,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/DZA3ZE6Q/ZHENG HYPERSPHERE.pdf:application/pdf},
}

@article{zheng_deep_2022-2,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/UEV77GUK/1-s2.0-S1568494622004057-main-2.pdf:application/pdf},
}

@article{zheng_deep_2022-3,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/L7KDQG9D/1-s2.0-S1568494622004057-main.pdf:application/pdf},
}

@article{maturo_environmental_2024-1,
	title = {Environmental {Loss} {Assessment} via {Functional} {Outlier} {Detection} of {Transformed} {Biodiversity} {Profiles}},
	issn = {15372693},
	doi = {10.1007/s13253-024-00648-4},
	abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Maturo, Fabrizio and Porreca, Annamaria},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers},
	file = {PDF:/home/miguel/Zotero/storage/XHQMNFRU/s13253-024-00648-4.pdf:application/pdf},
}

@article{zhu_high-dimensional_2023-1,
	title = {A {High}-{Dimensional} {Outlier} {Detection} {Approach} {Based} on {Local} {Coulomb} {Force}},
	volume = {35},
	issn = {15582191},
	doi = {10.1109/TKDE.2022.3172167},
	abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Pengyun and Zhang, Chaowei and Li, Xiaofeng and Zhang, Jifu and Qin, Xiao},
	month = jun,
	year = {2023},
	note = {Publisher: IEEE Computer Society},
	keywords = {High-dimensional outlier detection, local outlier coulomb force, neighborhood outlier factor, outlier coulomb resultant force, similarity metric},
	pages = {5506--5520},
	file = {PDF:/home/miguel/Zotero/storage/YXDUNTSJ/2023 Pengyun Zhu - A High Dimensional Outlier Detection Approach Base [retrieved_2025-01-01].pdf:application/pdf},
}

@article{li_outlier_2020-1,
	title = {Outlier {Detection} {Using} {Structural} {Scores} in a {High}-{Dimensional} {Space}},
	volume = {50},
	issn = {21682275},
	doi = {10.1109/TCYB.2018.2876615},
	abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
	number = {5},
	journal = {IEEE Transactions on Cybernetics},
	author = {Li, Xiaojie and Lv, Jiancheng and Yi, Zhang},
	month = may,
	year = {2020},
	pmid = {30418896},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {outlier detection, Discrimination, outlier factor, structural scores},
	pages = {2302--2310},
	file = {PDF:/home/miguel/Zotero/storage/TRLNXXL3/Outlier_Detection_Using_Structural_Scores_in_a_High-Dimensional_Space.pdf:application/pdf},
}

@article{todorov_r_2024-1,
	title = {The {R} {Package} {Ecosystem} for {Robust} {Statistics}},
	volume = {16},
	issn = {19390068},
	doi = {10.1002/wics.70007},
	abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
	number = {6},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Todorov, Valentin},
	month = nov,
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {robust, R, high dimensions, multivariate, outlier},
	file = {PDF:/home/miguel/Zotero/storage/N9V5GQQI/WIREs Computational Stats - 2024 - Todorov - The R Package Ecosystem for Robust Statistics.pdf:application/pdf},
}

@article{dietzel_shrinkage-based_2024-1,
	title = {Shrinkage-based {Bayesian} variable selection for species distribution modelling in complex environments: {An} application to urban biodiversity},
	volume = {81},
	issn = {15749541},
	doi = {10.1016/j.ecoinf.2024.102561},
	abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
	journal = {Ecological Informatics},
	author = {Dietzel, Andreas and Moretti, Marco and Cook, Lauren M.},
	month = jul,
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bayesian projection predictive variable selection, Blue-green infrastructure, Nature-based solutions, Shrinkage prior, Species distribution model, Urban biodiversity},
	file = {PDF:/home/miguel/Zotero/storage/C3ZD5RC5/1-s2.0-S1574954124001031-main.pdf:application/pdf},
}

@techreport{salehi_unified_nodate-2,
	title = {A {Unified} {Survey} on {Anomaly}, {Novelty}, {Open}-{Set}, and {Out}-of-{Distribution} {Detection}: {Solutions} and {Future} {Challenges}},
	url = {https://github.com/taslimisina/osr-ood-ad-methods},
	abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
	author = {Salehi, Mohammadreza and Nl, Salehidehnavi@uva and Mirzaei, Hossein and Hendrycks, Dan and Li, Yixuan and Rohban, Mohammad Hossein and Sabokrou, Mohammad},
	file = {PDF:/home/miguel/Zotero/storage/HXZH4MFR/234_A_Unified_Survey_on_Anomal.pdf:application/pdf},
}

@book{mateu_geostatistical_2022-1,
	title = {Geostatistical functional data analysis},
	isbn = {978-1-119-38784-8},
	abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Mateu, Jorge. and Giraldo, Ramon.},
	year = {2022},
	file = {PDF:/home/miguel/Zotero/storage/2VKUL2VQ/Geostatistical Functional Data Analysis - 2021 - Mateu.pdf:application/pdf},
}

@article{herrmann_geometric_2023,
	title = {A geometric framework for outlier detection in high-dimensional data},
	volume = {13},
	issn = {19424795},
	doi = {10.1002/widm.1491},
	abstract = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption, that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under: Technologies {\textgreater} Structure Discovery and Clustering Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Visualization.},
	number = {3},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Herrmann, Moritz and Pfisterer, Florian and Scheipl, Fabian},
	month = may,
	year = {2023},
	note = {arXiv: 2207.00367
Publisher: John Wiley and Sons Inc},
	keywords = {outlier detection, anomaly detection, dimension reduction, manifold learning},
	file = {PDF:/home/miguel/Zotero/storage/GRGMVL7I/WIREs Data Min   Knowl - 2023 - Herrmann - A geometric framework for outlier detection in high%E2%80%90dimensional data.pdf:application/pdf},
}

@article{herrmann_geometric_2023-1,
	title = {A geometric framework for outlier detection in high-dimensional data},
	volume = {13},
	issn = {19424795},
	doi = {10.1002/widm.1491},
	abstract = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption, that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under: Technologies {\textgreater} Structure Discovery and Clustering Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Visualization.},
	number = {3},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Herrmann, Moritz and Pfisterer, Florian and Scheipl, Fabian},
	month = may,
	year = {2023},
	note = {arXiv: 2207.00367
Publisher: John Wiley and Sons Inc},
	keywords = {outlier detection, anomaly detection, dimension reduction, manifold learning},
	file = {PDF:/home/miguel/Zotero/storage/HX6PMCS9/WIREs Data Min   Knowl - 2023 - Herrmann - A geometric framework for outlier detection in high%E2%80%90dimensional data.pdf:application/pdf},
}

@article{zhang_sparse_2016-1,
	title = {From sparse to dense functional data and beyond},
	volume = {44},
	issn = {00905364},
	doi = {10.1214/16-AOS1446},
	abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
	number = {5},
	journal = {Annals of Statistics},
	author = {Zhang, Xiaoke and Wang, Jane Ling},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Asymptotic normality, L2 convergence, Local linear smoothing, Uniform convergence, Weighing schemes},
	pages = {2281--2321},
	file = {PDF:/home/miguel/Zotero/storage/AIE228HZ/16-AOS1446.pdf:application/pdf},
}

@article{pigoli_distances_2014-1,
	title = {Distances and inference for covariance operators},
	volume = {101},
	issn = {14643510},
	doi = {10.1093/biomet/asu008},
	abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
	number = {2},
	journal = {Biometrika},
	author = {Pigoli, Davide and Aston, John A.D. and Dryden, Ian L. and Secchi, Piercesare},
	year = {2014},
	note = {Publisher: Oxford University Press},
	keywords = {Functional data analysis, Shape analysis, Distance metric, Procrustes analysis},
	pages = {409--422},
	file = {PDF:/home/miguel/Zotero/storage/3DKDVRCA/asu008.pdf:application/pdf},
}

@book{lawson_using_2021,
	address = {Oxon},
	title = {Using {R} for {Bayesian} {Spatial} and {Spatio}-{Temporal} {Health} {Modeling}},
	publisher = {CRC Press},
	author = {Lawson, Andrew B},
	year = {2021},
	keywords = {Bayesian Hierarchical Models, Count Models, Disease Mapping Models, Infectious Disease Modeling, Nimble, Spatial Health Data, Survival Modeling},
	file = {PDF:/home/miguel/Zotero/storage/M6PLBWSB/Using R for Bayesian Spatial and Spatio-Temporal Health Modeling_25_03_30_22_35_30.pdf:application/pdf},
}

@book{blangiardo_spatial_2015,
	title = {Spatial and spatio-temporal {Bayesian} models with {R}-{INLA}},
	isbn = {978-1-118-32655-8},
	abstract = {Spatial and Spatio-Temporal Bayesian Models with R-INLA provides a muchneeded, practically oriented \& innovative presentation of the combination ofBayesian methodology and spatial statistics. The authors combine an introduction toBayesian theory and methodology with a focus on the spatial and spatio­-temporal modelsused within the Bayesian framework and a series of practical examples which allowthe reader to link the statistical theory presented to real data problems. The numerousexamples from the fields of epidemiology, biostatistics and social science all arecoded in the R package R-INLA, which has proven to be a valid alternative to the commonlyused Markov Chain Monte Carlo simulations. Title Page; Copyright; Table of Contents; Dedication; Preface; Chapter 1: Introduction; 1.1 Why spatial and spatio-temporal statistics?; 1.2 Why do we use Bayesian methods for modeling spatial and spatio-temporal structures?; 1.3 Why INLA?; 1.4 Datasets; References; Chapter 2: Introduction to R; 2.1 The R language; 2.2 R objects; 2.3 Data and session management; 2.4 Packages; 2.5 Programming in R; 2.6 Basic statistical analysis with R; References; Chapter 3: Introduction to Bayesian methods; 3.1 Bayesian philosophy; 3.2 Basic probability elements; 3.3 Bayes theorem.},
	publisher = {John Wiley and Sons, Inc.},
	author = {Blangiardo, Marta. and Cameletti, Michela.},
	year = {2015},
	file = {PDF:/home/miguel/Zotero/storage/DLUMRR75/Spatial and Spatio%E2%80%90temporal Bayesian Models with R%E2%80%90INLA - 2015 - Blangiardo.pdf:application/pdf},
}

@article{post_monitoring_2018,
	title = {Monitoring spatial and temporal variation of dissolved oxygen and water temperature in the {Savannah} {River} using a sensor network},
	volume = {190},
	issn = {15732959},
	doi = {10.1007/s10661-018-6646-y},
	abstract = {Dissolved oxygen is a critical component of river water quality. This study investigated average weekly dissolved oxygen (AWDO) and average weekly water temperature (AWT) in the Savannah River during 2015 and 2016 using data from the Intelligent River® sensor network. Weekly data and seasonal summary statistics revealed distinct seasonal patterns that impact both AWDO and AWT regardless of location along the river. Within seasons, spatial patterns of AWDO and AWT along the river are also evident. Linear mixed effects models indicate that AWT and low and high river flow conditions had a significant impact on AWDO, but added little predictive information to the models. Low and high river flow conditions had a significant impact on AWT, but also added little predictive information to the models. Spatial linear mixed effects models yielded parameter estimates that were effectively the same as non-spatial linear mixed effects models. However, components of variance from spatial linear mixed effects models indicate that 23–32\% of the total variance in AWDO and that 12–18\% of total variance in AWT can be apportioned to the effect of spatial covariance. These results indicate that location, week, and flow-directional spatial relationships are critically important considerations for investigating relationships between space- and time-varying water quality metrics.},
	number = {5},
	journal = {Environmental Monitoring and Assessment},
	author = {Post, Christopher J. and Cope, Michael P. and Gerard, Patrick D. and Masto, Nicholas M. and Vine, Joshua R. and Stiglitz, Roxanne Y. and Hallstrom, Jason O. and Newman, Jillian C. and Mikhailova, Elena A.},
	month = may,
	year = {2018},
	pmid = {29637320},
	note = {Publisher: Springer International Publishing},
	keywords = {Geographic information systems (GIS), Intelligent River®, Spatial stream networks, Water quality monitoring},
	file = {PDF:/home/miguel/Zotero/storage/BQJQBUBB/s10661-018-6646-y.pdf:application/pdf},
}

@article{chen_comparison_2024,
	title = {A comparison of computational algorithms for the {Bayesian} analysis of clinical trials},
	issn = {17407753},
	doi = {10.1177/17407745241247334},
	abstract = {Background: Clinical trials are increasingly using Bayesian methods for their design and analysis. Inference in Bayesian trials typically uses simulation-based approaches such as Markov Chain Monte Carlo methods. Markov Chain Monte Carlo has high computational cost and can be complex to implement. The Integrated Nested Laplace Approximations algorithm provides approximate Bayesian inference without the need for computationally complex simulations, making it more efficient than Markov Chain Monte Carlo. The practical properties of Integrated Nested Laplace Approximations compared to Markov Chain Monte Carlo have not been considered for clinical trials. Using data from a published clinical trial, we aim to investigate whether Integrated Nested Laplace Approximations is a feasible and accurate alternative to Markov Chain Monte Carlo and provide practical guidance for trialists interested in Bayesian trial design. Methods: Data from an international Bayesian multi-platform adaptive trial that compared therapeutic-dose anticoagulation with heparin to usual care in non-critically ill patients hospitalized for COVID-19 were used to fit Bayesian hierarchical generalized mixed models. Integrated Nested Laplace Approximations was compared to two Markov Chain Monte Carlo algorithms, implemented in the software JAGS and stan, using packages available in the statistical software R. Seven outcomes were analysed: organ-support free days (an ordinal outcome), five binary outcomes related to survival and length of hospital stay, and a time-to-event outcome. The posterior distributions for the treatment and sex effects and the variances for the hierarchical effects of age, site and time period were obtained. We summarized these posteriors by calculating the mean, standard deviations and the 95\% equitailed credible intervals and presenting the results graphically. The computation time for each algorithm was recorded. Results: The average overlap of the 95\% credible interval for the treatment and sex effects estimated using Integrated Nested Laplace Approximations was 96\% and 97.6\% compared with stan, respectively. The graphical posterior densities for these effects overlapped for all three algorithms. The posterior mean for the variance of the hierarchical effects of age, site and time estimated using Integrated Nested Laplace Approximations are within the 95\% credible interval estimated using Markov Chain Monte Carlo but the average overlap of the credible interval is lower, 77\%, 85.6\% and 91.3\%, respectively, for Integrated Nested Laplace Approximations compared to stan. Integrated Nested Laplace Approximations and stan were easily implemented in clear, well-established packages in R, while JAGS required the direct specification of the model. Integrated Nested Laplace Approximations was between 85 and 269 times faster than stan and 26 and 1852 times faster than JAGS. Conclusion: Integrated Nested Laplace Approximations could reduce the computational complexity of Bayesian analysis in clinical trials as it is easy to implement in R, substantially faster than Markov Chain Monte Carlo methods implemented in JAGS and stan, and provides near identical approximations to the posterior distributions for the treatment effect. Integrated Nested Laplace Approximations was less accurate when estimating the posterior distribution for the variance of hierarchical effects, particularly for the proportional odds model, and future work should determine if the Integrated Nested Laplace Approximations algorithm can be adjusted to improve this estimation.},
	journal = {Clinical Trials},
	author = {Chen, Ziming and Berger, Jeffrey S. and Castellucci, Lana A. and Farkouh, Michael and Goligher, Ewan C. and Hade, Erinn M. and Hunt, Beverley J. and Kornblith, Lucy Z. and Laweler, Patrick R. and Leifer, Eric S. and Lorenzi, Elizabeth and Neal, Matthew D. and Zarychanski, Ryan and Heath, Anna},
	month = dec,
	year = {2024},
	pmid = {38752434},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Bayesian clinical trial analysis, Integrated Nested Laplace Approximations, JAGS, logistic regression, Markov chain Monte Carlo, proportional odds model, stan, survival analysis},
	file = {PDF:/home/miguel/Zotero/storage/AX57V9ZH/Stan v Inla in clinical trials.pdf:application/pdf},
}

@inproceedings{wallace_threat_2024,
	title = {The threat of coastal hypoxia in eastern {Canadian} waters - {New} opportunities for its mitigation and the potential of {Bedford} {Basin} ({Nova} {Scotia}) for required pilot study and demonstration},
	isbn = {979-8-3315-4008-1},
	doi = {10.1109/OCEANS55160.2024.10754451},
	abstract = {Ocean deoxygenation and expansion and intensification of hypoxia is a growing threat to marine biodiversity worldwide, especially in coastal waters. The causes include eutrophication due to excess input of nutrients. However, increasingly, warming and especially circulation- and mixing-related changes to oxygen supply connected with climate change are driving oxygen concentrations downwards. This is now a major problem on the Canadian east coast with growing hypoxic zones in the Lower St. Lawrence Estuary, as well as threats to shelf waters of Nova Scotia and coastal basins, including Bedford Basin. Current measures to protect marine biodiversity (e.g. marine protected areas) are ineffectual in addressing this threat. However, the sudden emergence of a green hydrogen industry offers a possible solution. Here we highlight the threat to eastern Canadian ecosystems, including commercial fish stocks, from hypoxia as well as the underlying causes and the potential for a solution. In the case of the Gulf of St. Lawrence, planned construction of a hydrogen plant near Stephenville, NL, holds potential to mitigate the growing threat through use of the oxygen by-product of hydrogen generation. Based on measurements and tracer studies, the location and magnitude of production appear to be 'just right' to compensate for current oxygen losses. This mitigation approach (direct oxygen injection) has rarely been considered for marine environments to-date and involves magnitudes and timescales of oxygen transport and response that have not been attempted before. Nevertheless, it seems essential and urgent for ocean scientists, engineers and policymakers to work together to address this threat and explore its potential solution. A coordinated research effort into direct oxygen injection, involving broad multidisciplinary expertise should be established immediately. Field scale pilot studies in well-characterized, accessible locations appear to be the next key step required, as well as planning for larger scale deployment. There are several reasons why Bedford Basin, Nova Scotia is an ideal location for pilot studies and demonstrations. The questions that need to be addressed, the suitability of Bedford Basin, pilot study design and practicality issues are presented and discussed.},
	booktitle = {Oceans {Conference} {Record} ({IEEE})},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wallace, Douglas and Austin, David},
	year = {2024},
	note = {ISSN: 01977385},
	keywords = {Bedford Basin, deoxygenation, direct oxygen injection, ecosystem restoration, green hydrogen, Gulf of St. Lawrence, hypoxia, marine ecosystem, reoxygenation},
	file = {PDF:/home/miguel/Zotero/storage/GLW3H22R/The_threat_of_coastal_hypoxia_in_eastern_Canadian_waters_-_new_opportunities_for_its_mitigation_and_the_potential_of_Bedford_Basin_Nova_Scotia_for_required_pilot_study_and_demonstration.pdf:application/pdf},
}

@techreport{jutras_physical_2023,
	title = {Physical and biogeochemical drivers of deoxygenation in the {Gulf} and {Lower} {St}. {Lawrence} {Estuary}},
	author = {Jutras, Mathilde},
	year = {2023},
	file = {PDF:/home/miguel/Zotero/storage/C8FVXNUD/out.pdf:application/pdf},
}

@article{xia_numerical_2011,
	title = {Numerical simulation of salinity and dissolved oxygen at {Perdido} {Bay} and adjacent coastal ocean},
	volume = {27},
	issn = {07490208},
	doi = {10.2112/JCOASTRES-D-09-00044.1},
	abstract = {Environmental fluid dynamic code (EFDC), a numerical estuarine and coastal ocean circulation hydrodynamic model, was used to simulate the distribution of the salinity, temperature, nutrients, and dissolved oxygen (DO) in Perdido Bay and adjacent Gulf of Mexico. External forcing factors included the coupled effects of the astronomical tides, river discharge, and atmospheric winds on the spatial and temporal distributions of salinity and DO. Modeled time series were in good agreement with field observations of water level, nutrients, temperature, salinity, and DO. Perdido Bay and adjacent northern Gulf of Mexico coasts can be divided into two areas according to salinity, water level, and DO concentrations. The first area was lower Perdido Bay and the associated Gulf of Mexico coasts, acting primarily under the influence of tidal forcing, which increases the vertical stratification. The second division was upper Perdido Bay, which was influenced by both tidal forcing and freshwater inflow. Simulations also indicated winds influenced the salinity and DO distributions, with an enhanced surface pressure gradient. Tidal effects were also important for conducting salinity and water quality simulations in Perdido Bay. Low amplitude tides induced relatively weak vertical mixing and favored the establishment of stratification at the bay, especially along deeper bathymetry. Flood tides influenced the distribution of salinity and DO more than ebb tides, specifically along shallow bathymetry. © Coastal Education \& Research Foundation 2011.},
	number = {1},
	journal = {Journal of Coastal Research},
	author = {Xia, Meng and Craig, Paul M. and Wallen, Christopher M. and Stoddard, Andrew and Mandrup-Poulsen, Jan and Peng, Machuan and Schaeffer, Blake and Liu, Zhijun},
	month = dec,
	year = {2011},
	keywords = {dissolved oxygen, EFDC, Perdido Bay, plume, salinity},
	pages = {73--86},
	file = {PDF:/home/miguel/Zotero/storage/5BI66IAK/Numerical_Simulation_of_Salini.pdf:application/pdf},
}

@techreport{noauthor_modeling_nodate,
	title = {Modeling {Spatio}-{Temporal} {Data}},
	abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
	file = {PDF:/home/miguel/Zotero/storage/HMYVCWW8/9781032623443_previewpdf.pdf:application/pdf},
}

@article{styles_spatial_2024,
	title = {Spatial and {Temporal} {Patterns} of {Southern} {Ocean} {Ventilation}},
	volume = {51},
	issn = {19448007},
	doi = {10.1029/2023GL106716},
	abstract = {Ocean ventilation translates atmospheric forcing into the ocean interior. The Southern Ocean is an important ventilation site for heat and carbon and is likely to influence the outcome of anthropogenic climate change. We conduct an extensive backwards-in-time trajectory experiment to identify spatial and temporal patterns of ventilation. Temporally, almost all ventilation occurs between August and November. Spatially, “hotspots” of ventilation account for 60\% of open-ocean ventilation on a 30 years timescale; the remaining 40\% ventilates in a circumpolar pattern. The densest waters ventilate on the Antarctic shelf, primarily near the Antarctic Peninsula (40\%) and the west Ross sea (20\%); the remaining 40\% is distributed across East Antarctica. Shelf-ventilated waters experience significant densification outside of the mixed layer.},
	number = {4},
	journal = {Geophysical Research Letters},
	author = {Styles, Andrew F. and MacGilchrist, Graeme A. and Bell, Michael J. and Marshall, David P.},
	month = feb,
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {hotspots, southern ocean, Stommel's demon, trajectories, ventilation},
	file = {PDF:/home/miguel/Zotero/storage/MZQZTAZC/Geophysical Research Letters - 2024 - Styles - Spatial and Temporal Patterns of Southern Ocean Ventilation-2.pdf:application/pdf},
}

@article{noauthor_fundy_20190604_99_delayed_corrected_v4_nodate,
	title = {fundy\_20190604\_99\_delayed\_corrected\_v4},
	file = {fundy_20190604_99_delayed_corrected_v4.csv:/home/miguel/Zotero/storage/AFIWBJME/fundy_20190604_99_delayed_corrected_v4.csv:text/csv},
}

@article{contractor_efficacy_2021,
	title = {Efficacy of {Feedforward} and {LSTM} {Neural} {Networks} at {Predicting} and {Gap} {Filling} {Coastal} {Ocean} {Timeseries}: {Oxygen}, {Nutrients}, and {Temperature}},
	volume = {8},
	issn = {22967745},
	doi = {10.3389/fmars.2021.637759},
	abstract = {Ocean data timeseries are vital for a diverse range of stakeholders (ranging from government, to industry, to academia) to underpin research, support decision making, and identify environmental change. However, continuous monitoring and observation of ocean variables is difficult and expensive. Moreover, since oceans are vast, observations are typically sparse in spatial and temporal resolution. In addition, the hostile ocean environment creates challenges for collecting and maintaining data sets, such as instrument malfunctions and servicing, often resulting in temporal gaps of varying lengths. Neural networks (NN) have proven effective in many diverse big data applications, but few oceanographic applications have been tested using modern frameworks and architectures. Therefore, here we demonstrate a “proof of concept” neural network application using a popular “off-the-shelf” framework called “TensorFlow” to predict subsurface ocean variables including dissolved oxygen and nutrient (nitrate, phosphate, and silicate) concentrations, and temperature timeseries and show how these models can be used successfully for gap filling data products. We achieved a final prediction accuracy of over 96\% for oxygen and temperature, and mean squared errors (MSE) of 2.63, 0.0099, and 0.78, for nitrates, phosphates, and silicates, respectively. The temperature gap-filling was done with an innovative contextual Long Short-Term Memory (LSTM) NN that uses data before and after the gap as separate feature variables. We also demonstrate the application of a novel dropout based approach to approximate the Bayesian uncertainty of these temperature predictions. This Bayesian uncertainty is represented in the form of 100 monte carlo dropout estimates of the two longest gaps in the temperature timeseries from a model with 25\% dropout in the input and recurrent LSTM connections. Throughout the study, we present the NN training process including the tuning of the large number of NN hyperparameters which could pose as a barrier to uptake among researchers and other oceanographic data users. Our models can be scaled up and applied operationally to provide consistent, gap-free data to all data users, thus encouraging data uptake for data-based decision making.},
	journal = {Frontiers in Marine Science},
	author = {Contractor, Steefan and Roughan, Moninya},
	month = may,
	year = {2021},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {machine learning, coastal oceanography, depth profile observations, East Australian Current, nitrate, phosphate, silicate, statistical modeling},
	file = {PDF:/home/miguel/Zotero/storage/X8SJ3WUG/fmars-08-637759.pdf:application/pdf},
}

@article{noauthor_fundy_20190604_99_delayed_corrected_v4_nodate-1,
	title = {fundy\_20190604\_99\_delayed\_corrected\_v4},
	file = {fundy_20190604_99_delayed_corrected_v4.csv:/home/miguel/Zotero/storage/YUGG9TVX/fundy_20190604_99_delayed_corrected_v4.csv:text/csv},
}

@article{noauthor_fundy_20190604_99_delayed_corrected_v4_nodate-2,
	title = {fundy\_20190604\_99\_delayed\_corrected\_v4},
	file = {fundy_20190604_99_delayed_corrected_v4.csv:/home/miguel/Zotero/storage/DUHUKUMM/fundy_20190604_99_delayed_corrected_v4.csv:text/csv},
}

@article{bourbonnais_marine_2023,
	title = {Marine {N2O} cycling from high spatial resolution concentration, stable isotopic and isotopomer measurements along a meridional transect in the eastern {Pacific} {Ocean}},
	volume = {10},
	issn = {22967745},
	doi = {10.3389/fmars.2023.1137064},
	abstract = {Nitrous oxide (N2O) is a potent greenhouse gas and ozone depleting substance, with the ocean accounting for about one third of global emissions. In marine environments, a significant amount of N2O is produced by biological processes in Oxygen Deficient Zones (ODZs). While recent technological advances are making surface N2O concentration more available, high temporal and spatial resolution water-column N2O concentration data are relatively scarce, limiting global N2O ocean models’ predictive capability. We present a N2O concentration, stable isotopic composition and isotopomer dataset of unprecedently large spatial coverage and depth resolution in the broader Pacific, crossing both the eastern tropical South and North Pacific Ocean ODZs collected as part of the GO-SHIP P18 repeat hydrography program in 2016/2017. We complement these data with dissolved gases (nitrogen, oxygen, argon) and nitrate isotope data to investigate the pathways controlling N2O production in relation to apparent oxygen utilization and fixed nitrogen loss. N2O yield significantly increased under low oxygen conditions near the ODZs. Keeling plot analysis revealed different N2O sources above the ODZs under different oxygen regimes. Our stable isotopic data and relationships between the N2O added by microbial processes (ΔN2O) and dissolved inorganic nitrogen (DIN) deficit confirm increased N2O production by denitrification under low oxygen conditions near the oxycline where the largest N2O accumulations were observed. The slope for δ18O-N2O versus site preference (SP, the difference between the central (α) and outer (β) N atoms in the linear N2O molecule) in the eastern tropical North Pacific ODZ was lower than expected for pure N2O reduction, likely because of the observed decrease in δ15Nβ. This trend is consistent with prior ODZ studies and attributed to concurrent production of N2O from nitrite with a low δ15N or denitrification with a SP {\textgreater}0‰. We estimated apparent isotope effects for N2O consumption in the ETNP ODZ of 3.6‰ for 15Nbulk, 9.4‰ for 15Nα, -2.3‰ for 15Nβ, 12.0‰ for 18O, and 11.7‰ for SP. These values were generally within ranges previously reported for previous laboratory and field experiments.},
	journal = {Frontiers in Marine Science},
	author = {Bourbonnais, Annie and Chang, Bonnie X. and Sonnerup, Rolf E. and Doney, Scott C. and Altabet, Mark A.},
	year = {2023},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {eastern North Pacific Ocean, eastern South Pacific Ocean, greenhouse gas, isotopomers, nitrous oxide, oxygen deficient zones, Southern Ocean, stable isotopes},
	file = {PDF:/home/miguel/Zotero/storage/FHJQJGL3/fmars-10-1137064.pdf:application/pdf},
}

@article{noauthor_scotia_20180720_87_delayed_corrected_v4_nodate,
	title = {scotia\_20180720\_87\_delayed\_corrected\_v4},
	file = {scotia_20180720_87_delayed_corrected_v4.csv:/home/miguel/Zotero/storage/L8WMBVXQ/scotia_20180720_87_delayed_corrected_v4.csv:text/csv},
}

@article{zhang_sparse_2016-2,
	title = {From sparse to dense functional data and beyond},
	volume = {44},
	issn = {00905364},
	doi = {10.1214/16-AOS1446},
	abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
	number = {5},
	journal = {Annals of Statistics},
	author = {Zhang, Xiaoke and Wang, Jane Ling},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Asymptotic normality, L2 convergence, Local linear smoothing, Uniform convergence, Weighing schemes},
	pages = {2281--2321},
	file = {PDF:/home/miguel/Zotero/storage/2KFIDKQC/16-AOS1446.pdf:application/pdf},
}

@article{yao_functional_2005-1,
	title = {Functional data analysis for sparse longitudinal data},
	volume = {100},
	issn = {01621459},
	doi = {10.1198/016214504000001745},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
	number = {470},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans Georg and Wang, Jane Ling},
	month = jun,
	year = {2005},
	keywords = {Smoothing, Asymptotics, Conditioning, Confidence band, Measurement error, Principal components, Simultaneous inference},
	pages = {577--590},
	file = {PDF:/home/miguel/Zotero/storage/6TCP5V5B/2005 Functional Data Analysis for Sparse Longitudinal D [retrieved_2025-03-17].pdf:application/pdf},
}

@article{zimek_survey_2012-3,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/FIQ57TJD/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data-2.pdf:application/pdf},
}

@article{zimek_survey_2012-4,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/BFTV8ZAN/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data.pdf:application/pdf},
}

@techreport{sikder_outlier_nodate-3,
	title = {Outlier {Detection} using {AI}: {A} {Survey}},
	author = {Sikder, Nazmul Kabir and Batarseh, Feras A},
	file = {PDF:/home/miguel/Zotero/storage/LZ2RDXXN/2112.00588v1.pdf:application/pdf},
}

@techreport{xu_comparison_2018-3,
	title = {A {Comparison} of {Outlier} {Detection} {Techniques} for {High}-{Dimensional} {Data}},
	abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
	author = {Xu, Xiaodan and Liu, Huawen and Li, Li and Yao, Minghai},
	year = {2018},
	keywords = {data mining, high-dimensional data, outlier detection, evaluation measurement},
	file = {PDF:/home/miguel/Zotero/storage/7RC7CV4T/25892518.pdf:application/pdf},
}

@inproceedings{suhermi_functional_2024-2,
	title = {Functional {Data} {Analysis} for {Household} {Appliance} {Energy} {Consumption} {Prediction}},
	doi = {10.1109/IC3INA64086.2024.10732718},
	abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
	booktitle = {International {Conference} on {Computer}, {Control}, {Informatics} and its {Applications}, {IC3INA}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Suhermi, Novri and Aisy, Rahida Rihhadatul},
	year = {2024},
	note = {Issue: 2024
ISSN: 29945925},
	keywords = {Prediction, Functional Data Analysis, Energy Consumption, Functional Regression},
	pages = {468--471},
	file = {PDF:/home/miguel/Zotero/storage/ZCVLZWQ4/Functional_Data_Analysis_for_Household_Appliance_Energy_Consumption_Prediction.pdf:application/pdf},
}

@article{pigoli_distances_2014-2,
	title = {Distances and inference for covariance operators},
	volume = {101},
	issn = {14643510},
	doi = {10.1093/biomet/asu008},
	abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
	number = {2},
	journal = {Biometrika},
	author = {Pigoli, Davide and Aston, John A.D. and Dryden, Ian L. and Secchi, Piercesare},
	year = {2014},
	note = {Publisher: Oxford University Press},
	keywords = {Functional data analysis, Shape analysis, Distance metric, Procrustes analysis},
	pages = {409--422},
	file = {PDF:/home/miguel/Zotero/storage/ZV748DVZ/asu008.pdf:application/pdf},
}

@article{liu_efficient_2018-1,
	title = {Efficient {Outlier} {Detection} for {High}-{Dimensional} {Data}},
	volume = {48},
	issn = {21682232},
	doi = {10.1109/TSMC.2017.2718220},
	abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter \$\{k\}\$ of \$\{k\}\$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
	number = {12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Liu, Huawen and Li, Xuelong and Li, Jiuyong and Zhang, Shichao},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Dimension reduction, high-dimensional data, outlier detection, k nearest neighbors (kNN), low-rank approximation},
	pages = {2451--2461},
	file = {PDF:/home/miguel/Zotero/storage/7I3ZNQ82/Efficient_Outlier_Detection_for_High-Dimensional_Data-2.pdf:application/pdf},
}

@article{liu_efficient_2018-2,
	title = {Efficient {Outlier} {Detection} for {High}-{Dimensional} {Data}},
	volume = {48},
	issn = {21682232},
	doi = {10.1109/TSMC.2017.2718220},
	abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter \$\{k\}\$ of \$\{k\}\$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
	number = {12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Liu, Huawen and Li, Xuelong and Li, Jiuyong and Zhang, Shichao},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Dimension reduction, high-dimensional data, outlier detection, k nearest neighbors (kNN), low-rank approximation},
	pages = {2451--2461},
	file = {PDF:/home/miguel/Zotero/storage/5VJ4H2CR/Efficient_Outlier_Detection_for_High-Dimensional_Data.pdf:application/pdf},
}

@article{li_ms2od_2024-2,
	title = {{MS2OD}: outlier detection using minimum spanning tree and medoid selection},
	volume = {5},
	issn = {26322153},
	doi = {10.1088/2632-2153/ad2492},
	abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
	number = {1},
	journal = {Machine Learning: Science and Technology},
	author = {Li, Jia and Li, Jiangwei and Wang, Chenxu and Verbeek, Fons J. and Schultz, Tanja and Liu, Hui},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Physics},
	keywords = {machine learning, data mining, outlier detection, clustering, medical data, medoid selection, minimum spanning tree},
	file = {PDF:/home/miguel/Zotero/storage/HLHWED8P/Li_2024_Mach._Learn.%3A_Sci._Technol._5_015025.pdf:application/pdf},
}

@article{li_variational_2019-2,
	title = {Variational autoencoder-based outlier detection for high-dimensional data},
	volume = {23},
	issn = {15714128},
	doi = {10.3233/IDA-184240},
	abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
	number = {5},
	journal = {Intelligent Data Analysis},
	author = {Li, Yongmou and Wang, Yijie and Ma, Xingkong},
	year = {2019},
	note = {Publisher: IOS Press},
	keywords = {high-dimensional data, outlier detection, Variational autoencoders},
	pages = {991--1002},
	file = {PDF:/home/miguel/Zotero/storage/ZES3BX8G/retrieve.pdf:application/pdf},
}

@article{olteanu_meta-survey_2023-1,
	title = {Meta-survey on outlier and anomaly detection},
	volume = {555},
	issn = {18728286},
	doi = {10.1016/j.neucom.2023.126634},
	abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
	journal = {Neurocomputing},
	author = {Olteanu, Madalina and Rossi, Fabrice and Yger, Florian},
	month = oct,
	year = {2023},
	note = {arXiv: 2312.07101
Publisher: Elsevier B.V.},
	keywords = {Outlier detection, Anomaly detection, Meta-survey},
	file = {PDF:/home/miguel/Zotero/storage/JWSBSFPS/1-s2.0-S0925231223007579-main.pdf:application/pdf},
}

@article{souiden_survey_2022-3,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/3LUGMKT2/1-s2.0-S1574013722000107-main.pdf:application/pdf},
}

@article{souiden_survey_2022-4,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/ZAFVMV9I/1-s2.0-S1574013722000107-main-2.pdf:application/pdf},
}

@article{mirzaie_state_2023-4,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/7CXJKRHQ/1-s2.0-S1574013723000217-main.pdf:application/pdf},
}

@article{zhang_outlier_2024-2,
	title = {Outlier {Detection} {Using} {Three}-{Way} {Neighborhood} {Characteristic} {Regions} and {Corresponding} {Fusion} {Measurement}},
	volume = {36},
	issn = {15582191},
	doi = {10.1109/TKDE.2023.3312108},
	abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Xianyong and Yuan, Zhong and Miao, Duoqian},
	month = may,
	year = {2024},
	note = {Publisher: IEEE Computer Society},
	keywords = {Data mining, outlier detection, neighborhood rough sets, three-way decision, uncertainty measurement},
	pages = {2082--2095},
	file = {PDF:/home/miguel/Zotero/storage/EIX5ZK66/Outlier.pdf:application/pdf},
}

@inproceedings{zhang_sparx_2022-1,
	title = {Sparx: {Distributed} {Outlier} {Detection} at {Scale}},
	isbn = {978-1-4503-9385-0},
	doi = {10.1145/3534678.3539076},
	abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Sean and Ursekar, Varun and Akoglu, Leman},
	month = aug,
	year = {2022},
	note = {arXiv: 2206.01281},
	keywords = {Apache Spark, data-parallel algorithms, distributed outlier detection},
	pages = {4530--4540},
	file = {PDF:/home/miguel/Zotero/storage/E4SVJZG3/Sparx- Distributed Outlier Detection at Scale.pdf:application/pdf},
}

@article{mirzaie_state_2023-5,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/6AFNPBGW/1-s2.0-S1574013723000217-main-2.pdf:application/pdf},
}

@article{fonvieille_swimming_2023-2,
	title = {Swimming in an ocean of curves: {A} functional approach to understanding elephant seal habitat use in the {Argentine} {Basin}},
	volume = {218},
	issn = {00796611},
	doi = {10.1016/j.pocean.2023.103120},
	abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
	journal = {Progress in Oceanography},
	author = {Fonvieille, Nadège and Guinet, Christophe and Saraceno, Martin and Picard, Baptiste and Tournier, Martin and Goulet, Pauline and Campagna, Claudio and Campagna, Julieta and Nerini, David},
	month = nov,
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Model-based clustering, Brazil-Malvinas confluence, Functional Data Analysis, Habitat use, Southern elephant seals},
	file = {PDF:/home/miguel/Zotero/storage/VBS5847C/1-s2.0-S0079661123001635-main.pdf:application/pdf},
}

@article{pronello_penalized_2023-2,
	title = {Penalized model-based clustering of complex functional data},
	volume = {33},
	issn = {15731375},
	doi = {10.1007/s11222-023-10288-2},
	abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Pronello, Nicola and Ignaccolo, Rosaria and Ippoliti, Luigi and Fontanella, Sara},
	month = dec,
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Functional zoning, Manifold data, Mixture models, Shape analysis, Spatial clustering, Surface data},
	file = {PDF:/home/miguel/Zotero/storage/42C2K8RV/s11222-023-10288-2.pdf:application/pdf},
}

@article{zheng_deep_2022-4,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/3SAT2CRH/1-s2.0-S1568494622004057-main-2.pdf:application/pdf},
}

@article{zheng_deep_2022-5,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/9W25AAZ7/1-s2.0-S1568494622004057-main.pdf:application/pdf},
}

@article{maturo_environmental_2024-2,
	title = {Environmental {Loss} {Assessment} via {Functional} {Outlier} {Detection} of {Transformed} {Biodiversity} {Profiles}},
	issn = {15372693},
	doi = {10.1007/s13253-024-00648-4},
	abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Maturo, Fabrizio and Porreca, Annamaria},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers},
	file = {PDF:/home/miguel/Zotero/storage/GW79D3DR/s13253-024-00648-4.pdf:application/pdf},
}

@article{jimenez-varon_pointwise_2024-1,
	title = {Pointwise data depth for univariate and multivariate functional outlier detection},
	volume = {35},
	issn = {1099095X},
	doi = {10.1002/env.2851},
	abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
	number = {5},
	journal = {Environmetrics},
	author = {Jiménez-Varón, Cristian F. and Harrou, Fouzi and Sun, Ying},
	month = aug,
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {functional data, data depth, magnitude outliers, pairwise depth, pointwise depth, shape outliers, visualization},
	file = {PDF:/home/miguel/Zotero/storage/RYFULDJ7/Environmetrics - 2024 - Jim%C3%A9nez%E2%80%90Var%C3%B3n - Pointwise data depth for univariate and multivariate functional outlier detection.pdf:application/pdf},
}

@article{zheng_deep_2022-6,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/2KXF2YUU/ZHENG HYPERSPHERE.pdf:application/pdf},
}

@article{zhu_high-dimensional_2023-2,
	title = {A {High}-{Dimensional} {Outlier} {Detection} {Approach} {Based} on {Local} {Coulomb} {Force}},
	volume = {35},
	issn = {15582191},
	doi = {10.1109/TKDE.2022.3172167},
	abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Pengyun and Zhang, Chaowei and Li, Xiaofeng and Zhang, Jifu and Qin, Xiao},
	month = jun,
	year = {2023},
	note = {Publisher: IEEE Computer Society},
	keywords = {High-dimensional outlier detection, local outlier coulomb force, neighborhood outlier factor, outlier coulomb resultant force, similarity metric},
	pages = {5506--5520},
	file = {PDF:/home/miguel/Zotero/storage/DCJ7KNKU/2023 Pengyun Zhu - A High Dimensional Outlier Detection Approach Base [retrieved_2025-01-01].pdf:application/pdf},
}

@article{ruff_unifying_2020,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2009.11732},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Grégoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Müller, Klaus-Robert},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.11732},
	file = {PDF:/home/miguel/Zotero/storage/LQQ628XZ/2009.11732v3.pdf:application/pdf},
}

@techreport{du_dream_nodate-1,
	title = {Dream the {Impossible}: {Outlier} {Imagination} with {Diffusion} {Models}},
	url = {https://github.com/deeplearning-wisc/dream-ood.},
	abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
	author = {Du, Xuefeng and Sun, Yiyou and Zhu, Xiaojin and Li, Yixuan},
	file = {PDF:/home/miguel/Zotero/storage/7XRX4G9Z/NeurIPS-2023-dream-the-impossible-outlier-imagination-with-diffusion-models-Paper-Conference.pdf:application/pdf},
}

@article{porreca_identifying_2024-1,
	title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized {Hill}’s numbers and their integral functions},
	issn = {15737845},
	doi = {10.1007/s11135-024-01876-z},
	abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
	journal = {Quality and Quantity},
	author = {Porreca, Annamaria and Maturo, Fabrizio},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers, Normalized Hill’s functions, Standardized Hill’s functions},
	file = {PDF:/home/miguel/Zotero/storage/Z352BAJH/s11135-024-01876-z.pdf:application/pdf},
}

@techreport{salehi_unified_nodate-3,
	title = {A {Unified} {Survey} on {Anomaly}, {Novelty}, {Open}-{Set}, and {Out}-of-{Distribution} {Detection}: {Solutions} and {Future} {Challenges}},
	url = {https://github.com/taslimisina/osr-ood-ad-methods},
	abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
	author = {Salehi, Mohammadreza and Nl, Salehidehnavi@uva and Mirzaei, Hossein and Hendrycks, Dan and Li, Yixuan and Rohban, Mohammad Hossein and Sabokrou, Mohammad},
	file = {PDF:/home/miguel/Zotero/storage/Y7TGUCHZ/234_A_Unified_Survey_on_Anomal.pdf:application/pdf},
}

@book{mateu_geostatistical_2022-2,
	title = {Geostatistical functional data analysis},
	isbn = {978-1-119-38784-8},
	abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Mateu, Jorge. and Giraldo, Ramon.},
	year = {2022},
	file = {PDF:/home/miguel/Zotero/storage/Y88Q72LN/Geostatistical Functional Data Analysis - 2021 - Mateu.pdf:application/pdf},
}

@article{li_outlier_2020-2,
	title = {Outlier {Detection} {Using} {Structural} {Scores} in a {High}-{Dimensional} {Space}},
	volume = {50},
	issn = {21682275},
	doi = {10.1109/TCYB.2018.2876615},
	abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
	number = {5},
	journal = {IEEE Transactions on Cybernetics},
	author = {Li, Xiaojie and Lv, Jiancheng and Yi, Zhang},
	month = may,
	year = {2020},
	pmid = {30418896},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {outlier detection, Discrimination, outlier factor, structural scores},
	pages = {2302--2310},
	file = {PDF:/home/miguel/Zotero/storage/VMDA5LIM/Outlier_Detection_Using_Structural_Scores_in_a_High-Dimensional_Space.pdf:application/pdf},
}

@article{todorov_r_2024-2,
	title = {The {R} {Package} {Ecosystem} for {Robust} {Statistics}},
	volume = {16},
	issn = {19390068},
	doi = {10.1002/wics.70007},
	abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
	number = {6},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Todorov, Valentin},
	month = nov,
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {robust, R, high dimensions, multivariate, outlier},
	file = {PDF:/home/miguel/Zotero/storage/MSU5ECT8/WIREs Computational Stats - 2024 - Todorov - The R Package Ecosystem for Robust Statistics.pdf:application/pdf},
}

@article{dietzel_shrinkage-based_2024-2,
	title = {Shrinkage-based {Bayesian} variable selection for species distribution modelling in complex environments: {An} application to urban biodiversity},
	volume = {81},
	issn = {15749541},
	doi = {10.1016/j.ecoinf.2024.102561},
	abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
	journal = {Ecological Informatics},
	author = {Dietzel, Andreas and Moretti, Marco and Cook, Lauren M.},
	month = jul,
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bayesian projection predictive variable selection, Blue-green infrastructure, Nature-based solutions, Shrinkage prior, Species distribution model, Urban biodiversity},
	file = {PDF:/home/miguel/Zotero/storage/8DTX9WDY/1-s2.0-S1574954124001031-main.pdf:application/pdf},
}

@article{ruff_unifying_2021-1,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	volume = {109},
	issn = {15582256},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gregoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Muller, Klaus Robert},
	month = may,
	year = {2021},
	note = {arXiv: 2009.11732
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {neural networks, deep learning, outlier detection, Anomaly detection (AD), explainable artificial intelligence, interpretability, kernel methods, novelty detection, one-class classification, out-of-distribution (OOD) detection, unsupervised learning.},
	pages = {756--795},
	file = {PDF:/home/miguel/Zotero/storage/8RT9SDG7/A_Unifying_Review_of_Deep_and_Shallow_Anomaly_Detection-2.pdf:application/pdf},
}

@article{noauthor_untitled_nodate,
	title = {Untitled},
	file = {Untitled.R:/home/miguel/Zotero/storage/FHV9PQDT/Untitled.R:application/octet-stream},
}

@inproceedings{wallace_threat_2024-1,
	title = {The threat of coastal hypoxia in eastern {Canadian} waters - {New} opportunities for its mitigation and the potential of {Bedford} {Basin} ({Nova} {Scotia}) for required pilot study and demonstration},
	isbn = {979-8-3315-4008-1},
	doi = {10.1109/OCEANS55160.2024.10754451},
	abstract = {Ocean deoxygenation and expansion and intensification of hypoxia is a growing threat to marine biodiversity worldwide, especially in coastal waters. The causes include eutrophication due to excess input of nutrients. However, increasingly, warming and especially circulation- and mixing-related changes to oxygen supply connected with climate change are driving oxygen concentrations downwards. This is now a major problem on the Canadian east coast with growing hypoxic zones in the Lower St. Lawrence Estuary, as well as threats to shelf waters of Nova Scotia and coastal basins, including Bedford Basin. Current measures to protect marine biodiversity (e.g. marine protected areas) are ineffectual in addressing this threat. However, the sudden emergence of a green hydrogen industry offers a possible solution. Here we highlight the threat to eastern Canadian ecosystems, including commercial fish stocks, from hypoxia as well as the underlying causes and the potential for a solution. In the case of the Gulf of St. Lawrence, planned construction of a hydrogen plant near Stephenville, NL, holds potential to mitigate the growing threat through use of the oxygen by-product of hydrogen generation. Based on measurements and tracer studies, the location and magnitude of production appear to be 'just right' to compensate for current oxygen losses. This mitigation approach (direct oxygen injection) has rarely been considered for marine environments to-date and involves magnitudes and timescales of oxygen transport and response that have not been attempted before. Nevertheless, it seems essential and urgent for ocean scientists, engineers and policymakers to work together to address this threat and explore its potential solution. A coordinated research effort into direct oxygen injection, involving broad multidisciplinary expertise should be established immediately. Field scale pilot studies in well-characterized, accessible locations appear to be the next key step required, as well as planning for larger scale deployment. There are several reasons why Bedford Basin, Nova Scotia is an ideal location for pilot studies and demonstrations. The questions that need to be addressed, the suitability of Bedford Basin, pilot study design and practicality issues are presented and discussed.},
	booktitle = {Oceans {Conference} {Record} ({IEEE})},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wallace, Douglas and Austin, David},
	year = {2024},
	note = {ISSN: 01977385},
	keywords = {Bedford Basin, deoxygenation, direct oxygen injection, ecosystem restoration, green hydrogen, Gulf of St. Lawrence, hypoxia, marine ecosystem, reoxygenation},
	file = {PDF:/home/miguel/Zotero/storage/G4JJ4A84/The_threat_of_coastal_hypoxia_in_eastern_Canadian_waters_-_new_opportunities_for_its_mitigation_and_the_potential_of_Bedford_Basin_Nova_Scotia_for_required_pilot_study_and_demonstration.pdf:application/pdf},
}

@article{xia_numerical_2011-1,
	title = {Numerical simulation of salinity and dissolved oxygen at {Perdido} {Bay} and adjacent coastal ocean},
	volume = {27},
	issn = {07490208},
	doi = {10.2112/JCOASTRES-D-09-00044.1},
	abstract = {Environmental fluid dynamic code (EFDC), a numerical estuarine and coastal ocean circulation hydrodynamic model, was used to simulate the distribution of the salinity, temperature, nutrients, and dissolved oxygen (DO) in Perdido Bay and adjacent Gulf of Mexico. External forcing factors included the coupled effects of the astronomical tides, river discharge, and atmospheric winds on the spatial and temporal distributions of salinity and DO. Modeled time series were in good agreement with field observations of water level, nutrients, temperature, salinity, and DO. Perdido Bay and adjacent northern Gulf of Mexico coasts can be divided into two areas according to salinity, water level, and DO concentrations. The first area was lower Perdido Bay and the associated Gulf of Mexico coasts, acting primarily under the influence of tidal forcing, which increases the vertical stratification. The second division was upper Perdido Bay, which was influenced by both tidal forcing and freshwater inflow. Simulations also indicated winds influenced the salinity and DO distributions, with an enhanced surface pressure gradient. Tidal effects were also important for conducting salinity and water quality simulations in Perdido Bay. Low amplitude tides induced relatively weak vertical mixing and favored the establishment of stratification at the bay, especially along deeper bathymetry. Flood tides influenced the distribution of salinity and DO more than ebb tides, specifically along shallow bathymetry. © Coastal Education \& Research Foundation 2011.},
	number = {1},
	journal = {Journal of Coastal Research},
	author = {Xia, Meng and Craig, Paul M. and Wallen, Christopher M. and Stoddard, Andrew and Mandrup-Poulsen, Jan and Peng, Machuan and Schaeffer, Blake and Liu, Zhijun},
	month = dec,
	year = {2011},
	keywords = {dissolved oxygen, EFDC, Perdido Bay, plume, salinity},
	pages = {73--86},
	file = {PDF:/home/miguel/Zotero/storage/2MJNFK5S/Numerical_Simulation_of_Salini.pdf:application/pdf},
}

@article{noauthor_case_study_ssc_nodate,
	title = {case\_study\_ssc},
	file = {case_study_ssc.Rproj:/home/miguel/Zotero/storage/VRNHWTRX/case_study_ssc.Rproj:application/octet-stream},
}

@article{xia_numerical_2011-2,
	title = {Numerical simulation of salinity and dissolved oxygen at {Perdido} {Bay} and adjacent coastal ocean},
	volume = {27},
	issn = {07490208},
	doi = {10.2112/JCOASTRES-D-09-00044.1},
	abstract = {Environmental fluid dynamic code (EFDC), a numerical estuarine and coastal ocean circulation hydrodynamic model, was used to simulate the distribution of the salinity, temperature, nutrients, and dissolved oxygen (DO) in Perdido Bay and adjacent Gulf of Mexico. External forcing factors included the coupled effects of the astronomical tides, river discharge, and atmospheric winds on the spatial and temporal distributions of salinity and DO. Modeled time series were in good agreement with field observations of water level, nutrients, temperature, salinity, and DO. Perdido Bay and adjacent northern Gulf of Mexico coasts can be divided into two areas according to salinity, water level, and DO concentrations. The first area was lower Perdido Bay and the associated Gulf of Mexico coasts, acting primarily under the influence of tidal forcing, which increases the vertical stratification. The second division was upper Perdido Bay, which was influenced by both tidal forcing and freshwater inflow. Simulations also indicated winds influenced the salinity and DO distributions, with an enhanced surface pressure gradient. Tidal effects were also important for conducting salinity and water quality simulations in Perdido Bay. Low amplitude tides induced relatively weak vertical mixing and favored the establishment of stratification at the bay, especially along deeper bathymetry. Flood tides influenced the distribution of salinity and DO more than ebb tides, specifically along shallow bathymetry. © Coastal Education \& Research Foundation 2011.},
	number = {1},
	journal = {Journal of Coastal Research},
	author = {Xia, Meng and Craig, Paul M. and Wallen, Christopher M. and Stoddard, Andrew and Mandrup-Poulsen, Jan and Peng, Machuan and Schaeffer, Blake and Liu, Zhijun},
	month = dec,
	year = {2011},
	keywords = {dissolved oxygen, EFDC, Perdido Bay, plume, salinity},
	pages = {73--86},
	file = {PDF:/home/miguel/Zotero/storage/GXQSDJUV/Numerical_Simulation_of_Salini.pdf:application/pdf},
}

@inproceedings{wallace_threat_2024-2,
	title = {The threat of coastal hypoxia in eastern {Canadian} waters - {New} opportunities for its mitigation and the potential of {Bedford} {Basin} ({Nova} {Scotia}) for required pilot study and demonstration},
	isbn = {979-8-3315-4008-1},
	doi = {10.1109/OCEANS55160.2024.10754451},
	abstract = {Ocean deoxygenation and expansion and intensification of hypoxia is a growing threat to marine biodiversity worldwide, especially in coastal waters. The causes include eutrophication due to excess input of nutrients. However, increasingly, warming and especially circulation- and mixing-related changes to oxygen supply connected with climate change are driving oxygen concentrations downwards. This is now a major problem on the Canadian east coast with growing hypoxic zones in the Lower St. Lawrence Estuary, as well as threats to shelf waters of Nova Scotia and coastal basins, including Bedford Basin. Current measures to protect marine biodiversity (e.g. marine protected areas) are ineffectual in addressing this threat. However, the sudden emergence of a green hydrogen industry offers a possible solution. Here we highlight the threat to eastern Canadian ecosystems, including commercial fish stocks, from hypoxia as well as the underlying causes and the potential for a solution. In the case of the Gulf of St. Lawrence, planned construction of a hydrogen plant near Stephenville, NL, holds potential to mitigate the growing threat through use of the oxygen by-product of hydrogen generation. Based on measurements and tracer studies, the location and magnitude of production appear to be 'just right' to compensate for current oxygen losses. This mitigation approach (direct oxygen injection) has rarely been considered for marine environments to-date and involves magnitudes and timescales of oxygen transport and response that have not been attempted before. Nevertheless, it seems essential and urgent for ocean scientists, engineers and policymakers to work together to address this threat and explore its potential solution. A coordinated research effort into direct oxygen injection, involving broad multidisciplinary expertise should be established immediately. Field scale pilot studies in well-characterized, accessible locations appear to be the next key step required, as well as planning for larger scale deployment. There are several reasons why Bedford Basin, Nova Scotia is an ideal location for pilot studies and demonstrations. The questions that need to be addressed, the suitability of Bedford Basin, pilot study design and practicality issues are presented and discussed.},
	booktitle = {Oceans {Conference} {Record} ({IEEE})},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wallace, Douglas and Austin, David},
	year = {2024},
	note = {ISSN: 01977385},
	keywords = {Bedford Basin, deoxygenation, direct oxygen injection, ecosystem restoration, green hydrogen, Gulf of St. Lawrence, hypoxia, marine ecosystem, reoxygenation},
	file = {PDF:/home/miguel/Zotero/storage/8AQYVSJP/The_threat_of_coastal_hypoxia_in_eastern_Canadian_waters_-_new_opportunities_for_its_mitigation_and_the_potential_of_Bedford_Basin_Nova_Scotia_for_required_pilot_study_and_demonstration.pdf:application/pdf},
}

@article{lavine_markov_1999,
	title = {A {Markov} random field spatio-temporal analysis of ocean temperature},
	abstract = {The National Oceanic Data Center (NODC) contains historical records from approximately 144,000 hydrographic stations in the North Atlantic. This data has been used by oceanographers to construct maps of point estimates of pressure, temperature, salinity and oxygen in the North Atlantic (Levitus (1994); Lozier et al. (1995)). Because data from any particular year are scarce, the previous maps have been for time-averaged values only. In addition, the maps have been reported without uncertainty estimates. This paper presents a Markov random ®eld (MRF) analysis that can generate maps for speci®c time periods along with associated uncertainties. To estimate changes in oceanic properties over time previous oceanographic work has focused on differences between a few time periods each having many observations. Due to data scarcity this poses a severe restriction for both spatial and temporal coverage of climatic change. The MRF analysis provides a means for temporal modeling that does not require high data density at each time period. To demonstrate the usefulness of a MRF analysis of oceanic data we investigate the temporal variability along 24.5 N in the North Atlantic. Our results are compared to an earlier analysis (Parrilla et al. (1994)) where data from only three time periods was used. We obtain a more thorough understanding of the temperature change found by this previous study.},
	number = {6},
	journal = {Environmental and Ecological Statistics},
	author = {Lavine, Michael and Lozier, Susan},
	year = {1999},
	keywords = {Academic, Bayesian analysis 1352-8505 \# 1999, Kluwer, Publishers},
	file = {PDF:/home/miguel/Zotero/storage/ZCUMFS6E/A-1009631429791-2.pdf:application/pdf},
}

@book{ferreira_modeling_2025,
	title = {Modeling {Spatio}-{Temporal} {Data}},
	abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
	publisher = {Taylor and Francis},
	author = {Ferreira, Marco A.R.},
	year = {2025},
	file = {PDF:/home/miguel/Zotero/storage/GX85MHEQ/9781032623443_previewpdf.pdf:application/pdf},
}

@techreport{ferreira_modeling_nodate,
	title = {Modeling {Spatio}-{Temporal} {Data}: {Markov} {Random} {Fields}, {Objective} {Bayes}, and {Multiscale} {Models}},
	abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
	author = {Ferreira, Marco AR},
	keywords = {areal data, Bayesian, dynamic linear models, ensembles of models, hierarchical models, Markov random fields, model selection paradox, multiscale models, objective Bayes, spatio-temporal modeling},
	file = {PDF:/home/miguel/Zotero/storage/4497379I/Modeling Spatio-Temporal Data_25_04_13_14_08_57.pdf:application/pdf},
}

@article{wikle_modern_2013,
	title = {Modern statistical methods in oceanography: {A} hierarchical perspective},
	volume = {28},
	issn = {08834237},
	doi = {10.1214/13-STS436},
	abstract = {Processes in ocean physics, air-sea interaction and ocean biogeochemistry span enormous ranges in spatial and temporal scales, that is, from molecular to planetary and from seconds to millennia. Identifying and implementing sustainable human practices depend critically on our understandings of key aspects of ocean physics and ecology within these scale ranges. The set of all ocean data is distorted such that three- and four-dimensional (i.e., timedependent) in situ data are very sparse, while observations of surface and upper ocean properties from space-borne platforms have become abundant in the past few decades. Precisions in observations of all types vary as well. In the face of these challenges, the interface between Statistics and Oceanography has proven to be a fruitful area for research and the development of useful models. With the recognition of the key importance of identifying, quantifying and managing uncertainty in data and models of ocean processes, a hierarchical perspective has become increasingly productive. As examples, we review a heterogeneous mix of studies from our own work demonstrating Bayesian hierarchical model applications in ocean physics, air-sea interaction, ocean forecasting and ocean ecosystem models. This review is by no means exhaustive and we have endeavored to identify hierarchical modeling work reported by others across the broad range of ocean-related topics reported in the statistical literature. We conclude by noting relevant oceanstatistics problems on the immediate research horizon, and some technical challenges they pose, for example, in terms of nonlinearity, dimensionality and computing ©Institute of Mathematical Statistics, 2013.},
	number = {4},
	journal = {Statistical Science},
	author = {Wikle, Christopher K. and Milliff, Ralph F. and Herbei, Radu and Leeds, William B.},
	month = nov,
	year = {2013},
	keywords = {Bayesian, Biogeochemical, Ecosystem, Ocean vector winds, Quadratic nonlinearity, Sea surface temperature, Spatio-temporal, State-space},
	pages = {466--486},
	file = {PDF:/home/miguel/Zotero/storage/TVQN29Y8/13-STS436.pdf:application/pdf},
}

@inproceedings{li_ocean_2023,
	title = {Ocean {Data} {Quality} {Assessment} through {Outlier} {Detection}-enhanced {Active} {Learning}},
	isbn = {979-8-3503-2445-7},
	doi = {10.1109/BigData59044.2023.10386969},
	abstract = {Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an Outlier Detection-Enhanced Active Learning (ODEAL) framework for ocean data quality assessment, employing Active Learning (AL) to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5\% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9\% using the initial set built with outlier detectors.},
	booktitle = {Proceedings - 2023 {IEEE} {International} {Conference} on {Big} {Data}, {BigData} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Li, Na and Qi, Yiyang and Xin, Ruyue and Zhao, Zhiming},
	year = {2023},
	keywords = {machine learning, active learning, Argo, initial set construction, ocean data quality control},
	pages = {102--107},
	file = {PDF:/home/miguel/Zotero/storage/RGNX8IA9/Ocean_Data_Quality_Assessment_through_Outlier_Detection-enhanced_Active_Learning.pdf:application/pdf},
}

@article{diamant_cross-sensor_2020,
	title = {Cross-sensor quality assurance for marine observatories},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12213470},
	abstract = {Measuring and forecasting changes in coastal and deep-water ecosystems and climates requires sustained long-term measurements from marine observation systems. One of the key considerations in analyzing data from marine observatories is quality assurance (QA). The data acquired by these infrastructures accumulates into Giga and Terabytes per year, necessitating an accurate automatic identification of false samples. A particular challenge in the QA of oceanographic datasets is the avoidance of disqualification of data samples that, while appearing as outliers, actually represent real short-term phenomena, that are of importance. In this paper, we present a novel cross-sensor QA approach that validates the disqualification decision of a data sample from an examined dataset by comparing it to samples from related datasets. This group of related datasets is chosen so as to reflect upon the same oceanographic phenomena that enable some prediction of the examined dataset. In our approach, a disqualification is validated if the detected anomaly is present only in the examined dataset, but not in its related datasets. Results for a surface water temperature dataset recorded by our Texas A\&M—Haifa Eastern Mediterranean Marine Observatory (THEMO)—over a period of 7 months, show an improved trade-off between accurate and false disqualification rates when compared to two standard benchmark schemes.},
	number = {21},
	journal = {Remote Sensing},
	author = {Diamant, Roee and Shachar, Ilan and Makovsky, Yizhaq and Ferreira, Bruno Miguel and Cruz, Nuno Alexandre},
	month = nov,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Regression, Change detector, Data validation, Ocean observatories, Ocean remote sensing, Prediction of data, Quality assurance, Quality control},
	pages = {1--16},
	file = {PDF:/home/miguel/Zotero/storage/ATU9UPE5/remotesensing-12-03470.pdf:application/pdf},
}

@article{skalvik_challenges_2023,
	title = {Challenges, limitations, and measurement strategies to ensure data quality in deep-sea sensors},
	volume = {10},
	issn = {22967745},
	doi = {10.3389/fmars.2023.1152236},
	abstract = {In this paper we give an overview of factors and limitations impairing deep-sea sensor data, and we show how automatic tests can give sensors self-validation and self-diagnostic capabilities. This work is intended to lay a basis for sophisticated use of smart sensors in long-term autonomous operation in remote deep-sea locations. Deep-sea observation relies on data from sensors operating in remote, harsh environments which may affect sensor output if uncorrected. In addition to the environmental impact, sensors are subject to limitations regarding power, communication, and limitations on recalibration. To obtain long-term measurements of larger deep-sea areas, fixed platform sensors on the ocean floor may be deployed for several years. As for any observation systems, data collected by deep-sea observation equipment are of limited use if the quality or accuracy (closeness of agreement between the measurement and the true value) is not known. If data from a faulty sensor are used directly, this may result in an erroneous understanding of deep water conditions, or important changes or conditions may not be detected. Faulty sensor data may significantly weaken the overall quality of the combined data from several sensors or any derived model. This is particularly an issue for wireless sensor networks covering large areas, where the overall measurement performance of the network is highly dependent on the data quality from individual sensors. Existing quality control manuals and initiatives for best practice typically recommend a selection of (near) real-time automated checks. These are mostly limited to basic and straight forward verification of metadata and data format, and data value or transition checks against pre-defined thresholds. Delayed-mode inspection is often recommended before a final data quality stamp is assigned.},
	journal = {Frontiers in Marine Science},
	author = {Skålvik, Astrid Marie and Saetre, Camilla and Frøysa, Kjell Eivind and Bjørk, Ranveig N. and Tengberg, Anders},
	year = {2023},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {calibration, data quality, in-situ, self-validation, sensor},
	file = {PDF:/home/miguel/Zotero/storage/AFRHRUSB/fmars-10-1152236.pdf:application/pdf},
}

@article{jutras_temporal_2023,
	title = {Temporal and spatial evolution of bottom-water hypoxia in the {St} {Lawrence} estuarine system},
	volume = {20},
	issn = {17264189},
	doi = {10.5194/bg-20-839-2023},
	abstract = {Persistent hypoxic bottom waters have developed in the Lower St Lawrence Estuary (LSLE) and have impacted fish and benthic species distributions. Minimum dissolved oxygen concentrations decreased from ∼ 125 μmol L-1 (38 \% saturation) in the 1930s to ∼ 65 μmol L-1 (21 \% saturation) in 1984. Minimum dissolved oxygen concentrations remained at hypoxic levels ({\textless} 62.5 μM = 2 mg L-1 or 20 \% saturation) between 1984 and 2019, but in 2020, they suddenly decreased to ∼ 35 μmol L-1. Concurrently, bottom-water temperatures in the LSLE have increased progressively from ∼ 3 °C in the 1930s to nearly 7 °C in 2021. The main driver of deoxygenation and warming in the bottom waters of the Gulf of St Lawrence and St Lawrence Estuary is a change in the circulation pattern in the western North Atlantic, more specifically a decrease in the relative contribution of younger, well-oxygenated and cold Labrador Current Waters to the waters of the Laurentian Channel, a deep valley that extends from the continental shelf edge, through Cabot Strait, the gulf and to the head of the LSLE. Hence, the warmer, oxygen-depleted North Atlantic Central Waters carried by the Gulf Stream now make up nearly 100 \% of the waters entering the Laurentian Channel. The areal extent of the hypoxic zone in the LSLE has varied since 1993 when it was first estimated at 1300 km2. In 2021, it reached 9400 km2, extending well into the western Gulf of St Lawrence. Severely hypoxic waters are now also found at the end of the two deep channels that branch out from the Laurentian Channel, namely, the Esquiman Channel and Anticosti Channel. Copyright:},
	number = {4},
	journal = {Biogeosciences},
	author = {Jutras, Mathilde and Mucci, Alfonso and Chaillou, Gwenaëlle and Nesbitt, William A. and Wallace, Douglas W.R.},
	month = feb,
	year = {2023},
	note = {Publisher: Copernicus Publications},
	pages = {839--849},
	file = {PDF:/home/miguel/Zotero/storage/ZGGGUFFN/bg-20-839-2023.pdf:application/pdf},
}

@techreport{graler_copulas_nodate,
	title = {Copulas, a novel approach to model spatial and spatio-temporal dependence},
	abstract = {Copulas are a statistical concept which allows for a novel approach to model dependencies of spatial and spatio-temporal variables. They capture the dependence structure of a multivariate distribution over its whole range detached from its specific margins. In contrast to a single measure of association, this allows for varying strength of dependence throughout the multivariate distribution. Thus, copulas are capable of capturing many different (i.e. non-Gaussian) dependence structures and allow for asymmet-ric dependencies which can be found in many natural processes. We applied this approach to data from the deforestation survey of the Brazilian Amazon in order to capture the dependence of deforestation on a selection of variables .},
	author = {Gräler, Benedikt and Kazianka, Hannes and Mira De Espindola, Giovana},
	file = {PDF:/home/miguel/Zotero/storage/DSHDKG4S/GEOChange_GraelerKaziankaDeEspindola.pdf:application/pdf},
}

@article{law_predicting_2008,
	title = {Predicting and monitoring the effects of large-scale ocean iron fertilization on marine trace gas emissions},
	volume = {364},
	issn = {01718630},
	doi = {10.3354/meps07549},
	abstract = {Large-scale ({\textgreater}40 000 km2, {\textgreater}1 yr) ocean iron fertilization (OIF) is being considered as an option for mitigating the increase in atmospheric CO2 concentrations. However OIF will influence trace gas production and atmospheric emissions, with consequences over broad temporal and spatial scales. To illustrate this, the response of nitrous oxide (N 2O) and dimethylsulphide (DMS) in the mesoscale iron addition experiments (FeAXs) and model scenarios of large-scale OIF are examined. FeAXs have shown negligible to minor increases in N2O production, whereas models of long-term OIF suggest significant N2O production with the potential to offset the benefit gained by iron-mediated increases in CO 2 uptake. N2O production and emission will be influenced by the magnitude and rate of vertical particle export, and along-isopycnal N2O transport will necessitate monitoring over large spatial scales. The N2O-O2 relationship provides a monitoring option using oxygen as a proxy, with spatial coverage by Argo and glider-mounted oxygen optodes. Although the initial FeAXs exhibited similar increases (1.5- to 1.6-fold) in DMS, a subsequent sub-arctic Pacific experiment observed DMS consumption relative to unfertilized waters, highlighting regional variability as a complicating factor when predicting the effects of large-scale OIF. DMS cycling and its influence on atmospheric composition may be studied using naturally occurring blooms and be constrained prior to OIF by pre-fertilization spatial mapping and aerial sampling using new technologies. As trace gases may have positive or negative synergistic effects on atmospheric chemistry and climate forcing, the net effect of altered trace gas emissions needs to be considered in both models and monitoring of large-scale OIF. © Inter-Research 2008.},
	journal = {Marine Ecology Progress Series},
	author = {Law, C. S.},
	month = jul,
	year = {2008},
	keywords = {Dimethlysulphide, Iron fertilization, Nitrous oxide, Remineralization, Trace gases},
	pages = {283--288},
	file = {PDF:/home/miguel/Zotero/storage/CDIKGW4M/m364p283.pdf:application/pdf},
}

@article{wikle_modern_2015,
	title = {Modern perspectives on statistics for spatio-temporal data},
	volume = {7},
	issn = {19390068},
	doi = {10.1002/wics.1341},
	abstract = {Spatio-temporal statistical models are increasingly being used across a wide variety of scientific disciplines to describe and predict spatially explicit processes that evolve over time. Although descriptive models that approach this problem from the second-order (covariance) perspective are important, many real-world processes are dynamic, and it can be more efficient in such cases to characterize the associated spatio-temporal dependence by the use of dynamical models. The challenge with the specification of such dynamical models has been related to the curse of dimensionality and the specification of realistic dependence structures. Even in fairly simple linear/Gaussian settings, spatio-temporal statistical models are often over parameterized. This problem is compounded when the spatio-temporal dynamical processes are nonlinear or multivariate. Hierarchical models have proven invaluable in their ability to deal to some extent with this issue by allowing dependency among groups of parameters and science-based parameterizations. Such models are best considered from a Bayesian perspective, with associated computational challenges. Spatio-temporal statistics remains an active and vibrant area of research.},
	number = {1},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Wikle, Christopher K.},
	month = jan,
	year = {2015},
	note = {Publisher: Wiley-Blackwell},
	keywords = {Quadratic nonlinearity, Bayesian hierarchical models, Rank reduction, Spatial basis functions, Spatio-temporal dynamic models},
	pages = {86--98},
	file = {PDF:/home/miguel/Zotero/storage/B7RDBFE9/WIREs Computational Stats - 2014 - Wikle - Modern perspectives on statistics for spatio%E2%80%90temporal data.pdf:application/pdf},
}

@article{tang_copula-based_2019,
	title = {Copula-based semiparametric models for spatiotemporal data},
	volume = {75},
	issn = {15410420},
	doi = {10.1111/biom.13066},
	abstract = {The joint analysis of spatial and temporal processes poses computational challenges due to the data's high dimensionality. Furthermore, such data are commonly non-Gaussian. In this paper, we introduce a copula-based spatiotemporal model for analyzing spatiotemporal data and propose a semiparametric estimator. The proposed algorithm is computationally simple, since it models the marginal distribution and the spatiotemporal dependence separately. Instead of assuming a parametric distribution, the proposed method models the marginal distributions nonparametrically and thus offers more flexibility. The method also provides a convenient way to construct both point and interval predictions at new times and locations, based on the estimated conditional quantiles. Through a simulation study and an analysis of wind speeds observed along the border between Oregon and Washington, we show that our method produces more accurate point and interval predictions for skewed data than those based on normality assumptions.},
	number = {4},
	journal = {Biometrics},
	author = {Tang, Yanlin and Wang, Huixia J. and Sun, Ying and Hering, Amanda S.},
	month = dec,
	year = {2019},
	pmid = {31009058},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {copula, Markov process, pseudo-likelihood, spatiotemporal},
	pages = {1156--1167},
	file = {PDF:/home/miguel/Zotero/storage/RJFX852Q/biometrics_75_4_1156.pdf:application/pdf},
}

@techreport{graler_spcopula_nodate,
	title = {spcopula: {Modelling} {Spatial} and {Spatio}-{Temporal} {Dependence} with {Copulas} in {R}},
	abstract = {The spcopula R package provides tools to model spatial and spatio-temporal phenomena with spatial and spatio-temporal vine copulas. Copulas allow us to flexibly build multivariate distributions with mixed margins where the copula describes the multivari-ate dependence structure coupling the margins. In classical geostatistics, a multivariate Gaussian distribution is typically assumed and dependence is summarized in a covari-ance matrix implying limitations like elliptical symmetry in the strength of dependence. Copulas allow for dependence structures beyond the Gaussian one, being for instance asymmetric. We developed the spatio-temporal vine copulas such that the bivariate cop-ula families in the lower trees may change with distance across space and time allowing not only for a varying strength of dependence but also for a changing dependence structure. These spatio-temporal distributions are used to predict values at unobserved locations, assess risk, or run simulations. Based on the concept of vine copulas, the spcopula package provides a large set of multivariate distributions. As bivariate spatial copulas do not have any probabilistic restrictions, the spatial vine copula is a powerful approach for modelling skewed or heavy tailed data with complex and potentially asymmetric dependence structures in the spatial and spatio-temporal domain.},
	author = {Gräler, Benedikt},
	keywords = {interpolation, multivariate distributions, spatial data, spatial modelling},
	file = {PDF:/home/miguel/Zotero/storage/IS6J75Y9/spcopula.pdf:application/pdf},
}

@inproceedings{li_ocean_2023-1,
	title = {Ocean {Data} {Quality} {Assessment} through {Outlier} {Detection}-enhanced {Active} {Learning}},
	isbn = {979-8-3503-2445-7},
	doi = {10.1109/BigData59044.2023.10386969},
	abstract = {Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an Outlier Detection-Enhanced Active Learning (ODEAL) framework for ocean data quality assessment, employing Active Learning (AL) to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5\% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9\% using the initial set built with outlier detectors.},
	booktitle = {Proceedings - 2023 {IEEE} {International} {Conference} on {Big} {Data}, {BigData} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Li, Na and Qi, Yiyang and Xin, Ruyue and Zhao, Zhiming},
	year = {2023},
	keywords = {machine learning, active learning, Argo, initial set construction, ocean data quality control},
	pages = {102--107},
	file = {PDF:/home/miguel/Zotero/storage/UQUPXS4T/Ocean_Data_Quality_Assessment_through_Outlier_Detection-enhanced_Active_Learning.pdf:application/pdf},
}

@article{hull_simultaneous_2021,
	title = {Simultaneous assessment of oxygen-and nitrate-based net community production in a temperate shelf sea from a single ocean glider},
	volume = {18},
	issn = {17264189},
	doi = {10.5194/bg-18-6167-2021},
	abstract = {The continental shelf seas are important at a global scale for ecosystem services. These highly dynamic regions are under a wide range of stresses, and as such future management requires appropriate monitoring measures. A key metric to understanding and predicting future change are the rates of biological production. We present here the use of an autonomous underwater glider with an oxygen (O2) and a wet-chemical microfluidic total oxidised nitrogen (NOx-NO3-+NO2-) sensor during a spring bloom as part of a 2019 pilot autonomous shelf sea monitoring study. We find exceptionally high rates of net community production using both O2 and NOx-water column inventory changes, corrected for air-sea gas exchange in case of O2. We compare these rates with 2007 and 2008 mooring observations finding similar rates of NOx-consumption. With these complementary methods we determine the O2:N amount ratio of the newly produced organic matter (7.8g±0.4) and the overall O2:N ratio for the total water column (5.7g±0.4). The former is close to the canonical Redfield O2:N ratio of 8.6g±1.0, whereas the latter may be explained by a combination of new organic matter production and preferential remineralisation of more reduced organic matter at a higher O2:N ratio below the euphotic zone.},
	number = {23},
	journal = {Biogeosciences},
	author = {Hull, Tom and Greenwood, Naomi and Birchill, Antony and Beaton, Alexander and Palmer, Matthew and Kaiser, Jan},
	month = dec,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {6167--6180},
	file = {PDF:/home/miguel/Zotero/storage/KG9T2IHR/bg-18-6167-2021.pdf:application/pdf},
}

@article{vanem_bayesian_2014,
	title = {Bayesian hierarchical spatio-temporal modelling of trends and future projections in the ocean wave climate with a {CO2} regression component},
	volume = {21},
	issn = {13528505},
	doi = {10.1007/s10651-013-0251-6},
	abstract = {Bad weather and rough seas continue to be a major cause for ship losses and is thus a significant contributor to the risk to maritime transportation. This stresses the importance of taking severe sea state conditions adequately into account, with due treatment of the uncertainties involved, in ship design and operation in order to enhance safety. Hence, there is a need for appropriate stochastic models describing the variability of sea states. These should also incorporate realistic projections of future return levels of extreme sea states, taking into account long-term trends related to climate change and inherent uncertainties. The stochastic ocean wave model presented in this paper exploits the flexible framework of Bayesian hierarchical space-time models. It allows modelling of complex dependence structures in space and time and incorporation of physical features and prior knowledge, yet at the same time remains intuitive and easily interpreted. Furthermore, by taking a Bayesian approach, the uncertainties of the model parameters are also taken into account. A regression component with CO2 as an explanatory variable has been introduced in order to extract long-term trends in the data. The model has been fitted by monthly maximum significant wave height data for an area in the North Atlantic ocean. The different components of the model will be outlined in the paper, and the results will be discussed. Furthermore, a discussion of possible extensions to the model will be given. © 2013 Springer Science+Business Media New York.},
	number = {2},
	journal = {Environmental and Ecological Statistics},
	author = {Vanem, Erik and Huseby, Arne Bang and Natvig, Bent},
	year = {2014},
	note = {Publisher: Kluwer Academic Publishers},
	keywords = {Stochastic processes, Bayesian hierarchical modelling, Climate change, MCMC, Modelling the effects of climate change, Ocean waves, Spatio-temporal modelling},
	pages = {189--220},
	file = {PDF:/home/miguel/Zotero/storage/ADJNB7LI/s10651-013-0251-6.pdf:application/pdf},
}

@article{rudnick_ocean_2016,
	title = {Ocean {Research} {Enabled} by {Underwater} {Gliders}},
	volume = {8},
	issn = {19410611},
	doi = {10.1146/annurev-marine-122414-033913},
	abstract = {Underwater gliders are autonomous underwater vehicles that profile vertically by changing their buoyancy and use wings to move horizontally. Gliders are useful for sustained observation at relatively fine horizontal scales, especially to connect the coastal and open ocean. In this review, research topics are grouped by time and length scales. Large-scale topics addressed include the eastern and western boundary currents and the regional effects of climate variability. The accessibility of horizontal length scales of order 1 km allows investigation of mesoscale and submesoscale features such as fronts and eddies. Because the submesoscales dominate vertical fluxes in the ocean, gliders have found application in studies of biogeochemical processes. At the finest scales, gliders have been used to measure internal waves and turbulent dissipation. The review summarizes gliders' achievements to date and assesses their future in ocean observation.},
	journal = {Annual Review of Marine Science},
	author = {Rudnick, Daniel L.},
	month = jan,
	year = {2016},
	pmid = {26291384},
	note = {Publisher: Annual Reviews Inc.},
	keywords = {Autonomous underwater vehicles, Biogeochemistry, Climate, Internal waves, Mesoscale, Sustained observations},
	pages = {519--541},
	file = {PDF:/home/miguel/Zotero/storage/CE9A4A75/Rudnick_2015.pdf:application/pdf},
}

@article{diamant_cross-sensor_2020-1,
	title = {Cross-sensor quality assurance for marine observatories},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12213470},
	abstract = {Measuring and forecasting changes in coastal and deep-water ecosystems and climates requires sustained long-term measurements from marine observation systems. One of the key considerations in analyzing data from marine observatories is quality assurance (QA). The data acquired by these infrastructures accumulates into Giga and Terabytes per year, necessitating an accurate automatic identification of false samples. A particular challenge in the QA of oceanographic datasets is the avoidance of disqualification of data samples that, while appearing as outliers, actually represent real short-term phenomena, that are of importance. In this paper, we present a novel cross-sensor QA approach that validates the disqualification decision of a data sample from an examined dataset by comparing it to samples from related datasets. This group of related datasets is chosen so as to reflect upon the same oceanographic phenomena that enable some prediction of the examined dataset. In our approach, a disqualification is validated if the detected anomaly is present only in the examined dataset, but not in its related datasets. Results for a surface water temperature dataset recorded by our Texas A\&M—Haifa Eastern Mediterranean Marine Observatory (THEMO)—over a period of 7 months, show an improved trade-off between accurate and false disqualification rates when compared to two standard benchmark schemes.},
	number = {21},
	journal = {Remote Sensing},
	author = {Diamant, Roee and Shachar, Ilan and Makovsky, Yizhaq and Ferreira, Bruno Miguel and Cruz, Nuno Alexandre},
	month = nov,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Regression, Change detector, Data validation, Ocean observatories, Ocean remote sensing, Prediction of data, Quality assurance, Quality control},
	pages = {1--16},
	file = {PDF:/home/miguel/Zotero/storage/9MJQ2DAD/Cross-sensor quality assessment.pdf:application/pdf},
}

@article{noauthor_analyzing_nodate,
	title = {Analyzing spatio-temporal data with {R}: {Everything} you always wanted to know-but were afraid to ask},
	issn = {2102-6238},
	url = {http://informatique-mia.inra.fr/resste/http://www.sfds.asso.fr/journal},
	abstract = {Titre: Donnees spatio-temporelles avec R : tout ce que vous avez toujours voulu savoir sans jamais avoir osé le demander RESSTE Network et al. 1,2 Abstract: We present an overview of (geo-)statistical models, methods and techniques for the analysis and prediction of continuous spatio-temporal processes residing in continuous space. Various approaches exist for building statistical models for such processes, estimating their parameters and performing predictions. We cover the Gaussian process approach, very common in spatial statistics and geostatistics, and we focus on R-based implementations of numerical procedures. To illustrate and compare the use of some of the most relevant packages, we treat a real-world application with high-dimensional data. The target variable is the daily mean PM 10 concentration predicted thanks to a chemistry-transport model and observation series collected at monitoring stations across France in 2014. We give R code covering the full work-flow from importing data sets to the prediction of PM 10 concentrations with a fitted parametric model, including the visualization of data, estimation of the parameters of the spatio-temporal covariance function and model selection. We conclude with some elements of comparison between the packages that are available today and some discussion for future developments. Résumé : Nous présentons un aperçu des modèles, méthodes et techniques (géo-)statistiques pour l'analyse et la prévision de processus spatio-temporels continus. De nombreuses approches sont possibles pour la construction de modèles statistiques pour ces processus, l'estimation de leurs paramètres et leur prédiction. Nous avons choisi de présenter l'approche par processus gaussien, la plus communément utilisée en statistiques spatiales et en géostatistiques, ainsi que son implémentation avec le logiciel R. La variable cible est la moyenne de la concentration quotidienne PM 10 à l'échelle de la France, prédite à l'aide d'un modèle de transport en chimie de l'atmosphère et de séries d'observations obtenues à des stations de surveillance de la qualité de l'air. En suivant le fil d'une application réelle de grande dimension, nous comparons certains des paquets R les plus utilisés. Le code R permettant la visualisation des données, l'estimation des paramètres de la fonction de covariance spatio-temporelle ainsi que la sélection d'un modèle et la prédiction de la concentration de PM 10 est également présenté afin d'illustrer l'enchaînement des étapes. Nous concluons avec une comparaison entre les paquets qui sont disponibles aujourd'hui et ainsi que les pistes de développement qui nous paraissent intéressantes.},
	keywords = {62-07, 62F99, 62M40, Air pollution Mots-clés : Fonction de covariance, Covariance function, Geostatistics, Géostatistique, Krigeage, Kriging, Pollution atmosphérique AMS 2000 subject classifications: 62-01, Space-time},
	file = {PDF:/home/miguel/Zotero/storage/MC3MD8FS/JSFS_2017__158_3_124_0.pdf:application/pdf},
}

@article{zaba_2014-2015_2016,
	title = {The 2014-2015 warming anomaly in the {Southern} {California} {Current} {System} observed by underwater gliders},
	volume = {43},
	issn = {19448007},
	doi = {10.1002/2015GL067550},
	abstract = {Large-scale patterns of positive temperature anomalies persisted throughout the surface waters of the North Pacific Ocean during 2014-2015. In the Southern California Current System, measurements by our sustained network of underwater gliders reveal the coastal effects of the recent warming. Regional upper ocean temperature anomalies were greatest since the initiation of the glider network in 2006. Additional observed physical anomalies included a depressed thermocline, high stratification, and freshening; induced biological consequences included changes in the vertical distribution of chlorophyll fluorescence. Contemporaneous surface heat flux and wind strength perturbations suggest that local anomalous atmospheric forcing caused the unusual oceanic conditions.},
	number = {3},
	journal = {Geophysical Research Letters},
	author = {Zaba, Katherine D. and Rudnick, Daniel L.},
	month = feb,
	year = {2016},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {California Current System, downwelling anomaly, Spray gliders, warm SST},
	pages = {1241--1248},
	file = {PDF:/home/miguel/Zotero/storage/5Z4LTHFD/Geophysical Research Letters - 2016 - Zaba - The 2014 2015 warming anomaly in the Southern California Current System.pdf:application/pdf},
}

@article{sekulic_spatio-temporal_2020,
	title = {Spatio-temporal regression kriging model of mean daily temperature for {Croatia}},
	volume = {140},
	issn = {14344483},
	doi = {10.1007/s00704-019-03077-3},
	abstract = {High resolution gridded mean daily temperature datasets are valuable for research and applications in agronomy, meteorology, hydrology, ecology, and many other disciplines depending on weather or climate. The gridded datasets and the models used for their estimation are being constantly improved as there is always a need for more accurate datasets as well as for datasets with a higher spatial and temporal resolution. We developed a spatio-temporal regression kriging model for Croatia at 1 km spatial resolution by adapting the spatio-temporal regression kriging model developed for global land areas. A geometrical temperature trend, digital elevation model, and topographic wetness index were used as covariates together with measurements from the Croatian national meteorological network for the year 2008. This model performed better than the global model and previously developed models for Croatia, based on MODIS land surface temperature images. The R2 was 97.8\% and RMSE was 1.2 °C for leave-one-out and 5-fold cross-validation. The proposed national model still has a high level of uncertainty at higher altitudes leaving it suitable for agricultural areas that are dominant in lower and medium altitudes.},
	number = {1-2},
	journal = {Theoretical and Applied Climatology},
	author = {Sekulić, Aleksandar and Kilibarda, Milan and Protić, Dragutin and Tadić, Melita Perčec and Bajat, Branislav},
	month = apr,
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Gridded data, Mean daily temperature, R meteo package, Spatio-temporal regression kriging},
	pages = {101--114},
	file = {PDF:/home/miguel/Zotero/storage/QHPBVSJ9/retrieve-4.pdf:application/pdf},
}

@article{wikle_modern_2013-1,
	title = {Modern statistical methods in oceanography: {A} hierarchical perspective},
	volume = {28},
	issn = {08834237},
	doi = {10.1214/13-STS436},
	abstract = {Processes in ocean physics, air-sea interaction and ocean biogeochemistry span enormous ranges in spatial and temporal scales, that is, from molecular to planetary and from seconds to millennia. Identifying and implementing sustainable human practices depend critically on our understandings of key aspects of ocean physics and ecology within these scale ranges. The set of all ocean data is distorted such that three- and four-dimensional (i.e., timedependent) in situ data are very sparse, while observations of surface and upper ocean properties from space-borne platforms have become abundant in the past few decades. Precisions in observations of all types vary as well. In the face of these challenges, the interface between Statistics and Oceanography has proven to be a fruitful area for research and the development of useful models. With the recognition of the key importance of identifying, quantifying and managing uncertainty in data and models of ocean processes, a hierarchical perspective has become increasingly productive. As examples, we review a heterogeneous mix of studies from our own work demonstrating Bayesian hierarchical model applications in ocean physics, air-sea interaction, ocean forecasting and ocean ecosystem models. This review is by no means exhaustive and we have endeavored to identify hierarchical modeling work reported by others across the broad range of ocean-related topics reported in the statistical literature. We conclude by noting relevant oceanstatistics problems on the immediate research horizon, and some technical challenges they pose, for example, in terms of nonlinearity, dimensionality and computing ©Institute of Mathematical Statistics, 2013.},
	number = {4},
	journal = {Statistical Science},
	author = {Wikle, Christopher K. and Milliff, Ralph F. and Herbei, Radu and Leeds, William B.},
	month = nov,
	year = {2013},
	keywords = {Bayesian, Biogeochemical, Ecosystem, Ocean vector winds, Quadratic nonlinearity, Sea surface temperature, Spatio-temporal, State-space},
	pages = {466--486},
	file = {PDF:/home/miguel/Zotero/storage/HCMUC5FH/13-STS436.pdf:application/pdf},
}

@techreport{noauthor_modeling_nodate-1,
	title = {Modeling {Spatio}-{Temporal} {Data}},
	abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
	file = {PDF:/home/miguel/Zotero/storage/9Q2NZ8A2/9781032623443_previewpdf.pdf:application/pdf},
}

@article{pizarro_underwater_2016,
	title = {Underwater glider observations in the oxygen minimum zone off central {Chile}},
	volume = {97},
	issn = {00030007},
	doi = {10.1175/BAMS-D-14-00040.1},
	number = {10},
	journal = {Bulletin of the American Meteorological Society},
	author = {Pizarro, Oscar and Ramirez, Nadin and Castillo, Manuel I. and Cifuentes, Ursula and Rojas, Winston and Pizarro-Koch, Matias},
	month = oct,
	year = {2016},
	note = {Publisher: American Meteorological Society},
	pages = {1783--1789},
	file = {PDF:/home/miguel/Zotero/storage/ZS6RMELM/bams-bams-d-14-00040.1.pdf:application/pdf},
}

@article{chakraborty_modelling_2015,
	title = {Modelling and analysis of spatio-temporal dynamics of a marine ecosystem},
	volume = {81},
	issn = {1573269X},
	doi = {10.1007/s11071-015-2114-1},
	abstract = {This paper examines the spatio-temporal dynamics of a marine ecosystem. The system is described by two reaction–diffusion equations. We consider a phytoplankton–zooplankton system with Ivlev-type grazing function. The dynamics of the reaction–diffusion system of phytoplankton–zooplankton interaction has been studied with both constant and variable diffusion coefficients. Periodic oscillations of the phytoplankton and zooplankton populations are shown with constant and variable diffusion coefficients. In order to obtain spatio-temporal patterns, we perform numerical simulations of the coupled system describing phytoplankton–zooplankton dynamics in the presence of diffusive forces. We explain how the concentration of species changes due to local reactions and diffusion. Our results suggest that patchiness is one of the basic characteristics of the functioning of an ecological system. Two-dimensional spatial patterns of phytoplankton–zooplankton dynamics are self-organized and, therefore, can be considered to provide a theoretical framework to understand patchiness in marine environments.},
	number = {4},
	journal = {Nonlinear Dynamics},
	author = {Chakraborty, Kunal and Manthena, Vamsi},
	month = sep,
	year = {2015},
	note = {Publisher: Kluwer Academic Publishers},
	keywords = {Diffusion-driven instability, Marine ecosystem, Patchiness, Reaction–diffusion equations, Spatio-temporal dynamics},
	pages = {1895--1906},
	file = {PDF:/home/miguel/Zotero/storage/H4YC77TS/s11071-015-2114-1-2.pdf:application/pdf},
}

@book{noauthor_2013_2014,
	title = {2013 {OCEANS}-{San} {Diego} : 23-27 {September} 2013},
	isbn = {978-0-933957-40-4},
	publisher = {IEEE},
	year = {2014},
	file = {PDF:/home/miguel/Zotero/storage/HJPRYLVX/Monitoring_Dissolved_Oxygen_in_New_Jersey_coastal_waters_using_autonomous_gliders_Multi-year_trends_and_event_response.pdf:application/pdf},
}

@article{zarokanellos_frontal_2022,
	title = {Frontal {Dynamics} in the {Alboran} {Sea}: 1. {Coherent} {3D} {Pathways} at the {Almeria}-{Oran} {Front} {Using} {Underwater} {Glider} {Observations}},
	volume = {127},
	issn = {21699291},
	doi = {10.1029/2021JC017405},
	abstract = {Ocean fronts are areas that can support phytoplankton production through fertilization in the sunlit layer and the subduction of biogeochemical properties from the surface to the interior of the ocean. The Almeria-Oran (AO) front is formed from the juxtaposition of fresh inflowing Atlantic waters and more saline re-circulating Mediterranean waters. A fleet of three gliders flying in parallel lines was deployed across the AO to obtain observations in the CALYPSO project. These observations were combined with remote sensing and modeling simulations, thus providing a novel approach to identifying the three-dimensional transport and the submesoscale across-front circulation. The resulting 33 cross-front sections reveal spatial and temporal changes in the frontal boundary, with isopycnals steepening and/or relaxing. The observations revealed strong horizontal density gradients (up to ∼1.4 kg m−3) and the spatial variability was observed over different length scales (∼10–45 km). The potential vorticity decreased across the front due to the vorticity component in the horizontal density gradient direction. The predominant cyclonic relative vorticity on the dense side of the AO is associated with downwelling processes. The biogeochemical observations also suggest vertical transport along coherent pathways through baroclinic instability. Phytoplankton biomass enhancement occurs as a result, and is subducted below the euphotic layer. The observed oxygen filaments show upwelling and downwelling, providing a mechanism for oxygenating deeper layers and reducing the ventilation of deep low-oxygenated waters. Understanding the mechanisms of vertical transport can help us evaluate the dynamics of ocean fronts and their impacts on biological carbon storage.},
	number = {3},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Zarokanellos, Nikolaos D. and Rudnick, Daniel L. and Garcia-Jove, Maximo and Mourre, Baptiste and Ruiz, Simon and Pascual, Ananda and Tintoré, Joaquin},
	month = mar,
	year = {2022},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Almeria-Oran front, frontal dynamics, Instability processes, phytoplankton and oxygen distribution, submesoscale filaments, vertical motions},
	file = {PDF:/home/miguel/Zotero/storage/L4WE3XJV/JGR Oceans - 2022 - Zarokanellos - Frontal Dynamics in the Alboran Sea  1  Coherent 3D Pathways at the Almeria%E2%80%90Oran Front.pdf:application/pdf},
}

@book{noauthor_oceans_2006,
	title = {{OCEANS} 2006},
	isbn = {1-4244-0115-1},
	abstract = {Title from content provider.},
	publisher = {[publisher not identified]},
	year = {2006},
	file = {PDF:/home/miguel/Zotero/storage/7BVC63EA/Studying_the_Dynamics_and_Biological_Significance_of_the_Hudson_River_Using_an_Ocean_Observatory.pdf:application/pdf},
}

@article{saba_development_2019,
	title = {The {Development} and {Validation} of a {Profiling} {Glider} {Deep} {ISFET}-{Based} {pH} {Sensor} for {High} {Resolution} {Observations} of {Coastal} and {Ocean} {Acidification}},
	volume = {6},
	issn = {22967745},
	doi = {10.3389/fmars.2019.00664},
	abstract = {Coastal and ocean acidification can alter ocean biogeochemistry, with ecological consequences that may result in economic and cultural losses. Yet few time series and high resolution spatial and temporal measurements exist to track the existence and movement of water low in pH and/or carbonate saturation. Past acidification monitoring efforts have either low spatial resolution (mooring) or high cost and low temporal and spatial resolution (research cruises). We developed the first integrated glider platform and sensor system for sampling pH throughout the water column of the coastal ocean. A deep ISFET (Ion Sensitive Field Effect Transistor)-based pH sensor system was modified and integrated into a Slocum glider, tank tested in natural seawater to determine sensor conditioning time under different scenarios, and validated in situ during deployments in the U.S. Northeast Shelf (NES). Comparative results between glider pH and pH measured spectrophotometrically from discrete seawater samples indicate that the glider pH sensor is capable of accuracy of 0.011 pH units or better for several weeks throughout the water column in the coastal ocean, with a precision of 0.005 pH units or better. Furthermore, simultaneous measurements from multiple sensors on the same glider enabled salinity-based estimates of total alkalinity (AT) and aragonite saturation state (ΩArag). During the Spring 2018 Mid-Atlantic deployment, glider pH and derived AT/ΩArag data along the cross-shelf transect revealed higher pH and ΩArag associated with the depth of chlorophyll and oxygen maxima and a warmer, saltier water mass. Lowest pH and ΩArag occurred in bottom waters of the middle shelf and slope, and nearshore following a period of heavy precipitation. Biofouling was revealed to be the primary limitation of this sensor during a summer deployment, whereby offsets in pH and AT increased dramatically. Advances in anti-fouling coatings and the ability to routinely clean and swap out sensors can address this challenge. The data presented here demonstrate the ability for gliders to routinely provide high resolution water column data on regional scales that can be applied to acidification monitoring efforts in other coastal regions.},
	journal = {Frontiers in Marine Science},
	author = {Saba, Grace K. and Wright-Fairbanks, Elizabeth and Chen, Baoshan and Cai, Wei Jun and Barnard, Andrew H. and Jones, Clayton P. and Branham, Charles W. and Wang, Kui and Miles, Travis},
	month = oct,
	year = {2019},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {glider, Mid-Atlantic, monitoring, ocean acidification, pH, U.S. Northeast Shelf},
	file = {PDF:/home/miguel/Zotero/storage/XSB6ZUTG/fmars-06-00664.pdf:application/pdf},
}

@article{burke_temporal_2023,
	title = {Temporal and spatial variability in hydrography and dissolved oxygen along southwest {Nova} {Scotia} using glider observations},
	volume = {254},
	issn = {18736955},
	doi = {10.1016/j.csr.2022.104908},
	abstract = {Dissolved oxygen (DO) in the global ocean is on the decline, resulting in the degradation of coastal habitats. As aquaculture production occurs in these regions, proper understanding of coastal DO dynamics is important for improved farm management (e.g. site selection). The main objective of this study was to quantify along-shore and cross-shore variability in DO dynamics, as well as onshore advection of offshore waters to the bays that could contain aquaculture farms. For that purpose, a Slocum underwater glider was deployed between September 25 and October 12, 2020 to collect high-resolution data of temperature, salinity and DO along a transect between Shelburne Bay and St. Margarets Bay, Nova Scotia, Canada, with an average distance of about 15 km from shore. The observations revealed the variable nature of cross-shore water properties. Shoreward bottom currents transport offshore waters to the coast, and combined with longer residence times, mixing of these waters with those present created a mosaic of differing water properties, with warmer, fresher, and less oxygenated shoreward waters. The glider intercepted an upwelling event due to strong and persistent southwesterly winds, which cooled the upper water layers by 6 °C and increased DO by 1.4 mg L−1. This strong upwelling event detected at 10 km from the coast was also captured 30 h later within St. Margarets Bay, depicting a potential offshore-inshore interaction. Therefore, bay-wide ecosystems and aquaculture production could be affected by intrusions of offshore waters.},
	journal = {Continental Shelf Research},
	author = {Burke, Meredith and Grant, Jon and Filgueira, Ramon and Sheng, Jinyu},
	month = feb,
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Along-/cross-shelf variability, Aquaculture, Dissolved oxygen, Scotian shelf, Slocum glider, Temperature and salinity},
	file = {PDF:/home/miguel/Zotero/storage/XUGJR69C/Burke 2023.pdf:application/pdf},
}

@article{wikle_annual_2025,
	title = {Annual {Review} of {Statistics} and {Its} {Application} {Statistical} {Deep} {Learning} for {Spatial} and {Spatiotemporal} {Data}},
	volume = {14},
	url = {https://doi.org/10.1146/annurev-statistics-033021-},
	doi = {10.1146/annurev-statistics-033021},
	abstract = {Deep neural network models have become ubiquitous in recent years and have been applied to nearly all areas of science, engineering, and industry. These models are particularly useful for data that have strong dependencies in space (e.g., images) and time (e.g., sequences). Indeed, deep models have also been extensively used by the statistical community to model spatial and spatiotemporal data through, for example, the use of multilevel Bayesian hierarchical models and deep Gaussian processes. In this review, we first present an overview of traditional statistical and machine learning perspectives for modeling spatial and spatiotemporal data, and then focus on a variety of hybrid models that have recently been developed for latent process, data, and parameter specifications. These hybrid models integrate statistical modeling ideas with deep neural network models in order to take advantage of the strengths of each modeling paradigm. We conclude by giving an overview of computational technologies that have proven useful for these hybrid models, and with a brief discussion on future research directions.},
	author = {Wikle, Christopher K and Zammit-Mangion, Andrew},
	year = {2025},
	keywords = {Bayesian hierarchical models, convolutional neural networks, deep Gaussian processes, recurrent neural networks, reinforcement learning, warping},
	pages = {5},
	file = {PDF:/home/miguel/Zotero/storage/R4KCIM5Y/annurev-statistics-033021-112628-2.pdf:application/pdf},
}

@techreport{sahu_bayesian_nodate,
	title = {Bayesian {Modeling} of {Spatio}-{Temporal} {Data} with {R}},
	url = {https://www.crcpress.com/Chapman--},
	author = {Sahu, Sujit K},
	keywords = {bayesian, modelling, spatio-temporal},
	file = {PDF:/home/miguel/Zotero/storage/QGN2GGYT/Bayesian Modeling of Spatio-Temporal Data with R_25_04_13_18_22_52.pdf:application/pdf},
}

@article{yang_general_2023,
	title = {General {Anomaly} {Detection} of {Underwater} {Gliders} {Validated} by {Large}-scale {Deployment} {Datasets}},
	url = {http://arxiv.org/abs/2308.00180},
	abstract = {Underwater gliders have been widely used in oceanography for a range of applications. However, unpredictable events like shark strikes or remora attachments can lead to abnormal glider behavior or even loss of the instrument. This paper employs an anomaly detection algorithm to assess operational conditions of underwater gliders in the real-world ocean environment. Prompt alerts are provided to glider pilots upon detecting any anomaly, so that they can take control of the glider to prevent further harm. The detection algorithm is applied to multiple datasets collected in real glider deployments led by the University of Georgia's Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). In order to demonstrate the algorithm generality, the experimental evaluation is applied to four glider deployment datasets, each highlighting various anomalies happening in different scenes. Specifically, we utilize high resolution datasets only available post-recovery to perform detailed analysis of the anomaly and compare it with pilot logs. Additionally, we simulate the online detection based on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery one, the online detection is of great importance as it allows glider pilots to monitor potential abnormal conditions in real time.},
	author = {Yang, Ruochu and Lembke, Chad and Zhang, Fumin and Edwards, Catherine},
	month = jul,
	year = {2023},
	note = {arXiv: 2308.00180},
	file = {PDF:/home/miguel/Zotero/storage/KLFG4D8F/2308.00180v3.pdf:application/pdf},
}

@techreport{grant_ingram_notes_1983,
	title = {Notes and {Discussions}},
	author = {Grant Ingram, R},
	year = {1983},
	note = {Publication Title: Estuarine, Coastal and Shelf Science
Volume: 16},
	keywords = {estuaries, internal tides, mixings, upwelling},
	pages = {333--338},
	file = {PDF:/home/miguel/Zotero/storage/SC6A4U46/1-s2.0-0272771483901506-main.pdf:application/pdf},
}

@article{jutras_temporal_2020,
	title = {Temporal {Changes} in the {Causes} of the {Observed} {Oxygen} {Decline} in the {St}. {Lawrence} {Estuary}},
	volume = {125},
	issn = {21699291},
	doi = {10.1029/2020JC016577},
	abstract = {Oxygen concentrations in the deep waters of the Lower St. Lawrence Estuary have decreased by 50\% over the past century. The drivers of this decrease are investigated by applying an extended Optimum Multiparameter analysis to a time series of physical and biogeochemical observations of the St. Lawrence Estuarine System in the 1970s and from late 1990s to 2018. This method reconstructs the relative contributions of the two major water masses feeding the system, the Labrador Current Waters (LCW) and the North Atlantic Central Waters (NACW), as well as oxygen utilization, and accounts for diapycnal mixing. The causes of the oxygen decline varied over the last 5 decades. Between the 1970s and late 1990s, the decrease was mainly driven by biogeochemical changes through an increase in microbial oxygen utilization in the St. Lawrence Estuary in response to warmer temperatures and eutrophication and lower oxygen concentrations in LCW and NACW. Between 2008 and 2018, the decrease was mainly driven by circulation changes in the western North Atlantic associated with a reduced inflow of high-oxygenated LCW to the deep waters of the system in favor of low-oxygenated NACW, reaching a historical minimum in 2016. The LCW:NACW ratio is strongly correlated with the volume transport of the Scotian shelf-break current, an extension of the Labrador Current. These results highlight the primary role of the Labrador Current in determining the oxygen concentration and other water properties of the St. Lawrence Estuarine System and on the western North Atlantic continental shelf and slope.},
	number = {12},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Jutras, M. and Dufour, C. O. and Mucci, A. and Cyr, F. and Gilbert, D.},
	month = dec,
	year = {2020},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {deoxygenation, eutrophication, Labrador Current, oxygen, slope waters, St. Lawrence Estuary},
	file = {PDF:/home/miguel/Zotero/storage/84SJXD5K/JGR Oceans - 2020 - Jutras - Temporal Changes in the Causes of the Observed Oxygen Decline in the St  Lawrence Estuary.pdf:application/pdf},
}

@techreport{rue_approximate_2009,
	title = {Approximate {Bayesian} inference for latent {Gaussian} models by using integrated nested {Laplace} approximations},
	abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (gener-alized) additive models, smoothing spline models, state space models, semiparametric regression , spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of struc-tured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality , which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	year = {2009},
	note = {Publication Title: J. R. Statist. Soc. B
Volume: 71
Issue: 2},
	keywords = {Approximate Bayesian inference, Gaussian Markov random fields, Generalized additive mixed models, Laplace approximation, Parallel computing, Sparse matrices, Structured additive regression models},
	pages = {319--392},
	file = {PDF:/home/miguel/Zotero/storage/5YLT4VDH/Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2009 - Rue - Approximate Bayesian inference.pdf:application/pdf},
}

@article{diamant_cross-sensor_2020-2,
	title = {Cross-sensor quality assurance for marine observatories},
	volume = {12},
	issn = {20724292},
	doi = {10.3390/rs12213470},
	abstract = {Measuring and forecasting changes in coastal and deep-water ecosystems and climates requires sustained long-term measurements from marine observation systems. One of the key considerations in analyzing data from marine observatories is quality assurance (QA). The data acquired by these infrastructures accumulates into Giga and Terabytes per year, necessitating an accurate automatic identification of false samples. A particular challenge in the QA of oceanographic datasets is the avoidance of disqualification of data samples that, while appearing as outliers, actually represent real short-term phenomena, that are of importance. In this paper, we present a novel cross-sensor QA approach that validates the disqualification decision of a data sample from an examined dataset by comparing it to samples from related datasets. This group of related datasets is chosen so as to reflect upon the same oceanographic phenomena that enable some prediction of the examined dataset. In our approach, a disqualification is validated if the detected anomaly is present only in the examined dataset, but not in its related datasets. Results for a surface water temperature dataset recorded by our Texas A\&M—Haifa Eastern Mediterranean Marine Observatory (THEMO)—over a period of 7 months, show an improved trade-off between accurate and false disqualification rates when compared to two standard benchmark schemes.},
	number = {21},
	journal = {Remote Sensing},
	author = {Diamant, Roee and Shachar, Ilan and Makovsky, Yizhaq and Ferreira, Bruno Miguel and Cruz, Nuno Alexandre},
	month = nov,
	year = {2020},
	note = {Publisher: MDPI AG},
	keywords = {Regression, Change detector, Data validation, Ocean observatories, Ocean remote sensing, Prediction of data, Quality assurance, Quality control},
	pages = {1--16},
	file = {PDF:/home/miguel/Zotero/storage/VHIHR5EG/Cross-sensor quality assessment.pdf:application/pdf},
}

@article{wikle_modern_2015-1,
	title = {Modern perspectives on statistics for spatio-temporal data},
	volume = {7},
	issn = {19390068},
	doi = {10.1002/wics.1341},
	abstract = {Spatio-temporal statistical models are increasingly being used across a wide variety of scientific disciplines to describe and predict spatially explicit processes that evolve over time. Although descriptive models that approach this problem from the second-order (covariance) perspective are important, many real-world processes are dynamic, and it can be more efficient in such cases to characterize the associated spatio-temporal dependence by the use of dynamical models. The challenge with the specification of such dynamical models has been related to the curse of dimensionality and the specification of realistic dependence structures. Even in fairly simple linear/Gaussian settings, spatio-temporal statistical models are often over parameterized. This problem is compounded when the spatio-temporal dynamical processes are nonlinear or multivariate. Hierarchical models have proven invaluable in their ability to deal to some extent with this issue by allowing dependency among groups of parameters and science-based parameterizations. Such models are best considered from a Bayesian perspective, with associated computational challenges. Spatio-temporal statistics remains an active and vibrant area of research.},
	number = {1},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Wikle, Christopher K.},
	month = jan,
	year = {2015},
	note = {Publisher: Wiley-Blackwell},
	keywords = {Quadratic nonlinearity, Bayesian hierarchical models, Rank reduction, Spatial basis functions, Spatio-temporal dynamic models},
	pages = {86--98},
	file = {PDF:/home/miguel/Zotero/storage/GVU59XK4/WIREs Computational Stats - 2014 - Wikle - Modern perspectives on statistics for spatio%E2%80%90temporal data.pdf:application/pdf},
}

@techreport{graler_spcopula_nodate-1,
	title = {spcopula: {Modelling} {Spatial} and {Spatio}-{Temporal} {Dependence} with {Copulas} in {R}},
	abstract = {The spcopula R package provides tools to model spatial and spatio-temporal phenomena with spatial and spatio-temporal vine copulas. Copulas allow us to flexibly build multivariate distributions with mixed margins where the copula describes the multivari-ate dependence structure coupling the margins. In classical geostatistics, a multivariate Gaussian distribution is typically assumed and dependence is summarized in a covari-ance matrix implying limitations like elliptical symmetry in the strength of dependence. Copulas allow for dependence structures beyond the Gaussian one, being for instance asymmetric. We developed the spatio-temporal vine copulas such that the bivariate cop-ula families in the lower trees may change with distance across space and time allowing not only for a varying strength of dependence but also for a changing dependence structure. These spatio-temporal distributions are used to predict values at unobserved locations, assess risk, or run simulations. Based on the concept of vine copulas, the spcopula package provides a large set of multivariate distributions. As bivariate spatial copulas do not have any probabilistic restrictions, the spatial vine copula is a powerful approach for modelling skewed or heavy tailed data with complex and potentially asymmetric dependence structures in the spatial and spatio-temporal domain.},
	author = {Gräler, Benedikt},
	keywords = {interpolation, multivariate distributions, spatial data, spatial modelling},
	file = {PDF:/home/miguel/Zotero/storage/HW5M9A2I/spcopula.pdf:application/pdf},
}

@techreport{noauthor_modeling_nodate-2,
	title = {Modeling {Spatio}-{Temporal} {Data}},
	abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
	file = {PDF:/home/miguel/Zotero/storage/PMFWYQFT/9781032623443_previewpdf.pdf:application/pdf},
}

@article{cameletti_spatio-temporal_2013,
	title = {Spatio-temporal modeling of particulate matter concentration through the {SPDE} approach},
	volume = {97},
	issn = {18638171},
	doi = {10.1007/s10182-012-0196-3},
	abstract = {In this work, we consider a hierarchical spatio-temporal model for particulate matter (PM) concentration in the North-Italian region Piemonte. The model involves a Gaussian Field (GF), affected by a measurement error, and a state process characterized by a first order autoregressive dynamic model and spatially correlated innovations. This kind of model is well discussed and widely used in the air quality literature thanks to its flexibility in modelling the effect of relevant covariates (i. e. meteorological and geographical variables) as well as time and space dependence. However, Bayesian inference-through Markov chain Monte Carlo (MCMC) techniques-can be a challenge due to convergence problems and heavy computational loads. In particular, the computational issue refers to the infeasibility of linear algebra operations involving the big dense covariance matrices which occur when large spatio-temporal datasets are present. The main goal of this work is to present an effective estimating and spatial prediction strategy for the considered spatio-temporal model. This proposal consists in representing a GF with Matérn covariance function as a Gaussian Markov Random Field (GMRF) through the Stochastic Partial Differential Equations (SPDE) approach. The main advantage of moving from a GF to a GMRF stems from the good computational properties that the latter enjoys. In fact, GMRFs are defined by sparse matrices that allow for computationally effective numerical methods. Moreover, when dealing with Bayesian inference for GMRFs, it is possible to adopt the Integrated Nested Laplace Approximation (INLA) algorithm as an alternative to MCMC methods giving rise to additional computational advantages. The implementation of the SPDE approach through the R-library INLA (www.r-inla.org) is illustrated with reference to the Piemonte PM data. In particular, providing the step-by-step R-code, we show how it is easy to get prediction and probability of exceedance maps in a reasonable computing time. © 2012 Springer-Verlag.},
	number = {2},
	journal = {AStA Advances in Statistical Analysis},
	author = {Cameletti, Michela and Lindgren, Finn and Simpson, Daniel and Rue, Håvard},
	month = apr,
	year = {2013},
	note = {Publisher: Springer Verlag},
	keywords = {Gaussian Markov random fields, Covariance functions, Gaussian fields, Hierarchical models, Integrated Nested Laplace Approximation},
	pages = {109--131},
	file = {PDF:/home/miguel/Zotero/storage/PGQI43PW/s10182-012-0196-3.pdf:application/pdf},
}

@techreport{lindgren_explicit_2011,
	title = {An explicit link between {Gaussian} fields and {Gaussian} {Markov} random fields: the stochastic partial differential equation approach},
	abstract = {Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gauss-ian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in R 2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link , for any triangulation of R d , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
	author = {Lindgren, Finn and Rue, Håvard and Lindström, Johan},
	year = {2011},
	note = {Publication Title: J. R. Statist. Soc. B
Volume: 73},
	keywords = {Approximate Bayesian inference, Gaussian Markov random fields, Sparse matrices, Covariance functions, Gaussian fields, Latent Gaussian models, Stochastic partial differential equations},
	pages = {423--498},
	file = {PDF:/home/miguel/Zotero/storage/SLVHHE7U/Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2011 - Lindgren - An explicit link between.pdf:application/pdf},
}

@article{law_predicting_2008-1,
	title = {Predicting and monitoring the effects of large-scale ocean iron fertilization on marine trace gas emissions},
	volume = {364},
	issn = {01718630},
	doi = {10.3354/meps07549},
	abstract = {Large-scale ({\textgreater}40 000 km2, {\textgreater}1 yr) ocean iron fertilization (OIF) is being considered as an option for mitigating the increase in atmospheric CO2 concentrations. However OIF will influence trace gas production and atmospheric emissions, with consequences over broad temporal and spatial scales. To illustrate this, the response of nitrous oxide (N 2O) and dimethylsulphide (DMS) in the mesoscale iron addition experiments (FeAXs) and model scenarios of large-scale OIF are examined. FeAXs have shown negligible to minor increases in N2O production, whereas models of long-term OIF suggest significant N2O production with the potential to offset the benefit gained by iron-mediated increases in CO 2 uptake. N2O production and emission will be influenced by the magnitude and rate of vertical particle export, and along-isopycnal N2O transport will necessitate monitoring over large spatial scales. The N2O-O2 relationship provides a monitoring option using oxygen as a proxy, with spatial coverage by Argo and glider-mounted oxygen optodes. Although the initial FeAXs exhibited similar increases (1.5- to 1.6-fold) in DMS, a subsequent sub-arctic Pacific experiment observed DMS consumption relative to unfertilized waters, highlighting regional variability as a complicating factor when predicting the effects of large-scale OIF. DMS cycling and its influence on atmospheric composition may be studied using naturally occurring blooms and be constrained prior to OIF by pre-fertilization spatial mapping and aerial sampling using new technologies. As trace gases may have positive or negative synergistic effects on atmospheric chemistry and climate forcing, the net effect of altered trace gas emissions needs to be considered in both models and monitoring of large-scale OIF. © Inter-Research 2008.},
	journal = {Marine Ecology Progress Series},
	author = {Law, C. S.},
	month = jul,
	year = {2008},
	keywords = {Dimethlysulphide, Iron fertilization, Nitrous oxide, Remineralization, Trace gases},
	pages = {283--288},
	file = {PDF:/home/miguel/Zotero/storage/RTY6EC2Z/m364p283.pdf:application/pdf},
}

@inproceedings{li_ocean_2023-2,
	title = {Ocean {Data} {Quality} {Assessment} through {Outlier} {Detection}-enhanced {Active} {Learning}},
	isbn = {979-8-3503-2445-7},
	doi = {10.1109/BigData59044.2023.10386969},
	abstract = {Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an Outlier Detection-Enhanced Active Learning (ODEAL) framework for ocean data quality assessment, employing Active Learning (AL) to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5\% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9\% using the initial set built with outlier detectors.},
	booktitle = {Proceedings - 2023 {IEEE} {International} {Conference} on {Big} {Data}, {BigData} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Li, Na and Qi, Yiyang and Xin, Ruyue and Zhao, Zhiming},
	year = {2023},
	keywords = {machine learning, active learning, Argo, initial set construction, ocean data quality control},
	pages = {102--107},
	file = {PDF:/home/miguel/Zotero/storage/24YQSCV2/Ocean_Data_Quality_Assessment_through_Outlier_Detection-enhanced_Active_Learning.pdf:application/pdf},
}

@article{sekulic_spatio-temporal_2020-1,
	title = {Spatio-temporal regression kriging model of mean daily temperature for {Croatia}},
	volume = {140},
	issn = {14344483},
	doi = {10.1007/s00704-019-03077-3},
	abstract = {High resolution gridded mean daily temperature datasets are valuable for research and applications in agronomy, meteorology, hydrology, ecology, and many other disciplines depending on weather or climate. The gridded datasets and the models used for their estimation are being constantly improved as there is always a need for more accurate datasets as well as for datasets with a higher spatial and temporal resolution. We developed a spatio-temporal regression kriging model for Croatia at 1 km spatial resolution by adapting the spatio-temporal regression kriging model developed for global land areas. A geometrical temperature trend, digital elevation model, and topographic wetness index were used as covariates together with measurements from the Croatian national meteorological network for the year 2008. This model performed better than the global model and previously developed models for Croatia, based on MODIS land surface temperature images. The R2 was 97.8\% and RMSE was 1.2 °C for leave-one-out and 5-fold cross-validation. The proposed national model still has a high level of uncertainty at higher altitudes leaving it suitable for agricultural areas that are dominant in lower and medium altitudes.},
	number = {1-2},
	journal = {Theoretical and Applied Climatology},
	author = {Sekulić, Aleksandar and Kilibarda, Milan and Protić, Dragutin and Tadić, Melita Perčec and Bajat, Branislav},
	month = apr,
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Gridded data, Mean daily temperature, R meteo package, Spatio-temporal regression kriging},
	pages = {101--114},
	file = {PDF:/home/miguel/Zotero/storage/SHGQYGFN/retrieve-4.pdf:application/pdf},
}

@article{noauthor_analyzing_nodate-1,
	title = {Analyzing spatio-temporal data with {R}: {Everything} you always wanted to know-but were afraid to ask},
	issn = {2102-6238},
	url = {http://informatique-mia.inra.fr/resste/http://www.sfds.asso.fr/journal},
	abstract = {Titre: Donnees spatio-temporelles avec R : tout ce que vous avez toujours voulu savoir sans jamais avoir osé le demander RESSTE Network et al. 1,2 Abstract: We present an overview of (geo-)statistical models, methods and techniques for the analysis and prediction of continuous spatio-temporal processes residing in continuous space. Various approaches exist for building statistical models for such processes, estimating their parameters and performing predictions. We cover the Gaussian process approach, very common in spatial statistics and geostatistics, and we focus on R-based implementations of numerical procedures. To illustrate and compare the use of some of the most relevant packages, we treat a real-world application with high-dimensional data. The target variable is the daily mean PM 10 concentration predicted thanks to a chemistry-transport model and observation series collected at monitoring stations across France in 2014. We give R code covering the full work-flow from importing data sets to the prediction of PM 10 concentrations with a fitted parametric model, including the visualization of data, estimation of the parameters of the spatio-temporal covariance function and model selection. We conclude with some elements of comparison between the packages that are available today and some discussion for future developments. Résumé : Nous présentons un aperçu des modèles, méthodes et techniques (géo-)statistiques pour l'analyse et la prévision de processus spatio-temporels continus. De nombreuses approches sont possibles pour la construction de modèles statistiques pour ces processus, l'estimation de leurs paramètres et leur prédiction. Nous avons choisi de présenter l'approche par processus gaussien, la plus communément utilisée en statistiques spatiales et en géostatistiques, ainsi que son implémentation avec le logiciel R. La variable cible est la moyenne de la concentration quotidienne PM 10 à l'échelle de la France, prédite à l'aide d'un modèle de transport en chimie de l'atmosphère et de séries d'observations obtenues à des stations de surveillance de la qualité de l'air. En suivant le fil d'une application réelle de grande dimension, nous comparons certains des paquets R les plus utilisés. Le code R permettant la visualisation des données, l'estimation des paramètres de la fonction de covariance spatio-temporelle ainsi que la sélection d'un modèle et la prédiction de la concentration de PM 10 est également présenté afin d'illustrer l'enchaînement des étapes. Nous concluons avec une comparaison entre les paquets qui sont disponibles aujourd'hui et ainsi que les pistes de développement qui nous paraissent intéressantes.},
	keywords = {62-07, 62F99, 62M40, Air pollution Mots-clés : Fonction de covariance, Covariance function, Geostatistics, Géostatistique, Krigeage, Kriging, Pollution atmosphérique AMS 2000 subject classifications: 62-01, Space-time},
	file = {PDF:/home/miguel/Zotero/storage/2QIMM8KR/JSFS_2017__158_3_124_0.pdf:application/pdf},
}

@article{baxevani_spatio-temporal_2009,
	title = {Spatio-temporal statistical modelling of significant wave height},
	volume = {20},
	issn = {11804009},
	doi = {10.1002/env.908},
	abstract = {In this paper, we construct a homogeneous spatio-temporal model to describe the variability of significant wave height over small regions of the sea and over short periods of time. Then, the model is extended to a nonhomogeneous one that is valid over larger areas of the sea and for time periods of up to 10 h. To validate the proposed model, we reconstruct the significant wave height surface under different scenarios and then compare it to satellite measurements and the C-ERA-40 field. Copyright © 2008 John Wiley \& Sons, Ltd.},
	number = {1},
	journal = {Environmetrics},
	author = {Baxevani, A. and Caires, S. and Rychlik, I.},
	month = feb,
	year = {2009},
	keywords = {Gaussian random fields, Random surface, Satellite data, Significant wave height, Stationary},
	pages = {14--31},
	file = {PDF:/home/miguel/Zotero/storage/ZA3RVT73/Environmetrics - 2008 - Baxevani - Spatio%E2%80%90temporal statistical modelling of significant wave height.pdf:application/pdf},
}

@techreport{graler_copulas_nodate-1,
	title = {Copulas, a novel approach to model spatial and spatio-temporal dependence},
	abstract = {Copulas are a statistical concept which allows for a novel approach to model dependencies of spatial and spatio-temporal variables. They capture the dependence structure of a multivariate distribution over its whole range detached from its specific margins. In contrast to a single measure of association, this allows for varying strength of dependence throughout the multivariate distribution. Thus, copulas are capable of capturing many different (i.e. non-Gaussian) dependence structures and allow for asymmet-ric dependencies which can be found in many natural processes. We applied this approach to data from the deforestation survey of the Brazilian Amazon in order to capture the dependence of deforestation on a selection of variables .},
	author = {Gräler, Benedikt and Kazianka, Hannes and Mira De Espindola, Giovana},
	file = {PDF:/home/miguel/Zotero/storage/PLGLZUAD/GEOChange_GraelerKaziankaDeEspindola.pdf:application/pdf},
}

@techreport{grant_ingram_notes_1983-1,
	title = {Notes and {Discussions}},
	author = {Grant Ingram, R},
	year = {1983},
	note = {Publication Title: Estuarine, Coastal and Shelf Science
Volume: 16},
	keywords = {estuaries, internal tides, mixings, upwelling},
	pages = {333--338},
	file = {PDF:/home/miguel/Zotero/storage/ZJVIGDZ7/1-s2.0-0272771483901506-main.pdf:application/pdf},
}

@article{tang_copula-based_2019-1,
	title = {Copula-based semiparametric models for spatiotemporal data},
	volume = {75},
	issn = {15410420},
	doi = {10.1111/biom.13066},
	abstract = {The joint analysis of spatial and temporal processes poses computational challenges due to the data's high dimensionality. Furthermore, such data are commonly non-Gaussian. In this paper, we introduce a copula-based spatiotemporal model for analyzing spatiotemporal data and propose a semiparametric estimator. The proposed algorithm is computationally simple, since it models the marginal distribution and the spatiotemporal dependence separately. Instead of assuming a parametric distribution, the proposed method models the marginal distributions nonparametrically and thus offers more flexibility. The method also provides a convenient way to construct both point and interval predictions at new times and locations, based on the estimated conditional quantiles. Through a simulation study and an analysis of wind speeds observed along the border between Oregon and Washington, we show that our method produces more accurate point and interval predictions for skewed data than those based on normality assumptions.},
	number = {4},
	journal = {Biometrics},
	author = {Tang, Yanlin and Wang, Huixia J. and Sun, Ying and Hering, Amanda S.},
	month = dec,
	year = {2019},
	pmid = {31009058},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {copula, Markov process, pseudo-likelihood, spatiotemporal},
	pages = {1156--1167},
	file = {PDF:/home/miguel/Zotero/storage/LSPYDWT4/biometrics_75_4_1156.pdf:application/pdf},
}

@article{zaba_2014-2015_2016-1,
	title = {The 2014-2015 warming anomaly in the {Southern} {California} {Current} {System} observed by underwater gliders},
	volume = {43},
	issn = {19448007},
	doi = {10.1002/2015GL067550},
	abstract = {Large-scale patterns of positive temperature anomalies persisted throughout the surface waters of the North Pacific Ocean during 2014-2015. In the Southern California Current System, measurements by our sustained network of underwater gliders reveal the coastal effects of the recent warming. Regional upper ocean temperature anomalies were greatest since the initiation of the glider network in 2006. Additional observed physical anomalies included a depressed thermocline, high stratification, and freshening; induced biological consequences included changes in the vertical distribution of chlorophyll fluorescence. Contemporaneous surface heat flux and wind strength perturbations suggest that local anomalous atmospheric forcing caused the unusual oceanic conditions.},
	number = {3},
	journal = {Geophysical Research Letters},
	author = {Zaba, Katherine D. and Rudnick, Daniel L.},
	month = feb,
	year = {2016},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {California Current System, downwelling anomaly, Spray gliders, warm SST},
	pages = {1241--1248},
	file = {PDF:/home/miguel/Zotero/storage/DTSJFAIY/Geophysical Research Letters - 2016 - Zaba - The 2014 2015 warming anomaly in the Southern California Current System.pdf:application/pdf},
}

@article{rudnick_ocean_2016-1,
	title = {Ocean {Research} {Enabled} by {Underwater} {Gliders}},
	volume = {8},
	issn = {19410611},
	doi = {10.1146/annurev-marine-122414-033913},
	abstract = {Underwater gliders are autonomous underwater vehicles that profile vertically by changing their buoyancy and use wings to move horizontally. Gliders are useful for sustained observation at relatively fine horizontal scales, especially to connect the coastal and open ocean. In this review, research topics are grouped by time and length scales. Large-scale topics addressed include the eastern and western boundary currents and the regional effects of climate variability. The accessibility of horizontal length scales of order 1 km allows investigation of mesoscale and submesoscale features such as fronts and eddies. Because the submesoscales dominate vertical fluxes in the ocean, gliders have found application in studies of biogeochemical processes. At the finest scales, gliders have been used to measure internal waves and turbulent dissipation. The review summarizes gliders' achievements to date and assesses their future in ocean observation.},
	journal = {Annual Review of Marine Science},
	author = {Rudnick, Daniel L.},
	month = jan,
	year = {2016},
	pmid = {26291384},
	note = {Publisher: Annual Reviews Inc.},
	keywords = {Autonomous underwater vehicles, Biogeochemistry, Climate, Internal waves, Mesoscale, Sustained observations},
	pages = {519--541},
	file = {PDF:/home/miguel/Zotero/storage/L462JD3J/Rudnick_2015.pdf:application/pdf},
}

@article{vanem_bayesian_2014-1,
	title = {Bayesian hierarchical spatio-temporal modelling of trends and future projections in the ocean wave climate with a {CO2} regression component},
	volume = {21},
	issn = {13528505},
	doi = {10.1007/s10651-013-0251-6},
	abstract = {Bad weather and rough seas continue to be a major cause for ship losses and is thus a significant contributor to the risk to maritime transportation. This stresses the importance of taking severe sea state conditions adequately into account, with due treatment of the uncertainties involved, in ship design and operation in order to enhance safety. Hence, there is a need for appropriate stochastic models describing the variability of sea states. These should also incorporate realistic projections of future return levels of extreme sea states, taking into account long-term trends related to climate change and inherent uncertainties. The stochastic ocean wave model presented in this paper exploits the flexible framework of Bayesian hierarchical space-time models. It allows modelling of complex dependence structures in space and time and incorporation of physical features and prior knowledge, yet at the same time remains intuitive and easily interpreted. Furthermore, by taking a Bayesian approach, the uncertainties of the model parameters are also taken into account. A regression component with CO2 as an explanatory variable has been introduced in order to extract long-term trends in the data. The model has been fitted by monthly maximum significant wave height data for an area in the North Atlantic ocean. The different components of the model will be outlined in the paper, and the results will be discussed. Furthermore, a discussion of possible extensions to the model will be given. © 2013 Springer Science+Business Media New York.},
	number = {2},
	journal = {Environmental and Ecological Statistics},
	author = {Vanem, Erik and Huseby, Arne Bang and Natvig, Bent},
	year = {2014},
	note = {Publisher: Kluwer Academic Publishers},
	keywords = {Stochastic processes, Bayesian hierarchical modelling, Climate change, MCMC, Modelling the effects of climate change, Ocean waves, Spatio-temporal modelling},
	pages = {189--220},
	file = {PDF:/home/miguel/Zotero/storage/6FBQMP3G/s10651-013-0251-6.pdf:application/pdf},
}

@article{pizarro_underwater_2016-1,
	title = {Underwater glider observations in the oxygen minimum zone off central {Chile}},
	volume = {97},
	issn = {00030007},
	doi = {10.1175/BAMS-D-14-00040.1},
	number = {10},
	journal = {Bulletin of the American Meteorological Society},
	author = {Pizarro, Oscar and Ramirez, Nadin and Castillo, Manuel I. and Cifuentes, Ursula and Rojas, Winston and Pizarro-Koch, Matias},
	month = oct,
	year = {2016},
	note = {Publisher: American Meteorological Society},
	pages = {1783--1789},
	file = {PDF:/home/miguel/Zotero/storage/CVJY7WBN/bams-bams-d-14-00040.1.pdf:application/pdf},
}

@article{zarokanellos_frontal_2022-1,
	title = {Frontal {Dynamics} in the {Alboran} {Sea}: 1. {Coherent} {3D} {Pathways} at the {Almeria}-{Oran} {Front} {Using} {Underwater} {Glider} {Observations}},
	volume = {127},
	issn = {21699291},
	doi = {10.1029/2021JC017405},
	abstract = {Ocean fronts are areas that can support phytoplankton production through fertilization in the sunlit layer and the subduction of biogeochemical properties from the surface to the interior of the ocean. The Almeria-Oran (AO) front is formed from the juxtaposition of fresh inflowing Atlantic waters and more saline re-circulating Mediterranean waters. A fleet of three gliders flying in parallel lines was deployed across the AO to obtain observations in the CALYPSO project. These observations were combined with remote sensing and modeling simulations, thus providing a novel approach to identifying the three-dimensional transport and the submesoscale across-front circulation. The resulting 33 cross-front sections reveal spatial and temporal changes in the frontal boundary, with isopycnals steepening and/or relaxing. The observations revealed strong horizontal density gradients (up to ∼1.4 kg m−3) and the spatial variability was observed over different length scales (∼10–45 km). The potential vorticity decreased across the front due to the vorticity component in the horizontal density gradient direction. The predominant cyclonic relative vorticity on the dense side of the AO is associated with downwelling processes. The biogeochemical observations also suggest vertical transport along coherent pathways through baroclinic instability. Phytoplankton biomass enhancement occurs as a result, and is subducted below the euphotic layer. The observed oxygen filaments show upwelling and downwelling, providing a mechanism for oxygenating deeper layers and reducing the ventilation of deep low-oxygenated waters. Understanding the mechanisms of vertical transport can help us evaluate the dynamics of ocean fronts and their impacts on biological carbon storage.},
	number = {3},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Zarokanellos, Nikolaos D. and Rudnick, Daniel L. and Garcia-Jove, Maximo and Mourre, Baptiste and Ruiz, Simon and Pascual, Ananda and Tintoré, Joaquin},
	month = mar,
	year = {2022},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Almeria-Oran front, frontal dynamics, Instability processes, phytoplankton and oxygen distribution, submesoscale filaments, vertical motions},
	file = {PDF:/home/miguel/Zotero/storage/4W962W84/JGR Oceans - 2022 - Zarokanellos - Frontal Dynamics in the Alboran Sea  1  Coherent 3D Pathways at the Almeria%E2%80%90Oran Front.pdf:application/pdf},
}

@book{noauthor_2013_2014-1,
	title = {2013 {OCEANS}-{San} {Diego} : 23-27 {September} 2013},
	isbn = {978-0-933957-40-4},
	publisher = {IEEE},
	year = {2014},
	file = {PDF:/home/miguel/Zotero/storage/2XLH598B/Monitoring_Dissolved_Oxygen_in_New_Jersey_coastal_waters_using_autonomous_gliders_Multi-year_trends_and_event_response.pdf:application/pdf},
}

@book{noauthor_oceans_2006-1,
	title = {{OCEANS} 2006},
	isbn = {1-4244-0115-1},
	abstract = {Title from content provider.},
	publisher = {[publisher not identified]},
	year = {2006},
	file = {PDF:/home/miguel/Zotero/storage/KKRSI7MC/Studying_the_Dynamics_and_Biological_Significance_of_the_Hudson_River_Using_an_Ocean_Observatory.pdf:application/pdf},
}

@article{olita_frontal_2017,
	title = {Frontal dynamics boost primary production in the summer stratified {Mediterranean} sea},
	volume = {67},
	issn = {16167228},
	doi = {10.1007/s10236-017-1058-z},
	abstract = {Bio-physical glider measurements from a unique process-oriented experiment in the Eastern Alboran Sea (AlborEx) allowed us to observe the distribution of the deep chlorophyll maximum (DCM) across an intense density front, with a resolution (∼ 400 m) suitable for investigating sub-mesoscale dynamics. This front, at the interface between Atlantic and Mediterranean waters, had a sharp density gradient (Δρ ∼ 1 kg/m3 in ∼ 10 km) and showed imprints of (sub-)mesoscale phenomena on tracer distributions. Specifically, the chlorophyll-a concentration within the DCM showed a disrupted pattern along isopycnal surfaces, with patches bearing a relationship to the stratification (buoyancy frequency) at depths between 30 and 60 m. In order to estimate the primary production (PP) rate within the chlorophyll patches observed at the sub-surface, we applied the Morel and Andrè (J Geophys Res 96:685–698 1991) bio-optical model using the photosynthetic active radiation (PAR) from Argo profiles collected simultaneously with glider data. The highest production was located concurrently with domed isopycnals on the fresh side of the front, suggestive that (sub-)mesoscale upwelling is carrying phytoplankton patches from less to more illuminated levels, with a contemporaneous delivering of nutrients. Integrated estimations of PP (1.3 g C m−2d−1) along the glider path are two to four times larger than the estimations obtained from satellite-based algorithms, i.e., derived from the 8-day composite fields extracted over the glider trip path. Despite the differences in spatial and temporal sampling between instruments, the differences in PP estimations are mainly due to the inability of the satellite to measure DCM patches responsible for the high production. The deepest (depth {\textgreater} 60 m) chlorophyll patches are almost unproductive and probably transported passively (subducted) from upper productive layers. Finally, the relationship between primary production and oxygen is also investigated. The logarithm of the primary production in the DCM interior (chlorophyll (Chl) {\textgreater} 0.5 mg/m3) shows a linear negative relationship with the apparent oxygen utilization, confirming that high chlorophyll patches are productive. The slope of this relationship is different for Atlantic, mixed interface waters and Mediterranean waters, suggesting the presence of differences in planktonic communities (whether physiological, population, or community level should be object of further investigation) on the different sides of the front. In addition, the ratio of optical backscatter to Chl is high within the intermediate (mixed) waters, which is suggestive of large phytoplankton cells, and lower within the core of the Atlantic and Mediterranean waters. These observations highlight the relevance of fronts in triggering primary production at DCM level and shaping the characteristic patchiness of the pelagic domain. This gains further relevance considering the inadequacy of optical satellite sensors to observe DCM concentrations at such fine scales.},
	number = {6},
	journal = {Ocean Dynamics},
	author = {Olita, Antonio and Capet, Arthur and Claret, Mariona and Mahadevan, Amala and Poulain, Pierre Marie and Ribotti, Alberto and Ruiz, Simón and Tintoré, Joaquín and Tovar-Sánchez, Antonio and Pascual, Ananda},
	month = jun,
	year = {2017},
	note = {Publisher: Springer Verlag},
	keywords = {AOU, Fronts, Glider, Mediterranean sea, Primary production, sub-mesoscale},
	pages = {767--782},
	file = {PDF:/home/miguel/Zotero/storage/7RVGJR28/retrieve-2.pdf:application/pdf},
}

@article{coppola_high-resolution_2023,
	title = {High-resolution study of the air-sea {CO2} flux and net community oxygen production in the {Ligurian} {Sea} by a fleet of gliders},
	volume = {10},
	issn = {22967745},
	doi = {10.3389/fmars.2023.1233845},
	abstract = {Intense glider monitoring was conducted in the Ligurian Sea for five months to capture the Net Community Production (NCP) variability in one of the most dynamic and productive regions of the Mediterranean Sea. Using the SeaExplorer glider technology, we were able to observe continuously from January to the end of May 2018 the physical and biogeochemical variables during the last period of intense convection observed in this region. High-frequency measurements from these gliders provided valuable information for determining dissolved O2 (DO) concentrations between coastal and open sea waters. Our DO balance approach provided an estimate of NCP fluxes complemented by the prediction of air-sea CO2 fluxes based on a neural network adapted to the Mediterranean Sea (CANYON-MED). Based on our NCP calculation method, our results show that the air-sea O2 flux and DO inventory have contributed largely to the NCP variability. The NCP values also suggest that heterotrophic conditions were predominant in winter and became autotrophic in spring, with strong variability in coastal waters due to the occurrence of sub-mesoscale structures. Finally, CO2 fluxes at the air-sea interface reveal that during the convection period, the central zone of the Ligurian Sea acted as a CO2 sink from January to March with little impact on NCP fluxes counterbalanced by a thermal effect of seawater.},
	journal = {Frontiers in Marine Science},
	author = {Coppola, Laurent and Fourrier, Marine and Pasqueron de Fommervault, Orens and Poteau, Antoine and Riquier, Emilie Diamond and Béguery, Laurent},
	year = {2023},
	note = {Publisher: Frontiers Media SA},
	keywords = {dissolved oxygen, air-sea CO2 flux, Mediterranean Sea, net community production, ocean gliders},
	file = {PDF:/home/miguel/Zotero/storage/TCVNU5GH/fmars-10-1233845.pdf:application/pdf},
}

@article{yang_general_2023-1,
	title = {General {Anomaly} {Detection} of {Underwater} {Gliders} {Validated} by {Large}-scale {Deployment} {Datasets}},
	url = {http://arxiv.org/abs/2308.00180},
	abstract = {Underwater gliders have been widely used in oceanography for a range of applications. However, unpredictable events like shark strikes or remora attachments can lead to abnormal glider behavior or even loss of the instrument. This paper employs an anomaly detection algorithm to assess operational conditions of underwater gliders in the real-world ocean environment. Prompt alerts are provided to glider pilots upon detecting any anomaly, so that they can take control of the glider to prevent further harm. The detection algorithm is applied to multiple datasets collected in real glider deployments led by the University of Georgia's Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). In order to demonstrate the algorithm generality, the experimental evaluation is applied to four glider deployment datasets, each highlighting various anomalies happening in different scenes. Specifically, we utilize high resolution datasets only available post-recovery to perform detailed analysis of the anomaly and compare it with pilot logs. Additionally, we simulate the online detection based on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery one, the online detection is of great importance as it allows glider pilots to monitor potential abnormal conditions in real time.},
	author = {Yang, Ruochu and Lembke, Chad and Zhang, Fumin and Edwards, Catherine},
	month = jul,
	year = {2023},
	note = {arXiv: 2308.00180},
	file = {PDF:/home/miguel/Zotero/storage/LFW4YHDU/2308.00180v3.pdf:application/pdf},
}

@techreport{sahu_bayesian_nodate-1,
	title = {Bayesian {Modeling} of {Spatio}-{Temporal} {Data} with {R}},
	url = {https://www.crcpress.com/Chapman--},
	author = {Sahu, Sujit K},
	keywords = {bayesian, modelling, spatio-temporal},
	file = {PDF:/home/miguel/Zotero/storage/VP34XV8R/Bayesian Modeling of Spatio-Temporal Data with R_25_04_13_18_22_52.pdf:application/pdf},
}

@article{verzelen_adaptive_2010,
	title = {Adaptive estimation of covariance matrices via cholesky decomposition},
	volume = {4},
	issn = {19357524},
	doi = {10.1214/10-EJS580},
	abstract = {This paper studies the estimation of a large covariance matrix. We introduce a novel procedure called ChoSelect based on the Cholesky factor of the inverse covariance. This method uses a dimension reduction strategy by selecting the pattern of zero of the Cholesky factor. Alternatively, ChoSelect can be interpreted as a graph estimation procedure for directed Gaussian graphical models. Our approach is particularly relevant when the variables under study have a natural ordering (e.g. time series) or more generally when the Cholesky factor is approximately sparse. ChoSelect achieves non-asymptotic oracle inequalities with respect to the Kullback-Leibler entropy. Moreover, it satisfies various adaptive properties from a minimax point of view. We also introduce and study a two-stage procedure that combines ChoSelect with the Lasso. This last method enables the practitioner to choose his own trade-off between statistical efficiency and computational complexity. Moreover, it is consistent under weaker assumptions than the Lasso. The practical performances of the different procedures are assessed on numerical examples. © 2010, Institute of Mathematical Statistics. All rights reserved.},
	journal = {Electronic Journal of Statistics},
	author = {Verzelen, Nicolas},
	year = {2010},
	note = {arXiv: 1010.1445},
	keywords = {Banding, Cholesky decomposition, Covariance matrix, Directed graphical models, Minimax rate of estimation, Penalized criterion},
	pages = {1113--1150},
	file = {PDF:/home/miguel/Zotero/storage/SZ776VT9/10-EJS580.pdf:application/pdf},
}

@article{sang_parametric_2017,
	title = {Parametric functional principal component analysis},
	volume = {73},
	issn = {15410420},
	doi = {10.1111/biom.12641},
	abstract = {Functional principal component analysis (FPCA) is a popular approach in functional data analysis to explore major sources of variation in a sample of random curves. These major sources of variation are represented by functional principal components (FPCs). Most existing FPCA approaches use a set of flexible basis functions such as B-spline basis to represent the FPCs, and control the smoothness of the FPCs by adding roughness penalties. However, the flexible representations pose difficulties for users to understand and interpret the FPCs. In this article, we consider a variety of applications of FPCA and find that, in many situations, the shapes of top FPCs are simple enough to be approximated using simple parametric functions. We propose a parametric approach to estimate the top FPCs to enhance their interpretability for users. Our parametric approach can also circumvent the smoothing parameter selecting process in conventional nonparametric FPCA methods. In addition, our simulation study shows that the proposed parametric FPCA is more robust when outlier curves exist. The parametric FPCA method is demonstrated by analyzing several datasets from a variety of applications.},
	number = {3},
	journal = {Biometrics},
	author = {Sang, Peijun and Wang, Liangliang and Cao, Jiguo},
	month = sep,
	year = {2017},
	pmid = {28295173},
	keywords = {Functional Data Analysis, Curve Variation, Eigenfuntions, Robust Estimation},
	pages = {802--810},
	file = {PDF:/home/miguel/Zotero/storage/9H48CCCC/biometrics_73_3_802.pdf:application/pdf},
}

@article{wang_low-rank_2022,
	title = {Low-{Rank} {Covariance} {Function} {Estimation} for {Multidimensional} {Functional} {Data}},
	volume = {117},
	issn = {1537274X},
	doi = {10.1080/01621459.2020.1820344},
	abstract = {Multidimensional function data arise from many fields nowadays. The covariance function plays an important role in the analysis of such increasingly common data. In this article, we propose a novel nonparametric covariance function estimation approach under the framework of reproducing kernel Hilbert spaces (RKHS) that can handle both sparse and dense functional data. We extend multilinear rank structures for (finite-dimensional) tensors to functions, which allow for flexible modeling of both covariance operators and marginal structures. The proposed framework can guarantee that the resulting estimator is automatically semipositive definite, and can incorporate various spectral regularizations. The trace-norm regularization in particular can promote low ranks for both covariance operator and marginal structures. Despite the lack of a closed form, under mild assumptions, the proposed estimator can achieve unified theoretical results that hold for any relative magnitudes between the sample size and the number of observations per sample field, and the rate of convergence reveals the phase-transition phenomenon from sparse to dense functional data. Based on a new representer theorem, an ADMM algorithm is developed for the trace-norm regularization. The appealing numerical performance of the proposed estimator is demonstrated by a simulation study and the analysis of a dataset from the Argo project. Supplementary materials for this article are available online.},
	number = {538},
	journal = {Journal of the American Statistical Association},
	author = {Wang, Jiayi and Wong, Raymond K.W. and Zhang, Xiaoke},
	year = {2022},
	note = {arXiv: 2008.12919
Publisher: American Statistical Association},
	keywords = {Functional data analysis, Multilinear ranks, Tensor product space, Unified theory},
	pages = {809--822},
	file = {PDF:/home/miguel/Zotero/storage/ZYXF2W85/Low-Rank Covariance Function Estimation for Multidimensional Functional Data.pdf:application/pdf},
}

@article{schmutz_clustering_2020,
	title = {Clustering multivariate functional data in group-specific functional subspaces},
	volume = {35},
	issn = {16139658},
	doi = {10.1007/s00180-020-00958-4},
	abstract = {With the emergence of numerical sensors in many aspects of everyday life, there is an increasing need in analyzing multivariate functional data. This work focuses on the clustering of such functional data, in order to ease their modeling and understanding. To this end, a novel clustering technique for multivariate functional data is presented. This method is based on a functional latent mixture model which fits the data into group-specific functional subspaces through a multivariate functional principal component analysis. A family of parsimonious models is obtained by constraining model parameters within and between groups. An Expectation Maximization algorithm is proposed for model inference and the choice of hyper-parameters is addressed through model selection. Numerical experiments on simulated datasets highlight the good performance of the proposed methodology compared to existing works. This algorithm is then applied to the analysis of the pollution in French cities for 1 year.},
	number = {3},
	journal = {Computational Statistics},
	author = {Schmutz, Amandine and Jacques, Julien and Bouveyron, Charles and Chèze, Laurence and Martin, Pauline},
	month = sep,
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Model-based clustering, EM algorithm, Multivariate functional curves, Multivariate functional principal component analysis},
	pages = {1101--1131},
	file = {PDF:/home/miguel/Zotero/storage/IIL73CIJ/s00180-020-00958-4.pdf:application/pdf},
}

@article{porcu_matern_2023,
	title = {The {Mat}{\textbackslash}'ern {Model}: {A} {Journey} through {Statistics}, {Numerical} {Analysis} and {Machine} {Learning}},
	url = {http://arxiv.org/abs/2303.02759},
	abstract = {The Mat{\textbackslash}'ern model has been a cornerstone of spatial statistics for more than half a century. More recently, the Mat{\textbackslash}'ern model has been central to disciplines as diverse as numerical analysis, approximation theory, computational statistics, machine learning, and probability theory. In this article we take a Mat{\textbackslash}'ern-based journey across these disciplines. First, we reflect on the importance of the Mat{\textbackslash}'ern model for estimation and prediction in spatial statistics, establishing also connections to other disciplines in which the Mat{\textbackslash}'ern model has been influential. Then, we position the Mat{\textbackslash}'ern model within the literature on big data and scalable computation: the SPDE approach, the Vecchia likelihood approximation, and recent applications in Bayesian computation are all discussed. Finally, we review recent devlopments, including flexible alternatives to the Mat{\textbackslash}'ern model, whose performance we compare in terms of estimation, prediction, screening effect, computation, and Sobolev regularity properties.},
	author = {Porcu, Emilio and Bevilacqua, Moreno and Schaback, Robert and Oates, Chris J.},
	month = mar,
	year = {2023},
	note = {arXiv: 2303.02759},
	file = {PDF:/home/miguel/Zotero/storage/IXG8UNZF/2303.02759v1.pdf:application/pdf},
}

@article{pulido_clustering_2025,
	title = {Clustering multivariate functional data with the epigraph and hypograph indices: a case study on {Madrid} air quality},
	issn = {14363259},
	doi = {10.1007/s00477-025-02986-2},
	abstract = {With the rapid growth of data generation, advancements in functional data analysis have become essential, especially for approaches that handle multiple variables at the same time. This paper introduces a novel formulation of the epigraph and hypograph indices, along with their generalized expressions, specifically designed for multivariate functional data (MFD). These new definitions account for interrelationships between variables, enabling effective clustering of MFD based on the original data curves and their first two derivatives. The methodology developed here has been tested on simulated datasets, demonstrating strong performance compared to state-of-the-art methods. Its practical utility is further illustrated with two environmental datasets: the Canadian weather dataset and a 2023 air quality study in Madrid. These applications highlight the potential of the method as a great tool for analyzing complex environmental data, offering valuable insights for researchers and policymakers in climate and environmental research.},
	journal = {Stochastic Environmental Research and Risk Assessment},
	author = {Pulido, Belén and Franco-Pereira, Alba M. and Lillo, Rosa E.},
	year = {2025},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Multivariate functional data, Clustering, EHyClus, Environmental data analysis, Epigraph, Hypograph},
	file = {PDF:/home/miguel/Zotero/storage/P3GYXNWL/s00477-025-02986-2.pdf:application/pdf},
}

@article{yao_functional_2005-2,
	title = {Functional data analysis for sparse longitudinal data},
	volume = {100},
	issn = {01621459},
	doi = {10.1198/016214504000001745},
	abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
	number = {470},
	journal = {Journal of the American Statistical Association},
	author = {Yao, Fang and Müller, Hans Georg and Wang, Jane Ling},
	month = jun,
	year = {2005},
	keywords = {Smoothing, Asymptotics, Conditioning, Confidence band, Measurement error, Principal components, Simultaneous inference},
	pages = {577--590},
	file = {PDF:/home/miguel/Zotero/storage/5IVHMSNE/2005 Functional Data Analysis for Sparse Longitudinal D [retrieved_2025-03-17].pdf:application/pdf},
}

@article{zhang_sparse_2016-3,
	title = {From sparse to dense functional data and beyond},
	volume = {44},
	issn = {00905364},
	doi = {10.1214/16-AOS1446},
	abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
	number = {5},
	journal = {Annals of Statistics},
	author = {Zhang, Xiaoke and Wang, Jane Ling},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Asymptotic normality, L2 convergence, Local linear smoothing, Uniform convergence, Weighing schemes},
	pages = {2281--2321},
	file = {PDF:/home/miguel/Zotero/storage/YSKRJ4HI/16-AOS1446.pdf:application/pdf},
}

@article{zimek_survey_2012-5,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/IRKYJC4V/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data-2.pdf:application/pdf},
}

@article{zimek_survey_2012-6,
	title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
	volume = {5},
	issn = {19321864},
	doi = {10.1002/sam.11161},
	abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Statistical Analysis and Data Mining},
	author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans Peter},
	year = {2012},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Anomalies in high-dimensional data, Approximate outlier detection, Correlation outlier detection, Curse of dimensionality, Outlier detection in high-dimensional data, Subspace outlier detection},
	pages = {363--387},
	file = {PDF:/home/miguel/Zotero/storage/685R7WPV/Statistical Analysis - 2012 - Zimek - A survey on unsupervised outlier detection in high%E2%80%90dimensional numerical data.pdf:application/pdf},
}

@article{evans_outlier_2015,
	title = {Outlier {Identification} in {Model}-{Based} {Cluster} {Analysis}},
	volume = {32},
	issn = {14321343},
	doi = {10.1007/s00357-015-9171-5},
	abstract = {In model-based clustering based on normal-mixture models, a few outlying observations can influence the cluster structure and number. This paper develops a method to identify these, however it does not attempt to identify clusters amidst a large field of noisy observations. We identify outliers as those observations in a cluster with minimal membership proportion or for which the cluster-specific variance with and without the observation is very different. Results from a simulation study demonstrate the ability of our method to detect true outliers without falsely identifying many non-outliers and improved performance over other approaches, under most scenarios. We use the contributed R package MCLUST for model-based clustering, but propose a modified prior for the cluster-specific variance which avoids degeneracies in estimation procedures. We also compare results from our outlier method to published results on National Hockey League data.},
	number = {1},
	journal = {Journal of Classification},
	author = {Evans, Katie and Love, Tanzy and Thurston, Sally W.},
	month = apr,
	year = {2015},
	note = {Publisher: Springer New York LLC},
	keywords = {Influential points, MCLUST, National Hockey League, Normal-mixture models, Prior},
	pages = {63--84},
	file = {PDF:/home/miguel/Zotero/storage/ZNKIFLG7/s00357-015-9171-5.pdf:application/pdf},
}

@techreport{pang_sparse_nodate,
	title = {Sparse {Modeling}-{Based} {Sequential} {Ensemble} {Learning} for {Effective} {Outlier} {Detection} in {High}-{Dimensional} {Numeric} {Data}},
	url = {www.aaai.org},
	abstract = {The large proportion of irrelevant or noisy features in real-life high-dimensional data presents a significant challenge to subspace/feature selection-based high-dimensional out-lier detection (a.k.a. outlier scoring) methods. These methods often perform the two dependent tasks: relevant feature subset search and outlier scoring independently, consequently retaining features/subspaces irrelevant to the scoring method and downgrading the detection performance. This paper introduces a novel sequential ensemble-based framework SEMSE and its instance CINFO to address this issue. SEMSE learns the sequential ensembles to mutually refine feature selection and outlier scoring by iterative sparse modeling with outlier scores as the pseudo target feature. CINFO instantiates SEMSE by using three successive recurrent components to build such sequential ensembles. Given outlier scores output by an existing outlier scoring method on a feature subset , CINFO first defines a Cantelli's inequality-based out-lier thresholding function to select outlier candidates with a false positive upper bound. It then performs lasso-based sparse regression by treating the outlier scores as the target feature and the original features as predictors on the out-lier candidate set to obtain a feature subset that is tailored for the outlier scoring method. Our experiments show that two different outlier scoring methods enabled by CINFO (i) perform significantly better on 11 real-life high-dimensional data sets, and (ii) have much better resilience to noisy features , compared to their bare versions and three state-of-the-art competitors. The source code of CINFO is available at https://sites.google.com/site/gspangsite/sourcecode.},
	author = {Pang, Guansong and Cao, Longbing and Chen, Ling and Lian, Defu and Liu, Huan},
	keywords = {Machine Learning Methods Track},
	file = {PDF:/home/miguel/Zotero/storage/VX8DST3I/11692-Article Text-15220-1-2-20201228.pdf:application/pdf},
}

@techreport{xu_comparison_2018-4,
	title = {A {Comparison} of {Outlier} {Detection} {Techniques} for {High}-{Dimensional} {Data}},
	abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
	author = {Xu, Xiaodan and Liu, Huawen and Li, Li and Yao, Minghai},
	year = {2018},
	keywords = {data mining, high-dimensional data, outlier detection, evaluation measurement},
	file = {PDF:/home/miguel/Zotero/storage/7MHJ3PEI/25892518.pdf:application/pdf},
}

@techreport{sikder_outlier_nodate-4,
	title = {Outlier {Detection} using {AI}: {A} {Survey}},
	author = {Sikder, Nazmul Kabir and Batarseh, Feras A},
	file = {PDF:/home/miguel/Zotero/storage/XIYH7NCE/2112.00588v1.pdf:application/pdf},
}

@article{asir_antony_gnana_singh_model-based_2016,
	title = {Model-based outlier detection system with statistical preprocessing},
	volume = {15},
	issn = {15389472},
	doi = {10.22237/jmasm/1462077480},
	abstract = {Reliability, lack of error, and security are important improvements to quality of service. Outlier detection is a process of detecting the erroneous parts or abnormal objects in defined populations, and can contribute to secured and error-free services. Outlier detection approaches can be categorized into four types: statistic-based, unsupervised, supervised, and semi-supervised. A model-based outlier detection system with statistical preprocessing is proposed, taking advantage of the statistical approach to preprocess training data and using unsupervised learning to construct the model. The robustness of the proposed system is evaluated using the performance evaluation metrics sum of squared error (SSE) and time to build model (TBM). The proposed system performs better for detecting outliers regardless of the application domain.},
	number = {1},
	journal = {Journal of Modern Applied Statistical Methods},
	author = {Asir Antony Gnana Singh, D. and Jebalamar Leavline, E.},
	year = {2016},
	note = {Publisher: Wayne State University},
	keywords = {Outlier, Anomaly detection, Inter-quartile range, Preprocessing},
	pages = {789--801},
	file = {PDF:/home/miguel/Zotero/storage/LAEHCP9Y/Model-Based ODS with Statistical Preprocessing.pdf:application/pdf},
}

@inproceedings{klerx_model-based_2014,
	title = {Model-{Based} {Anomaly} {Detection} for {Discrete} {Event} {Systems}},
	volume = {2014-December},
	isbn = {978-1-4799-6572-4},
	doi = {10.1109/ICTAI.2014.105},
	abstract = {Model-based anomaly detection in technical systems is an important application field of artificial intelligence. We consider discrete event systems, which is a system class to which a wide range of relevant technical systems belong and for which no comprehensive model-based anomaly detection approach exists so far. The original contributions of this paper are threefold: First, we identify the types of anomalies that occur in discrete event systems and we propose a tailored behavior model that captures all anomaly types, called probabilistic deterministic timed-transition automata (PDTTA). Second, we present a new algorithm to learn a PDTTA from sample observations of a system. Third, we describe an approach to detect anomalies based on a learned PDTTA. An empirical evaluation in a practical application, namely ATM fraud detection, shows promising results.},
	booktitle = {Proceedings - {International} {Conference} on {Tools} with {Artificial} {Intelligence}, {ICTAI}},
	publisher = {IEEE Computer Society},
	author = {Klerx, Timo and Anderka, Maik and Buning, Hans Kleine and Priesterjahn, Steffen},
	month = dec,
	year = {2014},
	note = {ISSN: 10823409},
	keywords = {ATM Fraud Detection, Automatic Model Generation, Discrete Event Systems, Model-based Anomaly Detection},
	pages = {665--672},
	file = {PDF:/home/miguel/Zotero/storage/TJBFC837/Model-Based_Anomaly_Detection_for_Discrete_Event_Systems.pdf:application/pdf},
}

@inproceedings{moallemi_model-based_2021,
	title = {Model-based vs. {Data}-driven {Approaches} for {Anomaly} {Detection} in {Structural} {Health} {Monitoring}: {A} {Case} {Study}},
	volume = {2021-May},
	isbn = {978-1-7281-9539-1},
	doi = {10.1109/I2MTC50364.2021.9459999},
	abstract = {Modern Structural Health Monitoring (SHM) systems are becoming of pervasive use in civil engineering because they can track the structural condition and detect damages of critical and civil infrastructures such as buildings, viaducts, and tunnels. Although noticeable work has been done to improve anomaly detection for ensuring public safety, algorithms that can be executed on low-cost hardware for long-term monitoring are still an open issue to the community. This paper presents a new framework that exploits compression techniques to identify anomalies in the structure, avoiding continuous streaming of raw data to the cloud. We used a real installation on a bridge in Italy to test the proposed anomaly detection algorithm. We trained three compression models, namely a Principal Component Analysis (PCA), a fully-connected autoencoder, and a convolutional autoencoder. Performance comparison is also provided through an ablation study that analyzes the impact of various parameters. Results demonstrate that the model-based approach, i.e., PCA, can reach a better accuracy whereas data-driven models, i.e., autoencoders, are limited by training set size.},
	booktitle = {Conference {Record} - {IEEE} {Instrumentation} and {Measurement} {Technology} {Conference}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Moallemi, Amirhossein and Burrello, Alessio and Brunelli, Davide and Benini, Luca},
	month = may,
	year = {2021},
	note = {ISSN: 10915281},
	keywords = {Anomaly detection, Compression Techniques, Deep Learning, Edge computing, Structural Health Monitoring},
	file = {PDF:/home/miguel/Zotero/storage/EAR2YFGL/Model-based_vs._Data-driven_Approaches_for_Anomaly_Detection_in_Structural_Health_Monitoring_a_Case_Study.pdf:application/pdf},
}

@techreport{angiulli_outlier_nodate,
	title = {Outlier {Mining} in {Large} {High}-{Dimensional} {Data} {Sets}},
	abstract = {In this paper, a new definition of distance-based outlier and an algorithm, called HilOut, designed to efficiently detect the top n outliers of a large and high-dimensional data set are proposed. Given an integer k, the weight of a point is defined as the sum of the distances separating it from its k nearest-neighbors. Outlier are those points scoring the largest values of weight. The algorithm HilOut makes use of the notion of space-filling curve to linearize the data set, and it consists of two phases. The first phase provides an approximate solution, within a rough factor, after the execution of at most d þ 1 sorts and scans of the data set, with temporal cost quadratic in d and linear in N and in k, where d is the number of dimensions of the data set and N is the number of points in the data set. During this phase, the algorithm isolates points candidate to be outliers and reduces this set at each iteration. If the size of this set becomes n, then the algorithm stops reporting the exact solution. The second phase calculates the exact solution with a final scan examining further the candidate outliers that remained after the first phase. Experimental results show that the algorithm always stops, reporting the exact solution, during the first phase after much less than d þ 1 steps. We present both an in-memory and disk-based implementation of the HilOut algorithm and a thorough scaling analysis for real and synthetic data sets showing that the algorithm scales well in both cases.},
	author = {Angiulli, Fabrizio and Pizzuti, Clara},
	keywords = {Index Terms-Outlier mining, space-filling curves},
	file = {PDF:/home/miguel/Zotero/storage/AZDGIJID/Outlier_mining_in_large_high-dimensional_data_sets.pdf:application/pdf},
}

@inproceedings{suhermi_functional_2024-3,
	title = {Functional {Data} {Analysis} for {Household} {Appliance} {Energy} {Consumption} {Prediction}},
	doi = {10.1109/IC3INA64086.2024.10732718},
	abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
	booktitle = {International {Conference} on {Computer}, {Control}, {Informatics} and its {Applications}, {IC3INA}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Suhermi, Novri and Aisy, Rahida Rihhadatul},
	year = {2024},
	note = {Issue: 2024
ISSN: 29945925},
	keywords = {Prediction, Functional Data Analysis, Energy Consumption, Functional Regression},
	pages = {468--471},
	file = {PDF:/home/miguel/Zotero/storage/5RSBUDNN/Functional_Data_Analysis_for_Household_Appliance_Energy_Consumption_Prediction.pdf:application/pdf},
}

@article{pigoli_distances_2014-3,
	title = {Distances and inference for covariance operators},
	volume = {101},
	issn = {14643510},
	doi = {10.1093/biomet/asu008},
	abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
	number = {2},
	journal = {Biometrika},
	author = {Pigoli, Davide and Aston, John A.D. and Dryden, Ian L. and Secchi, Piercesare},
	year = {2014},
	note = {Publisher: Oxford University Press},
	keywords = {Functional data analysis, Shape analysis, Distance metric, Procrustes analysis},
	pages = {409--422},
	file = {PDF:/home/miguel/Zotero/storage/8G348E66/asu008.pdf:application/pdf},
}

@article{correia_online_2024,
	title = {Online model-based anomaly detection in multivariate time series: {Taxonomy}, survey, research challenges and future directions},
	volume = {138},
	issn = {09521976},
	doi = {10.1016/j.engappai.2024.109323},
	abstract = {Time-series anomaly detection plays an important role in engineering processes, like development, manufacturing and other operations involving dynamic systems. These processes can greatly benefit from advances in the field, as state-of-the-art approaches may aid in cases involving, for example, highly dimensional data. To provide the reader with understanding of the terminology, this survey introduces a novel taxonomy where a distinction between online and offline, and training and inference is made. Additionally, it presents the most popular data sets and evaluation metrics used in the literature, as well as a detailed analysis. Furthermore, this survey provides an extensive overview of the state-of-the-art model-based online semi- and unsupervised anomaly detection approaches for multivariate time-series data, categorising them into different model families and other properties. The biggest research challenge revolves around benchmarking, as currently there is no reliable way to compare different approaches against one another. This problem is two-fold: on the one hand, public data sets suffers from at least one fundamental flaw, while on the other hand, there is a lack of intuitive and representative evaluation metrics in the field. Moreover, the way most publications choose a detection threshold disregards real-world conditions, which hinders the application in the real world. To allow for tangible advances in the field, these issues must be addressed in future work.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Correia, Lucas and Goos, Jan Christoph and Klein, Philipp and Bäck, Thomas and Kononova, Anna V.},
	month = dec,
	year = {2024},
	note = {Publisher: Elsevier Ltd},
	keywords = {Time series, Multivariate, Anomaly detection, Model-based, Online, Survey},
	file = {PDF:/home/miguel/Zotero/storage/J2YA32DZ/1-s2.0-S0952197624014817-mainext.pdf:application/pdf},
}

@article{liu_efficient_2018-3,
	title = {Efficient {Outlier} {Detection} for {High}-{Dimensional} {Data}},
	volume = {48},
	issn = {21682232},
	doi = {10.1109/TSMC.2017.2718220},
	abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter \$\{k\}\$ of \$\{k\}\$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
	number = {12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Liu, Huawen and Li, Xuelong and Li, Jiuyong and Zhang, Shichao},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Dimension reduction, high-dimensional data, outlier detection, k nearest neighbors (kNN), low-rank approximation},
	pages = {2451--2461},
	file = {PDF:/home/miguel/Zotero/storage/CCCWG246/Efficient_Outlier_Detection_for_High-Dimensional_Data-2.pdf:application/pdf},
}

@article{liu_efficient_2018-4,
	title = {Efficient {Outlier} {Detection} for {High}-{Dimensional} {Data}},
	volume = {48},
	issn = {21682232},
	doi = {10.1109/TSMC.2017.2718220},
	abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter \$\{k\}\$ of \$\{k\}\$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
	number = {12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Liu, Huawen and Li, Xuelong and Li, Jiuyong and Zhang, Shichao},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Dimension reduction, high-dimensional data, outlier detection, k nearest neighbors (kNN), low-rank approximation},
	pages = {2451--2461},
	file = {PDF:/home/miguel/Zotero/storage/H46LIJIV/Efficient_Outlier_Detection_for_High-Dimensional_Data.pdf:application/pdf},
}

@article{li_ms2od_2024-3,
	title = {{MS2OD}: outlier detection using minimum spanning tree and medoid selection},
	volume = {5},
	issn = {26322153},
	doi = {10.1088/2632-2153/ad2492},
	abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
	number = {1},
	journal = {Machine Learning: Science and Technology},
	author = {Li, Jia and Li, Jiangwei and Wang, Chenxu and Verbeek, Fons J. and Schultz, Tanja and Liu, Hui},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Physics},
	keywords = {machine learning, data mining, outlier detection, clustering, medical data, medoid selection, minimum spanning tree},
	file = {PDF:/home/miguel/Zotero/storage/I9NXPWI8/Li_2024_Mach._Learn.%3A_Sci._Technol._5_015025.pdf:application/pdf},
}

@article{eiteneuer_lstm_2020,
	title = {{LSTM} for {Model}-{Based} {Anomaly} {Detection} in {Cyber}-{Physical} {Systems}},
	url = {http://arxiv.org/abs/2010.15680},
	abstract = {Anomaly detection is the task of detecting data which differs from the normal behaviour of a system in a given context. In order to approach this problem, data-driven models can be learned to predict current or future observations. Oftentimes, anomalous behaviour depends on the internal dynamics of the system and looks normal in a static context. To address this problem, the model should also operate depending on state. Long Short-Term Memory (LSTM) neural networks have been shown to be particularly useful to learn time sequences with varying length of temporal dependencies and are therefore an interesting general purpose approach to learn the behaviour of arbitrarily complex Cyber-Physical Systems. In order to perform anomaly detection, we slightly modify the standard norm 2 error to incorporate an estimate of model uncertainty. We analyse the approach on artificial and real data.},
	author = {Eiteneuer, Benedikt and Niggemann, Oliver},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.15680},
	file = {PDF:/home/miguel/Zotero/storage/FDS2V4UX/2010.15680v1.pdf:application/pdf},
}

@article{li_variational_2019-3,
	title = {Variational autoencoder-based outlier detection for high-dimensional data},
	volume = {23},
	issn = {15714128},
	doi = {10.3233/IDA-184240},
	abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
	number = {5},
	journal = {Intelligent Data Analysis},
	author = {Li, Yongmou and Wang, Yijie and Ma, Xingkong},
	year = {2019},
	note = {Publisher: IOS Press},
	keywords = {high-dimensional data, outlier detection, Variational autoencoders},
	pages = {991--1002},
	file = {PDF:/home/miguel/Zotero/storage/PPJ2J4GT/retrieve.pdf:application/pdf},
}

@article{olteanu_meta-survey_2023-2,
	title = {Meta-survey on outlier and anomaly detection},
	volume = {555},
	issn = {18728286},
	doi = {10.1016/j.neucom.2023.126634},
	abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
	journal = {Neurocomputing},
	author = {Olteanu, Madalina and Rossi, Fabrice and Yger, Florian},
	month = oct,
	year = {2023},
	note = {arXiv: 2312.07101
Publisher: Elsevier B.V.},
	keywords = {Outlier detection, Anomaly detection, Meta-survey},
	file = {PDF:/home/miguel/Zotero/storage/WJ3BLGWH/1-s2.0-S0925231223007579-main.pdf:application/pdf},
}

@article{mirzaie_state_2023-6,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/GZYG33IT/1-s2.0-S1574013723000217-main.pdf:application/pdf},
}

@article{souiden_survey_2022-5,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/PIGXBYXT/1-s2.0-S1574013722000107-main.pdf:application/pdf},
}

@article{mirzaie_state_2023-7,
	title = {State of the art on quality control for data streams: {A} systematic literature review},
	volume = {48},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2023.100554},
	abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
	journal = {Computer Science Review},
	author = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
	month = may,
	year = {2023},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Data quality, Data streams, Quality framework, Systematic literature review},
	file = {PDF:/home/miguel/Zotero/storage/4RL643PF/1-s2.0-S1574013723000217-main-2.pdf:application/pdf},
}

@article{souiden_survey_2022-6,
	title = {A survey of outlier detection in high dimensional data streams},
	volume = {44},
	issn = {15740137},
	doi = {10.1016/j.cosrev.2022.100463},
	abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
	journal = {Computer Science Review},
	author = {Souiden, Imen and Omri, Mohamed Nazih and Brahmi, Zaki},
	month = may,
	year = {2022},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Outlier detection, Data streams, High dimensional data},
	file = {PDF:/home/miguel/Zotero/storage/XWLUXU5E/1-s2.0-S1574013722000107-main-2.pdf:application/pdf},
}

@article{zhang_outlier_2024-3,
	title = {Outlier {Detection} {Using} {Three}-{Way} {Neighborhood} {Characteristic} {Regions} and {Corresponding} {Fusion} {Measurement}},
	volume = {36},
	issn = {15582191},
	doi = {10.1109/TKDE.2023.3312108},
	abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Xianyong and Yuan, Zhong and Miao, Duoqian},
	month = may,
	year = {2024},
	note = {Publisher: IEEE Computer Society},
	keywords = {Data mining, outlier detection, neighborhood rough sets, three-way decision, uncertainty measurement},
	pages = {2082--2095},
	file = {PDF:/home/miguel/Zotero/storage/CFA9S9EB/Outlier.pdf:application/pdf},
}

@inproceedings{zhang_sparx_2022-2,
	title = {Sparx: {Distributed} {Outlier} {Detection} at {Scale}},
	isbn = {978-1-4503-9385-0},
	doi = {10.1145/3534678.3539076},
	abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Sean and Ursekar, Varun and Akoglu, Leman},
	month = aug,
	year = {2022},
	note = {arXiv: 2206.01281},
	keywords = {Apache Spark, data-parallel algorithms, distributed outlier detection},
	pages = {4530--4540},
	file = {PDF:/home/miguel/Zotero/storage/26WFWJMK/Sparx- Distributed Outlier Detection at Scale.pdf:application/pdf},
}

@article{fonvieille_swimming_2023-3,
	title = {Swimming in an ocean of curves: {A} functional approach to understanding elephant seal habitat use in the {Argentine} {Basin}},
	volume = {218},
	issn = {00796611},
	doi = {10.1016/j.pocean.2023.103120},
	abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
	journal = {Progress in Oceanography},
	author = {Fonvieille, Nadège and Guinet, Christophe and Saraceno, Martin and Picard, Baptiste and Tournier, Martin and Goulet, Pauline and Campagna, Claudio and Campagna, Julieta and Nerini, David},
	month = nov,
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Model-based clustering, Brazil-Malvinas confluence, Functional Data Analysis, Habitat use, Southern elephant seals},
	file = {PDF:/home/miguel/Zotero/storage/9ZMMHXE2/1-s2.0-S0079661123001635-main.pdf:application/pdf},
}

@article{tong_model-based_2022,
	title = {Model-based clustering and outlier detection with missing data},
	volume = {16},
	issn = {18625355},
	doi = {10.1007/s11634-021-00476-1},
	abstract = {The use of the multivariate contaminated normal (MCN) distribution in model-based clustering is recommended to cluster data characterized by mild outliers, the model can at the same time detect outliers automatically and produce robust parameter estimates in each cluster. However, one of the limitations of this approach is that it requires complete data, i.e. the MCN cannot be used directly on data with missing values. In this paper, we develop a framework for fitting a mixture of MCN distributions to incomplete data sets, i.e. data sets with some values missing at random. Parameter estimation is obtained using the expectation-conditional maximization algorithm—a variant of the expectation-maximization algorithm in which the traditional maximization steps are instead replaced by simpler conditional maximization steps. We perform a simulation study to compare the results of our model to a mixture of multivariate normal and Student’s t distributions for incomplete data. The simulation also includes a study on the effect of the percentage of missing data on the performance of the three algorithms. The model is then applied to the Automobile data set (UCI machine learning repository). The results show that, while the Student’s t distribution gives similar classification performance, the MCN works better in detecting outliers with a lower false positive rate of outlier detection. The performance of all the techniques decreases linearly as the percentage of missing values increases.},
	number = {1},
	journal = {Advances in Data Analysis and Classification},
	author = {Tong, Hung and Tortora, Cristina},
	month = mar,
	year = {2022},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Outliers, Model-based clustering, Contaminated normal distribution, Data missing at random},
	pages = {5--30},
	file = {PDF:/home/miguel/Zotero/storage/MQZ47PRA/s11634-021-00476-1.pdf:application/pdf},
}

@article{pronello_penalized_2023-3,
	title = {Penalized model-based clustering of complex functional data},
	volume = {33},
	issn = {15731375},
	doi = {10.1007/s11222-023-10288-2},
	abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
	number = {6},
	journal = {Statistics and Computing},
	author = {Pronello, Nicola and Ignaccolo, Rosaria and Ippoliti, Luigi and Fontanella, Sara},
	month = dec,
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Functional zoning, Manifold data, Mixture models, Shape analysis, Spatial clustering, Surface data},
	file = {PDF:/home/miguel/Zotero/storage/XTMHU8BE/s11222-023-10288-2.pdf:application/pdf},
}

@article{claeskens_multivariate_2014,
	title = {Multivariate functional halfspace depth},
	volume = {109},
	issn = {1537274X},
	doi = {10.1080/01621459.2013.856795},
	abstract = {This article defines and studies a depth for multivariate functional data. By the multivariate nature and by including a weight function, it acknowledges important characteristics of functional data, namely differences in the amount of local amplitude, shape, and phase variation. We study both population and finite sample versions. The multivariate sample of curves may include warping functions, derivatives, and integrals of the original curves for a better overall representation of the functional data via the depth.We present a simulation study and data examples that confirm the good performance of this depth function. Supplementary materials for this article are available online. © 2014 American Statistical Association.},
	number = {505},
	journal = {Journal of the American Statistical Association},
	author = {Claeskens, Gerda and Hubert, Mia and Slaets, Leen and Vakili, Kaveh},
	year = {2014},
	note = {Publisher: American Statistical Association},
	keywords = {Functional data, Multivariate data, Statistical depth, Time warping},
	pages = {411--423},
	file = {PDF:/home/miguel/Zotero/storage/VHEVK6DU/Claeskens-MultivariateFunctionalHalfspace-2014.pdf:application/pdf},
}

@techreport{li_model-based_nodate,
	title = {Model-based analysis of oligonucleotide arrays: {Expression} index computation and outlier detection},
	url = {www.pnas.orgcgidoi10.1073pnas.011404098},
	abstract = {Recent advances in cDNA and oligonucleotide DNA arrays have made it possible to measure the abundance of mRNA transcripts for many genes simultaneously. The analysis of such experiments is nontrivial because of large data size and many levels of variation introduced at different stages of the experiments. The analysis is further complicated by the large differences that may exist among different probes used to interrogate the same gene. However, an attractive feature of high-density oligonucleotide arrays such as those produced by photolithography and inkjet technology is the standardization of chip manufacturing and hybridization process. As a result, probe-specific biases, although significant, are highly reproducible and predictable, and their adverse effect can be reduced by proper modeling and analysis methods. Here, we propose a statistical model for the probe-level data, and develop model-based estimates for gene expression indexes. We also present model-based methods for identifying and handling cross-hybridizing probes and contaminating array regions. Applications of these results will be presented elsewhere.},
	author = {Li, Cheng and Wong, Wing Hung},
	file = {PDF:/home/miguel/Zotero/storage/Z8RYAWJ4/li-wong-model-based-analysis-of-oligonucleotide-arrays-expression-index-computation-and-outlier-detection.pdf:application/pdf},
}

@article{maturo_environmental_2024-3,
	title = {Environmental {Loss} {Assessment} via {Functional} {Outlier} {Detection} of {Transformed} {Biodiversity} {Profiles}},
	issn = {15372693},
	doi = {10.1007/s13253-024-00648-4},
	abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
	journal = {Journal of Agricultural, Biological, and Environmental Statistics},
	author = {Maturo, Fabrizio and Porreca, Annamaria},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers},
	file = {PDF:/home/miguel/Zotero/storage/78JSREGV/s13253-024-00648-4.pdf:application/pdf},
}

@article{zheng_deep_2022-7,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/BHDLMHYK/1-s2.0-S1568494622004057-main-2.pdf:application/pdf},
}

@article{zheng_deep_2022-8,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/5C8GKSBD/1-s2.0-S1568494622004057-main.pdf:application/pdf},
}

@article{jimenez-varon_pointwise_2024-2,
	title = {Pointwise data depth for univariate and multivariate functional outlier detection},
	volume = {35},
	issn = {1099095X},
	doi = {10.1002/env.2851},
	abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
	number = {5},
	journal = {Environmetrics},
	author = {Jiménez-Varón, Cristian F. and Harrou, Fouzi and Sun, Ying},
	month = aug,
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {functional data, data depth, magnitude outliers, pairwise depth, pointwise depth, shape outliers, visualization},
	file = {PDF:/home/miguel/Zotero/storage/IRWS887N/Environmetrics - 2024 - Jim%C3%A9nez%E2%80%90Var%C3%B3n - Pointwise data depth for univariate and multivariate functional outlier detection.pdf:application/pdf},
}

@article{zhu_high-dimensional_2023-3,
	title = {A {High}-{Dimensional} {Outlier} {Detection} {Approach} {Based} on {Local} {Coulomb} {Force}},
	volume = {35},
	issn = {15582191},
	doi = {10.1109/TKDE.2022.3172167},
	abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Pengyun and Zhang, Chaowei and Li, Xiaofeng and Zhang, Jifu and Qin, Xiao},
	month = jun,
	year = {2023},
	note = {Publisher: IEEE Computer Society},
	keywords = {High-dimensional outlier detection, local outlier coulomb force, neighborhood outlier factor, outlier coulomb resultant force, similarity metric},
	pages = {5506--5520},
	file = {PDF:/home/miguel/Zotero/storage/3CKY6WFP/2023 Pengyun Zhu - A High Dimensional Outlier Detection Approach Base [retrieved_2025-01-01].pdf:application/pdf},
}

@article{zheng_deep_2022-9,
	title = {A deep hypersphere approach to high-dimensional anomaly detection},
	volume = {125},
	issn = {15684946},
	doi = {10.1016/j.asoc.2022.109146},
	abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
	journal = {Applied Soft Computing},
	author = {Zheng, Jian and Qu, Hongchun and Li, Zhaoni and Li, Lin and Tang, Xiaoming},
	month = aug,
	year = {2022},
	note = {Publisher: Elsevier BV},
	pages = {109146},
	file = {PDF:/home/miguel/Zotero/storage/3962EAC8/ZHENG HYPERSPHERE.pdf:application/pdf},
}

@article{li_outlier_2020-3,
	title = {Outlier {Detection} {Using} {Structural} {Scores} in a {High}-{Dimensional} {Space}},
	volume = {50},
	issn = {21682275},
	doi = {10.1109/TCYB.2018.2876615},
	abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
	number = {5},
	journal = {IEEE Transactions on Cybernetics},
	author = {Li, Xiaojie and Lv, Jiancheng and Yi, Zhang},
	month = may,
	year = {2020},
	pmid = {30418896},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {outlier detection, Discrimination, outlier factor, structural scores},
	pages = {2302--2310},
	file = {PDF:/home/miguel/Zotero/storage/SKBH6PIR/Outlier_Detection_Using_Structural_Scores_in_a_High-Dimensional_Space.pdf:application/pdf},
}

@article{todorov_r_2024-3,
	title = {The {R} {Package} {Ecosystem} for {Robust} {Statistics}},
	volume = {16},
	issn = {19390068},
	doi = {10.1002/wics.70007},
	abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
	number = {6},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	author = {Todorov, Valentin},
	month = nov,
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {robust, R, high dimensions, multivariate, outlier},
	file = {PDF:/home/miguel/Zotero/storage/HC43SK6Y/WIREs Computational Stats - 2024 - Todorov - The R Package Ecosystem for Robust Statistics.pdf:application/pdf},
}

@article{welbaum_mean_2025,
	title = {Mean shift-based clustering for misaligned functional data},
	volume = {206},
	issn = {01679473},
	doi = {10.1016/j.csda.2024.108107},
	abstract = {Misalignment often occurs in functional data and can severely impact their clustering results. A clustering algorithm for misaligned functional data is developed, by adapting the original mean shift algorithm in the Euclidean space. This mean shift algorithm is applied to the quotient space of the orbits of the square root velocity functions induced by the misaligned functional data, in which the elastic distance is equipped. Convergence properties of this algorithm are studied. The efficacy of the algorithm is demonstrated through simulations and various real data applications.},
	journal = {Computational Statistics and Data Analysis},
	author = {Welbaum, Andrew and Qiao, Wanli},
	month = jun,
	year = {2025},
	note = {Publisher: Elsevier B.V.},
	keywords = {Functional data, Clustering, Elastic distance, Gradient ascent, Mean shift, Misalignment},
	file = {PDF:/home/miguel/Zotero/storage/MJTNGY2A/1-s2.0-S0167947324001919-main.pdf:application/pdf},
}

@article{dietzel_shrinkage-based_2024-3,
	title = {Shrinkage-based {Bayesian} variable selection for species distribution modelling in complex environments: {An} application to urban biodiversity},
	volume = {81},
	issn = {15749541},
	doi = {10.1016/j.ecoinf.2024.102561},
	abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
	journal = {Ecological Informatics},
	author = {Dietzel, Andreas and Moretti, Marco and Cook, Lauren M.},
	month = jul,
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bayesian projection predictive variable selection, Blue-green infrastructure, Nature-based solutions, Shrinkage prior, Species distribution model, Urban biodiversity},
	file = {PDF:/home/miguel/Zotero/storage/I3M38PTQ/1-s2.0-S1574954124001031-main.pdf:application/pdf},
}

@techreport{du_dream_nodate-2,
	title = {Dream the {Impossible}: {Outlier} {Imagination} with {Diffusion} {Models}},
	url = {https://github.com/deeplearning-wisc/dream-ood.},
	abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
	author = {Du, Xuefeng and Sun, Yiyou and Zhu, Xiaojin and Li, Yixuan},
	file = {PDF:/home/miguel/Zotero/storage/ESJRP8E8/NeurIPS-2023-dream-the-impossible-outlier-imagination-with-diffusion-models-Paper-Conference.pdf:application/pdf},
}

@article{ruff_unifying_2020-1,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2009.11732},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Grégoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Müller, Klaus-Robert},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.11732},
	file = {PDF:/home/miguel/Zotero/storage/XBN5ANE5/2009.11732v3.pdf:application/pdf},
}

@book{becker_robustness_2013,
	title = {Robustness and complex data structures: {Festschrift} in honour of {Ursula} {Gather}},
	isbn = {978-3-642-35494-6},
	abstract = {This Festschrift in honour of Ursula Gather’s 60th birthday deals with modern topics in the field of robust statistical methods, especially for time series and regression analysis, and with statistical methods for complex data structures. The individual contributions of leading experts provide a textbook-style overview of the topic, supplemented by current research results and questions. The statistical theory and methods in this volume aim at the analysis of data which deviate from classical stringent model assumptions, which contain outlying values and/or have a complex structure. Written for researchers as well as master and PhD students with a good knowledge of statistics.},
	publisher = {Springer Berlin Heidelberg},
	author = {Becker, Claudia and Fried, Roland and Kuhnt, Sonja},
	month = jan,
	year = {2013},
	doi = {10.1007/978-3-642-35494-6},
	note = {Publication Title: Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather},
	file = {PDF:/home/miguel/Zotero/storage/3L8BIDXF/978-3-642-35494-6.pdf:application/pdf},
}

@article{porreca_identifying_2024-2,
	title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized {Hill}’s numbers and their integral functions},
	issn = {15737845},
	doi = {10.1007/s11135-024-01876-z},
	abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
	journal = {Quality and Quantity},
	author = {Porreca, Annamaria and Maturo, Fabrizio},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {FDA, Biodiversity, Diversity, Functional outlier detection, Hill’s numbers, Normalized Hill’s functions, Standardized Hill’s functions},
	file = {PDF:/home/miguel/Zotero/storage/W6U4SEN9/s11135-024-01876-z.pdf:application/pdf},
}

@techreport{salehi_unified_nodate-4,
	title = {A {Unified} {Survey} on {Anomaly}, {Novelty}, {Open}-{Set}, and {Out}-of-{Distribution} {Detection}: {Solutions} and {Future} {Challenges}},
	url = {https://github.com/taslimisina/osr-ood-ad-methods},
	abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
	author = {Salehi, Mohammadreza and Nl, Salehidehnavi@uva and Mirzaei, Hossein and Hendrycks, Dan and Li, Yixuan and Rohban, Mohammad Hossein and Sabokrou, Mohammad},
	file = {PDF:/home/miguel/Zotero/storage/QM2TWMRF/234_A_Unified_Survey_on_Anomal.pdf:application/pdf},
}

@article{ruff_unifying_2021-2,
	title = {A {Unifying} {Review} of {Deep} and {Shallow} {Anomaly} {Detection}},
	volume = {109},
	issn = {15582256},
	doi = {10.1109/JPROC.2021.3052449},
	abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gregoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Muller, Klaus Robert},
	month = may,
	year = {2021},
	note = {arXiv: 2009.11732
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {neural networks, deep learning, outlier detection, Anomaly detection (AD), explainable artificial intelligence, interpretability, kernel methods, novelty detection, one-class classification, out-of-distribution (OOD) detection, unsupervised learning.},
	pages = {756--795},
	file = {PDF:/home/miguel/Zotero/storage/BCTB883D/A_Unifying_Review_of_Deep_and_Shallow_Anomaly_Detection-2.pdf:application/pdf},
}

@book{mateu_geostatistical_2022-3,
	title = {Geostatistical functional data analysis},
	isbn = {978-1-119-38784-8},
	abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Mateu, Jorge. and Giraldo, Ramon.},
	year = {2022},
	file = {PDF:/home/miguel/Zotero/storage/94TGCFWY/Geostatistical Functional Data Analysis - 2021 - Mateu.pdf:application/pdf},
}

@article{herrmann_geometric_2023-2,
	title = {A geometric framework for outlier detection in high-dimensional data},
	volume = {13},
	issn = {19424795},
	doi = {10.1002/widm.1491},
	abstract = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption, that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under: Technologies {\textgreater} Structure Discovery and Clustering Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Visualization.},
	number = {3},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Herrmann, Moritz and Pfisterer, Florian and Scheipl, Fabian},
	month = may,
	year = {2023},
	note = {arXiv: 2207.00367
Publisher: John Wiley and Sons Inc},
	keywords = {outlier detection, anomaly detection, dimension reduction, manifold learning},
	file = {PDF:/home/miguel/Zotero/storage/M7YFYAKQ/WIREs Data Min   Knowl - 2023 - Herrmann - A geometric framework for outlier detection in high%E2%80%90dimensional data.pdf:application/pdf},
}

@article{brault_mixture_2024,
	title = {Mixture of segmentation for heterogeneous functional data},
	volume = {18},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-18/issue-2/Mixture-of-segmentation-for-heterogeneous-functional-data/10.1214/24-EJS2286.full},
	doi = {10.1214/24-EJS2286},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Brault, Vincent and Devijver, Émilie and Laclau, Charlotte},
	month = jan,
	year = {2024},
	file = {PDF:/home/miguel/Zotero/storage/IN2SUW6X/24-EJS2286.pdf:application/pdf},
}

@article{james_clustering_2003,
	title = {Clustering for sparsely sampled functional data},
	volume = {98},
	issn = {01621459},
	doi = {10.1198/016214503000189},
	abstract = {We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low-dimensional representations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite-dimensional covariates and show how it can be applied to standard finite-dimensional clustering problems involving missing data.},
	number = {462},
	journal = {Journal of the American Statistical Association},
	author = {James, Gareth M. and Sugar, Catherine A.},
	month = jun,
	year = {2003},
	keywords = {Curve estimation, High-dimensional data, Discriminant functions, Functional clustering},
	pages = {397--408},
	file = {PDF:/home/miguel/Zotero/storage/T6VSE4RT/Clustering for Sparsely Sampled Functional Data.pdf:application/pdf},
}

@article{li_fast_2020,
	title = {Fast covariance estimation for multivariate sparse functional data},
	volume = {9},
	issn = {20491573},
	doi = {10.1002/sta4.245},
	abstract = {Covariance estimation is essential yet underdeveloped for analysing multivariate functional data. We propose a fast covariance estimation method for multivariate sparse functional data using bivariate penalized splines. The tensor-product B-spline formulation of the proposed method enables a simple spectral decomposition of the associated covariance operator and explicit expressions of the resulting eigenfunctions as linear combinations of B-spline bases, thereby dramatically facilitating subsequent principal component analysis. We derive a fast algorithm for selecting the smoothing parameters in covariance smoothing using leave-one-subject-out cross-validation. The method is evaluated with extensive numerical studies and applied to an Alzheimer's disease study with multiple longitudinal outcomes.},
	number = {1},
	journal = {Stat},
	author = {Li, Cai and Xiao, Luo and Luo, Sheng},
	month = dec,
	year = {2020},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {multivariate functional data, functional principal component analysis, bivariate smoothing, covariance function, longitudinal data, prediction},
	file = {PDF:/home/miguel/Zotero/storage/UVFSPULV/STA4-9-0.pdf:application/pdf},
}

@article{chiou_multivariate_2014,
	title = {Multivariate functional principal component analysis: {A} normalization approach},
	volume = {24},
	issn = {10170405},
	doi = {10.5705/ss.2013.305},
	abstract = {We propose an extended version of the classical Karhunen-Lòeve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
	number = {4},
	journal = {Statistica Sinica},
	author = {Chiou, Jeng Min and Chen, Yu Ting and Yang, Ya Fang},
	month = oct,
	year = {2014},
	note = {Publisher: Institute of Statistical Science},
	keywords = {Multivariate functional data, Karhunen-LòEve expansion, Mercer's theorem, Normalization, Traffic flow},
	pages = {1571--1596},
	file = {PDF:/home/miguel/Zotero/storage/D5G2CGKJ/A24n45.pdf:application/pdf},
}

@article{happ_multivariate_2018,
	title = {Multivariate {Functional} {Principal} {Component} {Analysis} for {Data} {Observed} on {Different} ({Dimensional}) {Domains}},
	volume = {113},
	issn = {1537274X},
	doi = {10.1080/01621459.2016.1273115},
	abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen–Loève Theorem. For the practically relevant case of a finite Karhunen–Loève representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
	number = {522},
	journal = {Journal of the American Statistical Association},
	author = {Happ, Clara and Greven, Sonja},
	month = apr,
	year = {2018},
	note = {arXiv: 1509.02029
Publisher: American Statistical Association},
	keywords = {Multivariate functional data, Functional data analysis, Dimension reduction, Image analysis},
	pages = {649--659},
	file = {PDF:/home/miguel/Zotero/storage/KV46V3SN/Multivariate Functional Principal Component Analysis for Data Observed on Different  Dimensional  Domains.pdf:application/pdf},
}

@article{berrendero_principal_2011,
	title = {Principal components for multivariate functional data},
	volume = {55},
	issn = {01679473},
	doi = {10.1016/j.csda.2011.03.011},
	abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
	number = {9},
	journal = {Computational Statistics and Data Analysis},
	author = {Berrendero, J. R. and Justel, A. and Svarc, M.},
	month = sep,
	year = {2011},
	keywords = {Dimension reduction, Eigenvalue functions, Explained variability},
	pages = {2619--2634},
	file = {PDF:/home/miguel/Zotero/storage/IWZITQ68/1-s2.0-S0167947311001022-main.pdf:application/pdf},
}

@article{rigueira_multivariate_2025,
	title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
	url = {https://doi.org/10.5281/zenodo.1},
	doi = {10.5281/zenodo.1},
	abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93\%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
	author = {Rigueira, Xurxo and Olivieri, David and Araujo, Maria and Saavedra, Angeles and Pazo, Maria},
	year = {2025},
	file = {PDF:/home/miguel/Zotero/storage/RLSDNNFB/1-s2.0-S1364815225001276-main (1).pdf:application/pdf},
}

@article{chiou_multivariate_2014-1,
	title = {Multivariate functional principal component analysis: {A} normalization approach},
	volume = {24},
	issn = {10170405},
	doi = {10.5705/ss.2013.305},
	abstract = {We propose an extended version of the classical Karhunen-Lòeve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
	number = {4},
	journal = {Statistica Sinica},
	author = {Chiou, Jeng Min and Chen, Yu Ting and Yang, Ya Fang},
	month = oct,
	year = {2014},
	note = {Publisher: Institute of Statistical Science},
	keywords = {Multivariate functional data, Karhunen-LòEve expansion, Mercer's theorem, Normalization, Traffic flow},
	pages = {1571--1596},
	file = {PDF:/home/miguel/Zotero/storage/Q8EQACF2/A24n45.pdf:application/pdf},
}

@article{kuhnt_angle-based_2016,
	title = {An angle-based multivariate functional pseudo-depth for shape outlier detection},
	volume = {146},
	issn = {10957243},
	doi = {10.1016/j.jmva.2015.10.016},
	abstract = {A measure especially designed for detecting shape outliers in functional data is presented. It is based on the tangential angles of the intersections of the centred data and can be interpreted like a data depth. Due to its theoretical properties we call it functional tangential angle (FUNTA) pseudo-depth. Furthermore we introduce a robustification (rFUNTA). The existence of intersection angles is ensured through the centring. Assuming that shape outliers in functional data follow a different pattern, the distribution of intersection angles differs. Furthermore we formulate a population version of FUNTA in the context of Gaussian processes. We determine sample breakdown points of FUNTA and compare its performance with respect to outlier detection in simulation studies and a real data example.},
	journal = {Journal of Multivariate Analysis},
	author = {Kuhnt, Sonja and Rehage, André},
	month = apr,
	year = {2016},
	note = {Publisher: Academic Press Inc.},
	keywords = {Data depth, Functional data, Bootstrap, Robust estimate, Shape outlier detection},
	pages = {325--340},
	file = {PDF:/home/miguel/Zotero/storage/JDX83HRF/1-s2.0-S0047259X15002675-main.pdf:application/pdf},
}

@article{berrendero_principal_2011-1,
	title = {Principal components for multivariate functional data},
	volume = {55},
	issn = {01679473},
	doi = {10.1016/j.csda.2011.03.011},
	abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
	number = {9},
	journal = {Computational Statistics and Data Analysis},
	author = {Berrendero, J. R. and Justel, A. and Svarc, M.},
	month = sep,
	year = {2011},
	keywords = {Dimension reduction, Eigenvalue functions, Explained variability},
	pages = {2619--2634},
	file = {PDF:/home/miguel/Zotero/storage/TPIW6DSD/1-s2.0-S0167947311001022-main.pdf:application/pdf},
}

@article{happ_multivariate_2018-1,
	title = {Multivariate {Functional} {Principal} {Component} {Analysis} for {Data} {Observed} on {Different} ({Dimensional}) {Domains}},
	volume = {113},
	issn = {1537274X},
	doi = {10.1080/01621459.2016.1273115},
	abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen–Loève Theorem. For the practically relevant case of a finite Karhunen–Loève representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
	number = {522},
	journal = {Journal of the American Statistical Association},
	author = {Happ, Clara and Greven, Sonja},
	month = apr,
	year = {2018},
	note = {arXiv: 1509.02029
Publisher: American Statistical Association},
	keywords = {Multivariate functional data, Functional data analysis, Dimension reduction, Image analysis},
	pages = {649--659},
	file = {PDF:/home/miguel/Zotero/storage/ZZQVUFA6/Multivariate Functional Principal Component Analysis for Data Observed on Different  Dimensional  Domains.pdf:application/pdf},
}

@article{rigueira_multivariate_2025-1,
	title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
	url = {https://doi.org/10.5281/zenodo.1},
	doi = {10.5281/zenodo.1},
	abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93\%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
	author = {Rigueira, Xurxo and Olivieri, David and Araujo, Maria and Saavedra, Angeles and Pazo, Maria},
	year = {2025},
	file = {PDF:/home/miguel/Zotero/storage/SUQ9L9N8/1-s2.0-S1364815225001276-main-2.pdf:application/pdf},
}

@article{diquigiovanni_conformal_2022,
	title = {Conformal prediction bands for multivariate functional data},
	volume = {189},
	issn = {10957243},
	doi = {10.1016/j.jmva.2021.104879},
	abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.},
	journal = {Journal of Multivariate Analysis},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	month = may,
	year = {2022},
	note = {arXiv: 2106.01792
Publisher: Academic Press Inc.},
	keywords = {Functional data, Conformal Prediction, Distribution-free prediction set, Exact prediction set, Finite-sample prediction set, Prediction band},
	file = {PDF:/home/miguel/Zotero/storage/EF3F3E58/1-s2.0-S0047259X21001573-main.pdf:application/pdf},
}

@article{ojo_multivariate_2023,
	title = {Multivariate functional outlier detection using the fast massive unsupervised outlier detection indices},
	volume = {12},
	issn = {20491573},
	doi = {10.1002/sta4.567},
	abstract = {We present definitions and properties of the fast massive unsupervised outlier detection (FastMUOD) indices, used for outlier detection (OD) in functional data. FastMUOD detects outliers by computing, for each curve, an amplitude, magnitude, and shape index meant to target the corresponding types of outliers. Some methods adapting FastMUOD to outlier detection in multivariate functional data are then proposed. These include applying FastMUOD on the components of the multivariate data and using random projections. Moreover, these techniques are tested on various simulated and real multivariate functional datasets. Compared with the state of the art in multivariate functional OD, the use of random projections showed the most effective results with similar, and in some cases improved, OD performance. Based on the proportion of random projections that flag each multivariate function as an outlier, we propose a new graphical tool, the magnitude-shape-amplitude (MSA) plot, useful for visualizing the magnitude, shape and amplitude outlyingness of multivariate functional data.},
	number = {1},
	journal = {Stat},
	author = {Ojo, Oluwasegun Taiwo and Fernández Anta, Antonio and Genton, Marc G. and Lillo, Rosa E.},
	month = jan,
	year = {2023},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {functional data, multivariate functional data, FastMUOD, functional outlier detection, outlier classification, video data},
	file = {PDF:/home/miguel/Zotero/storage/RUF4MZXE/Stat - 2023 - Ojo - Multivariate functional outlier detection using the fast massive unsupervised outlier detection indices.pdf:application/pdf},
}

@article{elias_integrated_2023,
	title = {Integrated {Depths} for {Partially} {Observed} {Functional} {Data}},
	volume = {32},
	issn = {15372715},
	doi = {10.1080/10618600.2022.2070171},
	abstract = {Partially observed functional data are frequently encountered in applications and are the object of an increasing interest by the literature. We here address the problem of measuring the centrality of a datum in a partially observed functional sample. We propose an integrated functional depth for partially observed functional data, dealing with the very challenging case where partial observability can occur systematically on any observation of the functional dataset. In particular, differently from many techniques for partially observed functional data, we do not request that some functional datum is fully observed, nor we require that a common domain exist, where all of the functional data are recorded. Because of this, our proposal can also be used in those frequent situations where reconstructions methods and other techniques for partially observed functional data are inapplicable. By means of simulation studies, we demonstrate the very good performances of the proposed depth on finite samples. Our proposal enables the use of benchmark methods based on depths, originally introduced for fully observed data, in the case of partially observed functional data. This includes the functional boxplot, the outliergram and the depth versus depth classifiers. We illustrate our proposal on two case studies, the first concerning a problem of outlier detection in German electricity supply functions, the second regarding a classification problem with data obtained from medical imaging. Supplementary materials for this article are available online.},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Elías, Antonio and Jiménez, Raúl and Paganoni, Anna M. and Sangalli, Laura M.},
	year = {2023},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Robustness, Functional depth, Functional boxplot, Functional outliers, Classification of partially observed functional data, Incomplete functional data},
	pages = {341--352},
	file = {PDF:/home/miguel/Zotero/storage/DZCAW58X/2023 Integrated Depths for Partially Observed Functiona [retrieved_2025-06-27].pdf:application/pdf},
}

@article{cuevas_robust_2007,
	title = {Robust estimation and classification for functional data via projection-based depth notions},
	volume = {22},
	issn = {09434062},
	doi = {10.1007/s00180-007-0053-0},
	abstract = {Five notions of data depth are considered. They are mostly designed for functional data but they can be also adapted to the standard multivariate case. The performance of these depth notions, when used as auxiliary tools in estimation and classification, is checked through a Monte Carlo study. © 2007 Springer-Verlag.},
	number = {3},
	journal = {Computational Statistics},
	author = {Cuevas, Antonio and Febrero, Manuel and Fraiman, Ricardo},
	month = sep,
	year = {2007},
	keywords = {Depth measures, Functional data, Projections method, Supervised classification},
	pages = {481--496},
	file = {PDF:/home/miguel/Zotero/storage/HZJYIFN6/Robust_estimation_and_classifi.pdf:application/pdf},
}

@book{ferraty_nonparametric_2006-1,
	address = {Berlin},
	title = {Nonparametric {Functional} {Data} {Analysis}},
	publisher = {Springer},
	author = {Ferraty, Frédéric and Vieu, Philippe},
	year = {2006},
	file = {PDF:/home/miguel/Zotero/storage/J89J5W34/0-387-36620-2.pdf:application/pdf},
}

@article{yeon_regularized_2024,
	title = {Regularized {Halfspace} {Depth} for {Functional} {Data}},
	url = {http://arxiv.org/abs/2311.07034},
	abstract = {Data depth is a powerful nonparametric tool originally proposed to rank multivariate data from center outward. In this context, one of the most archetypical depth notions is Tukey's halfspace depth. In the last few decades notions of depth have also been proposed for functional data. However, Tukey's depth cannot be extended to handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at center, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in \$L{\textasciicircum}2\$, a very general space of functions that include non-smooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in terms of detecting outliers of different types. Three real data examples showcase the proposed depth notion.},
	author = {Yeon, Hyemin and Dai, Xiongtao and Lopez-Pintado, Sara},
	month = may,
	year = {2024},
	note = {arXiv: 2311.07034},
	file = {PDF:/home/miguel/Zotero/storage/R7WWDXTN/2311.07034v2.pdf:application/pdf},
}

@article{gijbels_general_2017,
	title = {On a general definition of depth for functional data},
	volume = {32},
	issn = {08834237},
	doi = {10.1214/17-STS625},
	abstract = {In this paper, we provide an elaboration on the desirable properties of statistical depths for functional data. Although a formal definition has been put forward in the literature, there are still several unclarities to be tackled, and further insights to be gained. Herein, a few interesting connections between the wanted properties are found. In particular, it is demonstrated that the conditions needed for some desirable properties to hold are extremely demanding, and virtually impossible to be met for common depths. We establish adaptations of these properties which prove to be still sensible, and more easily met by common functional depths.},
	number = {4},
	journal = {Statistical Science},
	author = {Gijbels, Irène and Nagy, Stanislav},
	month = nov,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Data depth, Robustness, Functional data, Multivariate statistics},
	pages = {630--639},
	file = {PDF:/home/miguel/Zotero/storage/AFAJTJ9J/17-STS625.pdf:application/pdf},
}

@article{nagy_depth-based_2017,
	title = {Depth-{Based} {Recognition} of {Shape} {Outlying} {Functions}},
	volume = {26},
	issn = {15372715},
	doi = {10.1080/10618600.2017.1336445},
	abstract = {A major drawback of many established depth functionals is their ineffectiveness in identifying functions outlying merely in shape. Herein, a simple modification of functional depth is proposed to provide a remedy for this difficulty. The modification is versatile, widely applicable, and introduced without imposing any assumptions on the data, such as differentiability. It is shown that many favorable attributes of the original depths for functions, including consistency properties, remain preserved for the modified depths. The powerfulness of the new approach is demonstrated on a number of examples for which the known depths fail to identify the outlying functions. Supplementary material for this article is available online.},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Nagy, Stanislav and Gijbels, Irène and Hlubinka, Daniel},
	month = oct,
	year = {2017},
	note = {Publisher: American Statistical Association},
	keywords = {Data depth, Functional data, Shape outliers, Infimal depth, Integrated depth, Outlying functions},
	pages = {883--893},
	file = {PDF:/home/miguel/Zotero/storage/B8KC2JPK/Depth-Based Recognition of Shape Outlying Functions.pdf:application/pdf},
}

@article{nieto-reyes_topologically_2016,
	title = {A topologically valid definition of depth for functional data},
	volume = {31},
	issn = {08834237},
	doi = {10.1214/15-STS532},
	abstract = {The main focus of this work is on providing a formal definition of statistical depth for functional data on the basis of six properties, recognising topological features such as continuity, smoothness and contiguity. Amongst our depth defining properties is one that addresses the delicate challenge of inherent partial observability of functional data, with fulfillment giving rise to a minimal guarantee on the performance of the empirical depth beyond the idealised and practically infeasible case of full observability. As an incidental product, functional depths satisfying our definition achieve a robustness that is commonly ascribed to depth, despite the absence of a formal guarantee in the multivariate definition of depth. We demonstrate the fulfillment or otherwise of our properties for six widely used functional depth proposals, thereby providing a systematic basis for selection of a depth function.},
	number = {1},
	journal = {Statistical Science},
	author = {Nieto-Reyes, Alicia and Battey, Heather},
	year = {2016},
	note = {arXiv: 1410.5686
Publisher: Institute of Mathematical Statistics},
	keywords = {Robustness, Functional data, Statistical depth, Multivariate statistics, Partial observability},
	pages = {61--79},
	file = {PDF:/home/miguel/Zotero/storage/DAK8M9AY/24780833.pdf:application/pdf},
}

@article{nagy_integrated_2016,
	title = {Integrated depth for functional data: {Statistical} properties and consistency},
	volume = {20},
	issn = {12623318},
	doi = {10.1051/ps/2016005},
	abstract = {Several depths suitable for infinite-dimensional functional data that are available in the literature are of the form of an integral of a finite-dimensional depth function. These functionals are characterized by projecting functions into low-dimensional spaces, taking finite-dimensional depths of the projected quantities, and finally integrating these projected marginal depths over a preset collection of projections. In this paper, a general class of integrated depths for functions is considered. Several depths for functional data proposed in the literature during the last decades are members of this general class. A comprehensive study of its most important theoretical properties, including measurability and consistency, is given. It is shown that many, but not all, properties of the integrated depth are shared with the finite-dimensional depth that constitutes its building block. Some pending measurability issues connected with all integrated depth functionals are resolved, a broad new notion of symmetry for functional data is proposed, and difficulties with respect to consistency results are identified. A general universal consistency result for the sample depth version, and for the generalized median, for integrated depth for functions is derived.},
	journal = {ESAIM - Probability and Statistics},
	author = {Nagy, Stanislav and Gijbels, Irène and Omelka, Marek and Hlubinka, Daniel},
	year = {2016},
	note = {Publisher: EDP Sciences},
	keywords = {Functional data, Integrated depth, Center of symmetry, Generalized median, Measurability, Strong consistency, Weak consistency},
	pages = {95--130},
	file = {PDF:/home/miguel/Zotero/storage/XD5LE5P5/ps160005.pdf:application/pdf},
}

@article{yeon_regularized_2025,
	title = {Regularized halfspace depth for functional data},
	issn = {1369-7412},
	doi = {10.1093/jrsssb/qkaf030},
	abstract = {Data depth is a powerful tool originally proposed to rank multivariate data from centre outward. In this context, one of the most archetypical depth notions is Tukey’s halfspace depth. In the last few decades, notions of depth have also been proposed for functional data. However, a naive extension of Tukey’s depth cannot handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data, which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at centre, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in the space of all square-integrable functions, a very general space of functions that include nonsmooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in detecting outliers of different types. Real data examples showcase the proposed depth.},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Yeon, Hyemin and Dai, Xiongtao and Lopez-Pintado, Sara},
	month = jun,
	year = {2025},
	note = {Publisher: Oxford University Press (OUP)},
	file = {PDF:/home/miguel/Zotero/storage/HEDEJ9GT/qkaf030.pdf:application/pdf},
}

@article{cuevas_depth_2009,
	title = {On depth measures and dual statistics. {A} methodology for dealing with general data},
	volume = {100},
	issn = {0047259X},
	doi = {10.1016/j.jmva.2008.08.002},
	abstract = {A general depth measure, based on the use of one-dimensional linear continuous projections, is proposed. The applicability of this idea in different statistical setups (including inference in functional data analysis, image analysis and classification) is discussed. A special emphasis is made on the possible usefulness of this method in some statistical problems where the data are elements of a Banach space. The asymptotic properties of the empirical approximation of the proposed depth measure are investigated. In particular, its asymptotic distribution is obtained through U-statistics techniques. The practical aspects of these ideas are discussed through a small simulation study and a real-data example. © 2008 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Multivariate Analysis},
	author = {Cuevas, Antonio and Fraiman, Ricardo},
	month = apr,
	year = {2009},
	keywords = {Depth measures, Functional data, Projections method, Supervised classification, secondary, 62G07, 62G20, primary},
	pages = {753--766},
	file = {PDF:/home/miguel/Zotero/storage/INFY35NR/1-s2.0-S0047259X08001796-mainext.pdf:application/pdf},
}

@article{mosler_general_2018,
	title = {General notions of depth for functional data},
	url = {http://arxiv.org/abs/1208.1981},
	abstract = {A data depth measures the centrality of a point with respect to an empirical distribution. Postulates are formulated, which a depth for functional data should satisfy, and a general approach is proposed to construct multivariate data depths in Banach spaces. The new approach, mentioned as Phi-depth, is based on depth infima over a proper set Phi of R{\textasciicircum}d-valued linear functions. Several desirable properties are established for the Phi-depth and a generalized version of it. The general notions include many new depths as special cases. In particular a location-slope depth and a principal component depth are introduced.},
	author = {Mosler, Karl and Polyakova, Yulia},
	month = jan,
	year = {2018},
	note = {arXiv: 1208.1981},
	file = {PDF:/home/miguel/Zotero/storage/IPBX7DR3/1208.1981v3.pdf:application/pdf},
}

@article{cuevas_depth_2009-1,
	title = {On depth measures and dual statistics. {A} methodology for dealing with general data},
	volume = {100},
	issn = {0047259X},
	doi = {10.1016/j.jmva.2008.08.002},
	abstract = {A general depth measure, based on the use of one-dimensional linear continuous projections, is proposed. The applicability of this idea in different statistical setups (including inference in functional data analysis, image analysis and classification) is discussed. A special emphasis is made on the possible usefulness of this method in some statistical problems where the data are elements of a Banach space. The asymptotic properties of the empirical approximation of the proposed depth measure are investigated. In particular, its asymptotic distribution is obtained through U-statistics techniques. The practical aspects of these ideas are discussed through a small simulation study and a real-data example. © 2008 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Multivariate Analysis},
	author = {Cuevas, Antonio and Fraiman, Ricardo},
	month = apr,
	year = {2009},
	keywords = {Depth measures, Functional data, Projections method, Supervised classification, secondary, 62G07, 62G20, primary},
	pages = {753--766},
	file = {PDF:/home/miguel/Zotero/storage/KQCSUADW/1-s2.0-S0047259X08001796-mainext.pdf:application/pdf},
}

@article{ramsay_integrated_2019,
	title = {Integrated rank-weighted depth},
	volume = {173},
	issn = {10957243},
	doi = {10.1016/j.jmva.2019.02.001},
	abstract = {We study depth measures for multivariate data defined by integrating univariate depth measures, specifically, integrated dual (ID) depth introduced by Cuevas and Fraiman (2009) which integrates univariate simplicial depth, and integrated rank-weighted (IRW) depth, which integrates univariate Tukey depth. We build on the results of Cuevas and Fraiman (2009) to show that IRW depth shares many depth properties with ID depth. Further, we provide additional results on exact computation, decreasing along rays, continuity and breakdown point that apply to both ID and IRW depth. We also establish asymptotic normality and consistency of the sample IRW depths. Lastly, we demonstrate the use of this depth measure with real and simulated datasets: calculating robust location estimators and dd-plots.},
	journal = {Journal of Multivariate Analysis},
	author = {Ramsay, Kelly and Durocher, Stéphane and Leblanc, Alexandre},
	month = sep,
	year = {2019},
	note = {Publisher: Academic Press Inc.},
	keywords = {dd-plots, High-dimensional data depth, Integrated dual depth},
	pages = {51--69},
	file = {PDF:/home/miguel/Zotero/storage/YZGS7X5N/1-s2.0-S0047259X18304068-main.pdf:application/pdf},
}

@article{bai_filling_2020,
	title = {Filling the gaps of in situ hourly {PM2}.5 concentration data with the aid of empirical orthogonal function analysis constrained by diurnal cycles},
	volume = {13},
	issn = {18678548},
	doi = {10.5194/amt-13-1213-2020},
	abstract = {Data gaps in surface air quality measurements significantly impair the data quality and the exploration of these valuable data sources. In this study, a novel yet practical method called diurnal-cycle-constrained empirical orthogonal function (DCCEOF) was developed to fill in data gaps present in data records with evident temporal variability. The hourly PM2:5 concentration data retrieved from the national ambient air quality monitoring network in China were used as a demonstration. The DCCEOF method aims to reconstruct the diurnal cycle of PM2:5 concentration from its discrete neighborhood field in space and time firstly and then predict the missing values by calibrating the reconstructed diurnal cycle to the level of valid PM2:5 concentrations observed at adjacent times. The statistical results indicate a high frequency of data gaps in our retrieved hourly PM2:5 concentration record, with PM2:5 concentration measured on about 40 \% of the days suffering from data gaps. Further sensitivity analysis results reveal that data gaps in the hourly PM2:5 concentration record may introduce significant bias to its daily averages, especially during clean episodes at which PM2:5 daily averages are observed to be subject to larger uncertainties compared to the polluted days (even in the presence of the same amount of missingness). The cross-validation results indicate that our suggested DCCEOF method has a good prediction accuracy, particularly in predicting daily peaks and/or minima that cannot be restored by conventional interpolation approaches, thus confirming the effectiveness of the consideration of the local diurnal variation pattern in gap filling. By applying the DCCEOF method to the hourly PM2:5 concentration record measured in China from 2014 to 2019, the data completeness ratio was substantially improved while the frequency of days with gapped PM2:5 records reduced from 42.6 \% to 5.7 \%. In general, our DCCEOF method provides a practical yet effective approach to handle data gaps in time series of geophysical parameters with significant diurnal variability, and this method is also transferable to other data sets with similar barriers because of its self-consistent capability.},
	number = {3},
	journal = {Atmospheric Measurement Techniques},
	author = {Bai, Kaixu and Li, Ke and Guo, Jianping and Yang, Yuanjian and Chang, Ni Bin},
	month = mar,
	year = {2020},
	note = {Publisher: Copernicus GmbH},
	pages = {1213--1226},
	file = {PDF:/home/miguel/Zotero/storage/XBWRXDCC/amt-13-1213-2020.pdf:application/pdf},
}

@article{correia_online_2024-1,
	title = {Online model-based anomaly detection in multivariate time series: {Taxonomy}, survey, research challenges and future directions},
	volume = {138},
	issn = {09521976},
	doi = {10.1016/j.engappai.2024.109323},
	abstract = {Time-series anomaly detection plays an important role in engineering processes, like development, manufacturing and other operations involving dynamic systems. These processes can greatly benefit from advances in the field, as state-of-the-art approaches may aid in cases involving, for example, highly dimensional data. To provide the reader with understanding of the terminology, this survey introduces a novel taxonomy where a distinction between online and offline, and training and inference is made. Additionally, it presents the most popular data sets and evaluation metrics used in the literature, as well as a detailed analysis. Furthermore, this survey provides an extensive overview of the state-of-the-art model-based online semi- and unsupervised anomaly detection approaches for multivariate time-series data, categorising them into different model families and other properties. The biggest research challenge revolves around benchmarking, as currently there is no reliable way to compare different approaches against one another. This problem is two-fold: on the one hand, public data sets suffers from at least one fundamental flaw, while on the other hand, there is a lack of intuitive and representative evaluation metrics in the field. Moreover, the way most publications choose a detection threshold disregards real-world conditions, which hinders the application in the real world. To allow for tangible advances in the field, these issues must be addressed in future work.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Correia, Lucas and Goos, Jan Christoph and Klein, Philipp and Bäck, Thomas and Kononova, Anna V.},
	month = dec,
	year = {2024},
	note = {Publisher: Elsevier Ltd},
	keywords = {Time series, Multivariate, Anomaly detection, Model-based, Online, Survey},
	file = {PDF:/home/miguel/Zotero/storage/ULADBZA7/1-s2.0-S0952197624014817-mainext.pdf:application/pdf},
}

@article{arribas-gil_shape_2014,
	title = {Shape outlier detection and visualization for functional data: {The} outliergram},
	volume = {15},
	issn = {14684357},
	doi = {10.1093/biostatistics/kxu006},
	abstract = {We propose a new method to visualize and detect shape outliers in samples of curves. In functional data analysis, we observe curves defined over a given real interval and shape outliers may be defined as those curves that exhibit a different shape from the rest of the sample. Whereas magnitude outliers, that is, curves that lie outside the range of the majority of the data, are in general easy to identify, shape outliers are often masked among the rest of the curves and thus difficult to detect. In this article, we exploit the relationship between two measures of depth for functional data to help to visualize curves in terms of shape and to develop an algorithm for shape outlier detection. We illustrate the use of the visualization tool, the outliergram, through several examples and analyze the performance of the algorithm on a simulation study. Finally, we apply our method to assess cluster quality in a real set of time course microarray data.},
	number = {4},
	journal = {Biostatistics},
	author = {Arribas-Gil, Ana and Romo, Juan},
	month = jul,
	year = {2014},
	pmid = {24622037},
	note = {arXiv: 1306.1718
Publisher: Oxford University Press},
	keywords = {Depth for functional data, Outlier visualization, Robust estimation, Time course microarray data},
	pages = {603--619},
	file = {PDF:/home/miguel/Zotero/storage/GNSMDHDK/kxu006.pdf:application/pdf},
}

@article{sun_adjusted_2012,
	title = {Adjusted functional boxplots for spatio-temporal data visualization and outlier detection},
	volume = {23},
	issn = {11804009},
	doi = {10.1002/env.1136},
	abstract = {This article proposes a simulation-based method to adjust functional boxplots for correlations when visualizing functional and spatio-temporal data, as well as detecting outliers. We start by investigating the relationship between the spatio-temporal dependence and the 1.5 times the 50\% central region empirical outlier detection rule. Then, we propose to simulate observations without outliers on the basis of a robust estimator of the covariance function of the data. We select the constant factor in the functional boxplot to control the probability of correctly detecting no outliers. Finally, we apply the selected factor to the functional boxplot of the original data. As applications, the factor selection procedure and the adjusted functional boxplots are demonstrated on sea surface temperatures, spatio-temporal precipitation and general circulation model (GCM) data. The outlier detection performance is also compared before and after the factor adjustment. © 2011 John Wiley \& Sons, Ltd.},
	number = {1},
	journal = {Environmetrics},
	author = {Sun, Ying and Genton, Marc G.},
	month = feb,
	year = {2012},
	keywords = {Outlier detection, Functional data, Precipitation data, GCM data, Robust covariance, Spatio-temporal data},
	pages = {54--64},
	file = {PDF:/home/miguel/Zotero/storage/GW6VGB93/Environmetrics - 2011 - Sun - Adjusted functional boxplots for spatio%E2%80%90temporal data visualization and outlier detection.pdf:application/pdf},
}

@techreport{berrendero_mahalanobis_2020,
	title = {On {Mahalanobis} {Distance} in {Functional} {Settings}},
	url = {http://jmlr.org/papers/v21/18-156.html.},
	abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.},
	author = {Berrendero, José R and Bueno-Larraz, Beatriz and Cuevas, Antonio},
	year = {2020},
	note = {Publication Title: Journal of Machine Learning Research
Volume: 21},
	keywords = {Functional data, kernel methods in statistics, Mahalanobis distance, reproducing kernel Hilbert spaces, square root operator},
	pages = {1--33},
	file = {PDF:/home/miguel/Zotero/storage/EIAKCLUL/18-156.pdf:application/pdf},
}

@article{rousseeuw_unmasking_nodate,
	title = {Unmasking {Multivariate} {Outliers} and {Leverage} {Points}},
	language = {en},
	author = {Rousseeuw, Peter J and Zomeren, Bert C Van},
	file = {PDF:/home/miguel/Zotero/storage/PYVUUU98/Rousseeuw and Zomeren - Unmasking Multivariate Outliers and Leverage Points.pdf:application/pdf},
}

@article{albert-smet_band_2023,
	title = {Band depth based initialization of {K}-means for functional data clustering},
	volume = {17},
	issn = {1862-5347, 1862-5355},
	url = {https://link.springer.com/10.1007/s11634-022-00510-w},
	doi = {10.1007/s11634-022-00510-w},
	abstract = {The k-Means algorithm is one of the most popular choices for clustering data but is well-known to be sensitive to the initialization process. There is a substantial number of methods that aim at ﬁnding optimal initial seeds for k-Means, though none of them is universally valid. This paper presents an extension to longitudinal data of one of such methods, the BRIk algorithm, that relies on clustering a set of centroids derived from bootstrap replicates of the data and on the use of the versatile Modiﬁed Band Depth. In our approach we improve the BRIk method by adding a step where we ﬁt appropriate B-splines to our observations and a resampling process that allows computational feasibility and handling issues such as noise or missing data. We have derived two techniques for providing suitable initial seeds, each of them stressing respectively the multivariate or the functional nature of the data. Our results with simulated and real data sets indicate that our Functional Data Approach to the BRIK method (FABRIk) and our Functional Data Extension of the BRIK method (FDEBRIk) are more effective than previous proposals at providing seeds to initialize k-Means in terms of clustering recovery.},
	language = {en},
	number = {2},
	urldate = {2025-09-17},
	journal = {Advances in Data Analysis and Classification},
	author = {Albert-Smet, Javier and Torrente, Aurora and Romo, Juan},
	month = jun,
	year = {2023},
	pages = {463--484},
	file = {PDF:/home/miguel/Zotero/storage/RQYTCLKH/Albert-Smet et al. - 2023 - Band depth based initialization of K-means for functional data clustering.pdf:application/pdf},
}

@article{yu_distance-based_2025,
	title = {Distance-based {Clustering} of {Functional} {Data} with {Derivative} {Principal} {Component} {Analysis}},
	volume = {34},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2024.2366499},
	doi = {10.1080/10618600.2024.2366499},
	abstract = {Functional data analysis (FDA) is an important modern paradigm for handling infinite-dimensional data. An important task in FDA is clustering, which identifies subgroups based on the shapes of measured curves. Considering that derivatives can provide additional useful information about the shapes of functionals, we propose a novel L2 distance between two random functions by incorporating the functions and their derivative information to determine the dissimilarity of curves under a unified scheme for dense observations. The Karhunen–Loève expansion is used to approximate the curves and their derivatives. Cluster membership prediction for each curve intends to minimize the new distances between the observed and predicted curves through subspace projection among all possible clusters. We provide consistent estimators for the curves, curve derivatives, and the proposed distance. Identifiability issues of the clustering procedure are also discussed. The utility of the proposed method is illustrated via simulation studies and applications to two real datasets. The proposed method can considerably improve cluster performance compared with existing functional clustering methods. Supplementary materials for the article are available online.},
	language = {en},
	number = {1},
	urldate = {2025-09-17},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Yu, Ping and Shi, Gongming and Wang, Chunjie and Song, Xinyuan},
	month = jan,
	year = {2025},
	pages = {47--58},
	file = {PDF:/home/miguel/Zotero/storage/LVSXHR2I/Yu et al. - 2025 - Distance-based Clustering of Functional Data with Derivative Principal Component Analysis.pdf:application/pdf},
}

@article{zhang_review_2023,
	title = {Review of {Clustering} {Methods} for {Functional} {Data}},
	volume = {17},
	issn = {1556-4681, 1556-472X},
	url = {https://dl.acm.org/doi/10.1145/3581789},
	doi = {10.1145/3581789},
	abstract = {Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.},
	language = {en},
	number = {7},
	urldate = {2025-09-17},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Zhang, Mimi and Parnell, Andrew},
	month = aug,
	year = {2023},
	pages = {1--34},
	file = {PDF:/home/miguel/Zotero/storage/VR3TU9LS/Zhang and Parnell - 2023 - Review of Clustering Methods for Functional Data.pdf:application/pdf},
}

@article{wang_review_nodate-1,
	title = {Review of functional data analysis},
	abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. They are both examples of “functional data”, which have become a commonly encountered type of data. Functional Data Analysis (FDA) encompasses the statistical methodology for such data. Broadly interpreted, FDA deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of FDA, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is Functional Principal Component Analysis (FPCA). FPCA is an important dimension reduction tool and in sparse data situations can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classiﬁcation of functional data. Beyond linear and single or multiple index methods we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models, and models that feature time warping, manifold learning, and empirical diﬀerential equations. The paper concludes with a brief discussion of future directions.},
	language = {en},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Muller, Hans-Georg},
	file = {PDF:/home/miguel/Zotero/storage/3TJTJESQ/Wang et al. - Review of functional data analysis.pdf:application/pdf},
}
