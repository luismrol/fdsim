@article{Febrero2008,
   abstract = {This paper analyzes outlier detection for functional data by means of functional depths, which measures the centrality of a given curve within a group of trajectories providing center-outward orderings of the set of curves. We give some insights of the usefulness of looking for outliers in functional datasets and propose a method based in depths for the functional outlier detection. The performance of the proposed procedure is analyzed by several Monte Carlo experiments. Finally, we illustrate the procedure by finding outliers in a dataset of NOx (nitrogen oxides) emissions taken from a control station near an industrial area. Copyright © 2007 John Wiley & Sons, Ltd.},
   author = {Manuel Febrero and Pedro Galeano and Wenceslao González-Manteiga},
   doi = {10.1002/env.878},
   issn = {11804009},
   issue = {4},
   journal = {Environmetrics},
   keywords = {Depths,Functional median,Functional trimmed mean,Nitrogen oxides,Outliers,Smoothed bootstrap},
   pages = {331-345},
   title = {Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels},
   volume = {19},
   year = {2008}
}
@article{Ieva2013,
   abstract = {In this article, we address the problem of mining and analyzing multivariate functional data. That is, data where each observation is a set of possibly correlated functions. Complex data of this kind is more and more common in many research fields, particularly in the biomedical context. In this work, we propose and apply a new concept of depth measure for multivariate functional data. With this new depth measure it is possible to generalize robust statistics, such as the median, to the multivariate functional framework, which in turn allows the application of outlier detection, boxplots construction, and nonparametric tests also in this more general framework. We present an application to Electrocardiographic (ECG) signals. Copyright © Taylor & Francis Group, LLC.},
   author = {Francesca Ieva and Anna M. Paganoni},
   doi = {10.1080/03610926.2012.746368},
   issn = {03610926},
   issue = {7},
   journal = {Communications in Statistics - Theory and Methods},
   keywords = {Depth measures,ECG signals,Multivariate functional data,Rank tests},
   pages = {1265-1276},
   title = {Depth measures for multivariate functional data},
   volume = {42},
   year = {2013}
}
@article{Ieva2017,
   abstract = {We propose a new method for detecting outliers in multivariate functional data. We exploit the joint use of two different depth measures, and generalize the outliergram to the multivariate functional framework, aiming at detecting and discarding both shape and magnitude outliers. The main application consists in robustifying the reference samples of data, composed by G different known groups to be used, for example, in classification procedures in order to make them more robust. We asses by means of a simulation study the method’s performance in comparison with different outlier detection methods. Finally we consider a real dataset: we classify data minimizing a suitable distance from the center of reference groups. We compare performance of supervised classification on test sets training the algorithm on original dataset and on the robustified one, respectively.},
   author = {Francesca Ieva and Anna Maria Paganoni},
   doi = {10.1007/s00362-017-0953-1},
   issn = {09325026},
   journal = {Statistical Papers},
   pages = {1-20},
   publisher = {Springer Berlin Heidelberg},
   title = {Component-wise outlier detection methods for robustifying multivariate functional samples},
   year = {2017}
}
@article{Li2012,
   abstract = {Using the DD-plot (depth vs. depth plot), we introduce a new nonparametric classification algorithm and call it DD-classifier. The algorithm is completely nonparametric, and it requires no prior knowledge of the underlying distributions or the form of the separating curve. Thus, it can be applied to a wide range of classification problems. The algorithm is completely data driven and its classification outcome can be easily visualized in a two-dimensional plot regardless of the dimension of the data. Moreover, it has the advantage of bypassing the estimation of underlying parameters such as means and scales, which is often required by the existing classification procedures. We study the asymptotic properties of the DD-classifier and its misclassification rate. Specifically, we show that DD-classifier is asymptotically equivalent to the Bayes rule under suitable conditions, and it can achieve Bayes error for a family broader than elliptical distributions. The performance of the classifier is also examined using simulated and real datasets. Overall, the DD-classifier performs well across a broad range of settings, and compares favorably with existing classifiers. It can also be robust against outliers or contamination. © 2012 American Statistical Association.},
   author = {Jun Li and Juan A. Cuesta-Albertos and Regina Y. Liu},
   doi = {10.1080/01621459.2012.688462},
   issn = {01621459},
   issue = {498},
   journal = {Journal of the American Statistical Association},
   keywords = {Classification,DD-classifier,DD-plot,Data depth,Maximum depth classifier,Misclassification rates,Nonparametric,Robustness},
   pages = {737-753},
   title = {DD-classifier: Nonparametric classification procedure based on DD-plot},
   volume = {107},
   year = {2012}
}
@article{Liu1999,
   abstract = {A data depth can be used to measure the "depth" or "outlyingness" of a given multivariate sample with respect to its underlying distribution. This leads to a natural center-outward ordering of the sample points. Based on this ordering, quantitative and graphical methods are introduced for analyzing multivariate distributional characteristics such as location, scale, bias, skewness and kurtosis, as well as for comparing inference methods. All graphs are one-dimensional curves in the plane and can be easily visualized and interpreted. A "sunburst plot" is presented as a bivariate generalization of the box-plot. DD-(depth versus depth) plots are proposed and examined as graphical inference tools. Some new diagnostic tools for checking multivariate normality are introduced. One of them monitors the exact rate of growth of the maximum deviation from the mean, while the others examine the ratio of the overall dispersion to the dispersion of a certain central region. The affine invariance property of a data depth also leads to appropriate invariance properties for the proposed statistics and methods.},
   author = {Regina Y. Liu and Jesse M. Parelius and Kesar Singh},
   doi = {10.2307/120138},
   issn = {00905364},
   issue = {3},
   journal = {Annals of Statistics},
   keywords = {Bias,DD-plots,Data depth,Depth ordering,Depth-L-statistics,Kurtosis,Location,Multivariate descriptive statistics,Multivariate normality,Multivariate ordering,Scale,Skewness,Sunburst plots},
   pages = {783-858},
   title = {Multivariate analysis by data depth: Descriptive statistics, graphics and inference},
   volume = {27},
   year = {1999}
}
@article{Zuo2000,
   author = {Yijun Zuo and Robert Serfling},
   issue = {2},
   journal = {Statistics},
   pages = {461-482},
   title = {General Notions of Statistical Depth Function},
   volume = {28},
   year = {2000}
}
@article{,
   title = {Another approach to polychotomous classification}
}
@inbook{Jiang2017,
   author = {Qing Jiang and Simos G. Meintanis and Lixing Zhu},
   doi = {10.1007/978-3-319-55846-2},
   isbn = {978-3-319-55845-5},
   booktitle = {Functional Statistics and Related Fields},
   keywords = {empirical characteristic function,functional data,sample problem,two},
   pages = {145-154},
   title = {Two-sample test for multivariate functional data},
   url = {http://link.springer.com/10.1007/978-3-319-55846-2},
   year = {2017}
}
@article{Flores2018a,
   abstract = {In the context of functional data analysis, we propose new sample tests for homogeneity. Based on some well-known depth measures, we construct four different statistics in order to measure distance between the two samples. A simulation study is performed to check the efficiency of the tests when confronted with shape and magnitude perturbation. Finally, we apply these tools to measure the homogeneity in some samples of real data, and we obtain good results using this new method.},
   author = {Ramón Flores and Rosa Lillo and Juan Romo},
   doi = {10.1080/02664763.2017.1319470},
   issn = {13600532},
   issue = {5},
   journal = {Journal of Applied Statistics},
   keywords = {FDA,Functional depth,distance,homogeneity},
   pages = {868-883},
   title = {Homogeneity test for functional data},
   volume = {45},
   year = {2018}
}
@article{Nagy2016,
   author = {Stanislav Nagy},
   issue = {October},
   keywords = {phd, thesis, dissertation, doctoraat, kuleuven, St},
   title = {Statistical Depth for Functional Data},
   year = {2016}
}
@book{Ramsay2005,
   author = {J.O Ramsay and B.W. Silverman},
   isbn = {1-4419-0319-8},
   publisher = {Springer},
   title = {Functional Data Analysis},
   year = {2005}
}
@article{Calle-saldarriaga2020,
   author = {Alejandro Calle-saldarriaga and Henry Laniado and Francisco Zuluaga},
   keywords = {acalles,address,affiliation,alejandro calle-saldarriaga,bootstrap-t,carrera 49,co,colombia,contact,department of mathe-,e-mail,eafit,edu,hypothesis testing,matical sciences,medell,n,non-parametric,robust,school of science,universidad eafit,ın},
   pages = {1-25},
   title = {Homogeneity Test for Functional Data based on Data-Depth Plots .},
   year = {2020}
}
@article{Meneses-Bautista2017,
   abstract = {Resumen. El análisis de las series de tiempo permite caracterizar un fenómeno e incluso predecir, con un cierto grado de precisión, su com-portamiento a futuro. La posibilidad de anticipar los movimientos de los mercados resulta sumamente atractiva para los responsables de la toma de decisiones, tanto en la iniciativa privada como en el sector público. En esta investigación se aborda la regresión de series de tiempo del tipo de cambio dólar estadounidense/peso mexicano, empleando un modelo generado por redes neuronales artificiales de retropropagación. Los re-sultados obtenidos ratifican empíricamente las ventajas de la utilización de las redes neuronales en el análisis y pronóstico de series de tiempo financieras, producto de las capacidades de aproximación de funciones y generalización que presentan dichas redes. Palabras clave: pronóstico de tipos de cambio, redes neuronales arti-ficiales, series de tiempo. Abstract. The analysis of time series allows to characterize a phenomenon and even to predict, under a certain degree of precision, its future behavior. The possibility of anticipating market movements is extremely attractive for decision-makers, both in the private sector and in the public sector. This research addresses the regression of time series of the US dollar / Mexican peso exchange rate, using a model generated by backpropagation artificial neural networks. The results obtained ratify empirically the advantages of the use of neural networks in the analysis and forecast of financial time series, as a result of the capabilities of function approximation and generalization that these networks present.},
   author = {Francisco D Meneses-Bautista and Matías Alvarado},
   keywords = {artificial neural networks,exchange rates forecasting,time series},
   pages = {97-110},
   title = {Pronóstico del tipo de cambio USD/MXN con redes neuronales de retropropagación Forecasting USD/MXN Exchange Rate with Backpropagation Neural Networks},
   volume = {139},
   url = {http://www.rcs.cic.ipn.mx/2017_139/Pronostico del tipo de cambio USD_MXN con redes neuronales de retropropagacion.pdf},
   year = {2017}
}
@article{Henriquez2019,
   abstract = {The currency market is one of the most efficient markets, making it very difficult to predict future prices. Several studies have sought to develop more accurate models to predict the future exchange rate by analyzing econometric models, developing artificial intelligence models and combining both through the creation of hybrid models. This paper proposes a hybrid model for forecasting the variations of five exchange rates related to the US Dollar: Euro, British Pound, Japanese Yen, Swiss Franc and Canadian Dollar. The proposed model uses Independent Component Analysis (ICA) to deconstruct the series into independent components as well as neural networks (NN) to predict each component. This method differentiates this study from previous works where ICA has been used to extract the noise of time series or used to obtain explanatory variables that are then used in forecasting. The proposed model is then compared to random walk, autoregressive and conditional variance models, neural networks, recurrent neural networks and long–short term memory neural networks. The hypothesis of this study supposes that first deconstructing the exchange rate series and then predicting it separately would produce better forecasts than traditional models. By using the mean squared error and mean absolute percentage error as a measures of performance and Model Confidence Sets to statistically test the superiority of the proposed model, our results showed that this model outperformed the other models examined and significantly improved the accuracy of forecasts. These findings support this model's use in future research and in decision-making related to investments.},
   author = {Jonatan Henríquez and Werner Kristjanpoller},
   doi = {10.1016/j.asoc.2019.105654},
   issn = {15684946},
   journal = {Applied Soft Computing Journal},
   keywords = {Exchange rate,Hybrid model,Independent Component Analysis,Neural networks,Time series forecasting},
   pages = {105654},
   publisher = {Elsevier B.V.},
   title = {A combined Independent Component Analysis–Neural Network model for forecasting exchange rate variation},
   volume = {83},
   url = {https://doi.org/10.1016/j.asoc.2019.105654},
   year = {2019}
}
@article{Wu2019,
   abstract = {A hybrid ensemble learning approach is proposed for exchange rate forecasting combining variational mode decomposition (VMD) and support vector neural network (SVNN). First, VMD is employed to decompose the original exchange rate time series into several components. Then, SVNN is adopted to forecast different component series. In the end, the forecasting results of all the components are combined using SVNN as ensemble learning method to obtain the ensemble results. Four major daily exchange rate datasets are selected for model evaluation and comparison. The empirical study demonstrates that the proposed VMD–SVNN ensemble learning approach outperforms other single forecasting models and other ensemble learning approaches in terms of both level forecasting accuracy and directional forecasting accuracy. This suggests that the VMD–SVNN ensemble learning approach is a highly promising approach for exchange rates forecasting with high volatility and irregularity.},
   author = {Yungao Wu and Jianwei Gao},
   doi = {10.1007/s00500-018-3336-1},
   isbn = {0050001833},
   issn = {14337479},
   issue = {16},
   journal = {Soft Computing},
   keywords = {Ensemble learning,Exchange rates forecasting,Support vector neural network,Variational mode decomposition},
   pages = {6995-7004},
   publisher = {Springer Berlin Heidelberg},
   title = {Application of support vector neural network with variational mode decomposition for exchange rate forecasting},
   volume = {23},
   url = {https://doi.org/10.1007/s00500-018-3336-1},
   year = {2019}
}
@article{Zhai2020,
   abstract = {Volatility prediction, a central issue in financial econometrics, attracts increasing attention in the data science literature as advances in computational methods enable us to develop models with great forecasting precision. In this paper, we draw upon both strands of the literature and develop a novel two-component volatility model. The realized volatility is decomposed by a nonparametric filter into long- and short-run components, which are modeled by an artificial neural network and an ARMA process, respectively. We use intraday data on four major exchange rates and a Chinese stock index to construct daily realized volatility and perform out-of-sample evaluation of volatility forecasts generated by our model and well-established alternatives. Empirical results show that our model outperforms alternative models across all statistical metrics and over different forecasting horizons. Furthermore, volatility forecasts from our model offer economic gain to a mean-variance utility investor with higher portfolio returns and Sharpe ratio.},
   author = {Jia Zhai and Yi Cao and Xiaoquan Liu},
   doi = {10.1080/14697688.2019.1711148},
   issn = {14697696},
   journal = {Quantitative Finance},
   keywords = {ARMA process,Exchange rates,Volatility prediction,Wavelet analysis},
   publisher = {Routledge},
   title = {A neural network enhanced volatility component model},
   volume = {7688},
   year = {2020}
}
@article{Waheeb2019,
   abstract = {The training speed for multilayer neural networks is slow due to the multilayering. Therefore, removing the hidden layers, provided that the input layer is endowed with additional higher order units is suggested to avoid such problem. Tensor product functional link neural network (TPFLNN) is a single layer with higher order terms that extend the network’s structure by introducing supplementary inputs to the network (i.e., joint activations). Although the structure of the TPFLNN is simple, it suffers from weight combinatorial explosion problem when its order becomes excessively high. Furthermore, similarly to many neural network methods, selection of proper weights is one of the most challenging issues in the TPFLNN. Finding suitable weights could help to reduce the number of needed weights. Therefore, in this study, the genetic algorithm (GA) was used to find near-optimum weights for the TPFLNN. The proposed method is abbreviated as GA–TPFLNN. The GA–TPFLNN was used to forecast the daily exchange rate for the Euro/US Dollar, and Japanese Yen/US Dollar. Simulation results showed that the GA–TPFLNN produced more accurate forecasts as compared to the standard TPFLNN, GA, GA–TPFLNN with backpropagation, GA-functional expansion FLNN, multilayer perceptron, support vector regression, random forests for regression, and naive methods. The GA helps the TPFLNN to find low complexity network structure and/or near-optimum parameters which leads to this better result.},
   author = {Waddah Waheeb and Rozaida Ghazali},
   doi = {10.1007/s12065-019-00261-2},
   isbn = {0123456789},
   issn = {18645917},
   issue = {4},
   journal = {Evolutionary Intelligence},
   keywords = {Exchange rate,Forecasting,Functional link neural network,Genetic algorithm,Time series},
   pages = {593-608},
   publisher = {Springer Berlin Heidelberg},
   title = {A new genetically optimized tensor product functional link neural network: an application to the daily exchange rate forecasting},
   volume = {12},
   url = {https://doi.org/10.1007/s12065-019-00261-2},
   year = {2019}
}
@article{Sun2020,
   author = {Shaolong Sun and Shouyang Wang and Yunjie Wei},
   doi = {10.1108/imds-03-2019-0194},
   issn = {0263-5577},
   issue = {ahead-of-print},
   journal = {Industrial Management \& Data Systems},
   keywords = {autoregressive model,bivariate empirical mode decomposition,conflict of interests,exchange rate forecasting,interval-valued data,neural networks,of interests regarding the,paper type research paper,publication of,the authors declare that,there is no conflict},
   title = {Interval forecasting of exchange rates: a new interval decomposition ensemble approach},
   volume = {ahead-of-p},
   year = {2020}
}
@article{Jalil2006,
   author = {Munir Jalil and Martha Misas},
   keywords = {evaluación de pronóstico,redes neuronales artificiales,tipo de cambio},
   title = {Evaluación de pronósticos del tipo de cambio utilizando redes neuronales y funciones de pérdida asimétricas},
   year = {2006}
}
@article{Gharleghi2014,
   abstract = {The present study focuses upon the applications of currently available intelligence techniques to forecast exchange rates in short and long horizons. The predictability of exchange rate returns is investigated through the use of a novel cointegration-based neuro-fuzzy system, which is a combination of a cointegration technique; a Fuzzy Inference System; and Artificial Neural Networks. The Relative Price Monetary Model for exchange rate determination is used to determine the inputs, consisting of macroeconomic variables and the type of interactions amongst the variables, in order to develop the system. Considering exchange rate returns of three ASEAN countries (Malaysia, the Philippines and Singapore), our results reveal that the cointegration-based neuro-fuzzy system model consistently outperforms the Vector Error Correction Model by successfully forecasting exchange rate monthly returns with a high level of accuracy. © 2013 CEPII (Centre d'Etudes Prospectives et d'Informations Internationales), a center for research and expertise on the world economy.},
   author = {Behrooz Gharleghi and Abu Hassan Shaari and Najla Shafighi},
   doi = {10.1016/j.inteco.2013.12.001},
   issn = {21107017},
   journal = {International Economics},
   keywords = {Error correction model,Exchange rate,Intelligence systems,Neural networks,Unit root},
   pages = {88-103},
   title = {Predicting exchange rates using a novel "cointegration based neuro-fuzzy system"},
   volume = {137},
   year = {2014}
}
@article{ZapataGarrido2008,
   author = {Luis Alberto Zapata Garrido and Hugo Fabián Díaz Mojica},
   issn = {1657-6276},
   issue = {24},
   journal = {Pensamiento \& Gestión},
   keywords = {Artificial prediction of the type of change,Neuronal Networks,Predicción del tipo de cambio,Redes Neuronales Artificiales},
   pages = {29-42},
   title = {Predicción del tipo de cambio peso-dólar utilizando Redes Neuronales Artificiales (rna)},
   year = {2008}
}
@article{Parisi2003,
   author = {Antonino Parisi and Franco Parisi and José Luis Guerrero},
   journal = {Estudios de Administración},
   title = {Modelos de redes neuronales aplicados a la predicción del tipo de cambio del dólar observado en Chile},
   year = {2003}
}
@article{OquendoPatino2012,
   author = {Viviana María Oquendo Patiño},
   keywords = {así como a los,con el fin de,constituirse como una metodología,contrastar los resultados obtenidos,de redes neuronales artificiales,económicas,en que ésta puede,para la predicción en,pueden realizar estás implementaciones,se,se ajusta un modelo,series de tiempo,software en los que,y mostrar la manera},
   pages = {1-24},
   title = {Redes Neuronales Artificiales en las Ciencias Económicas},
   url = {https://www.rics.org/south-asia/upholding-professional-standards/standards-of-conduct/ethics/},
   year = {2012}
}
@article{Rifai2019,
   abstract = {Investment activities in the capital market have the possibility to generate profits and at the same time also cause losses. The composite stock price index as an indicator used to determine investment continues to change over time. Uncertainty of stock exchange composite index requires investors to be able to make predictions so as to produce maximum profits. The aim of this study is to forecast the composite stock price index. The input variables used are Indonesia interest rates, rupiah exchange rates, Dow Jones index, and world gold prices. All data obtained in the period from January 2008 to March 2019. Data are used to build the Fuzzy Backpropagation Neural Network (FBPNN), model. The weight of FBPNN model was optimized using Genetic Algorithm then used to forecast the composite stock price index. The forecasting result of the composite stock price index for April to June 2019 respectively were 5822.6, 5826.8, and 5767.3 with the MAPE value of 8.42%. These results indicate that Indonesia interest rates, rupiah exchange rate, Dow Jones index, and the gold price are the proper indicators to predict the composite stock price index.},
   author = {Anwar Rifa’i and Deni Mahdiana},
   doi = {10.23919/EECSI48112.2019.8977058},
   isbn = {9786020737287},
   issn = {2407439X},
   journal = {International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)},
   keywords = {Fuzzy backpropagation neural network,Genetic algorithm,Stock exchange composite index},
   pages = {195-199},
   title = {Optimized fuzzy backpropagation neural network using genetic algorithm for predicting indonesian stock exchange composite index},
   year = {2019}
}
@article{Srivastava2019,
   abstract = {Urban air pollution prediction becomes an indispensable alternative to curb its detrimental consequences. Numerous machine learning techniques have been adopted to forecast the air quality. In this paper, we implemented different classification and regression techniques like Linear Regression, SDG Regression, Random Forest Regression, Decision Tree Regression, Support Vector Regression, Artificial Neural Networks, Gradient Boosting Regression and Adaptive Boosting Regression to forecast the Air Quality Index of major pollutants like PM2.5, PM10, CO, NO2, SO2 and O3. The techniques are then evaluated using Mean square error, Mean absolute error and R2, which show that Support Vector Regression and Artificial Neural Networks are best suited for predicting the air quality in New Delhi.},
   author = {Chavi Srivastava and Shyamli Singh and Amit Prakash Singh},
   doi = {10.1109/GUCON.2018.8675022},
   isbn = {9781538644911},
   journal = {2018 International Conference on Computing, Power and Communication Technologies, GUCON 2018},
   keywords = {Air pollution monitoring,Artificial intelligence,Forecasting,Machine learning,Predictive models,Regression},
   pages = {304-309},
   publisher = {IEEE},
   title = {Estimation of air pollution in Delhi using machine learning techniques},
   year = {2019}
}
@article{Monia2019,
   abstract = {Growing trends in Air pollution is possessing threat to environment. Various Researchers have extended their work in predicting air pollution using various predictive analytics. In this paper, we are implementing a predictive model for monitoring air pollution level in different cities of India and publishing it as a web service.The algorithm being used is Collaborative Filtering Prediction Algorithm. A comparison has also been carried out in different predictive analytics mainly using Machine Learning techniques such as regression and Deep Learning Technique and Collaborative filtering technique.},
   author = {Monia and Akanksha Gupta and Sameiksha Sharma},
   doi = {10.1109/ICECCT.2019.8869266},
   isbn = {9781538681572},
   journal = {Proceedings of 2019 3rd IEEE International Conference on Electrical, Computer and Communication Technologies, ICECCT 2019},
   keywords = {Collaborative filtering,Deep learning,Heuristics,Pearson Coefficient},
   pages = {1-8},
   publisher = {IEEE},
   title = {Predictive Analysis of Air Pollution Using Collaborative Filtering Prediction Algorithm},
   year = {2019}
}
@article{Sinnott2019,
   abstract = {Prediction of pollution is an increasingly important problem. It can impact individuals and their health, e.g. asthma patients can be greatly affected by air pollution. Traditional air pollution prediction methods have limitations. Machine learning provides one approach that can offer new opportunities for prediction of air pollution. There are however many different machine learning approaches and identifying the best one for the problem at hand is often challenging. In this paper air pollution data, specifically particulate matter of less than 2.5 micrometers (PM2.5) was collected from a variety of web-based resources and following, data cleansing analysed with different machine learning models including linear regression, Artificial Neural Networks and Long Short Term Memory recurrent neural networks. We consider the accuracy and the ability of these different models to predict unhealthy levels of pollution. The advantages and disadvantages of these models are also discussed.},
   author = {Richard O. Sinnott and Ziyue Guan},
   doi = {10.1109/BDCAT.2018.00015},
   isbn = {9781538655023},
   journal = {Proceedings - 5th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies, BDCAT 2018},
   keywords = {PM2.5,air pollution,cloud computing,data analytics,traffic},
   pages = {51-60},
   title = {Prediction of Air Pollution through Machine Learning Approaches on the Cloud},
   year = {2019}
}
@article{Hu2017,
   abstract = {Metropolitan air pollution is a growing concern in both developing and developed countries. Fixed-station monitors, typically operated by governments, offer accurate but sparse data, and are increasingly being augmented by lower fidelity but denser measurements taken by mobile sensors carried by concerned citizens and researchers. In this paper, we introduce HazeEst - a machine learning model that combines sparse fixed-station data with dense mobile sensor data to estimate the air pollution surface for any given hour on any given day in Sydney. We assess our system using seven regression models and tenfold cross validation. The results show that estimation accuracy of support vector regression (SVR) is similar to decision tree regression and random forest regression, and higher than extreme gradient boosting, multi-layer perceptrons, linear regression, and adaptive boosting regression. The air pollution estimates from our models are validated via field trials, and results show that SVR not only yields high spatial resolution estimates that correspond well with the pollution surface obtained from fixed and mobile sensor monitoring systems, but also indicates boundaries of polluted area better than other regression models. Our results can be visualized using a Web-based application customized for metropolitan Sydney. We believe that the continuous estimates provided by our system can better inform air pollution exposure and its impact on human health.},
   author = {Ke Hu and Ashfaqur Rahman and Hari Bhrugubanda and Vijay Sivaraman},
   doi = {10.1109/JSEN.2017.2690975},
   issn = {1530437X},
   issue = {11},
   journal = {IEEE Sensors Journal},
   keywords = {Air pollution monitoring,machine learning,support vector regression,web application,wireless sensor network},
   pages = {3517-3525},
   title = {HazeEst: Machine Learning Based Metropolitan Air Pollution Estimation from Fixed and Mobile Sensors},
   volume = {17},
   year = {2017}
}
@article{Xi2015,
   abstract = {Urban air pollution prediction is one of the most important tasks in the treatment of urban air pollution. Due to the disadvantage that source list updated not in time for WRF-Chem which is a numeric model, the prediction result may be not good enough. In this paper, we take full advantages of forecast on pollution, weather, chemical component from WRF-Chem model as input features, design a comprehensive evaluation framework to improve the prediction performance. Experiments are implemented with different features groups and classification algorithms in machine learning method for 74 cities in China, to find the best model for each city. From experiments, for different city, the best result can be obtained by different group of feature selection and model selection. Experimental results indicate that the more feature we used, the more possibility to enhance the accuracy. For method aspect, the result from combined model is better than the unique model.},
   author = {Xia Xi and Zhao Wei and Rui Xiaoguang and Wang Yijie and Bai Xinxin and Yin Wenjun and Don Jin},
   doi = {10.1109/SOLI.2015.7367615},
   isbn = {9781467384803},
   journal = {10th IEEE Int. Conf. on Service Operations and Logistics, and Informatics, SOLI 2015 - In conjunction with ICT4ALL 2015},
   keywords = {air pollutation prediction,air quality index prediction,combined method,machine learning},
   pages = {176-181},
   publisher = {IEEE},
   title = {A comprehensive evaluation of air pollution prediction improvement by a machine learning method},
   year = {2015}
}
@article{Eldakhly2018,
   abstract = {The particulate matter air pollutant of diameter less than 10 micrometers (PM10), a category of pollutants including solid and liquid particles, can be a health hazard for several reasons: it can harm lung tissues and throat, aggravate asthma and increase respiratory illness. Accurate prediction models of PM10 concentrations are essential for proper management, control, and making public warning strategies. Therefore, machine learning techniques have the capability to develop methods or tools that can be used to discover unseen patterns from given data to solve a particular task or problem. The chance theory has advanced concepts pertinent to treat cases where both randomness and fuzziness play simultaneous roles at one time. The main objective is to study the modification of a single machine learning algorithm - support vector machine (SVM) - applying the chance weight of the target variable, based on the chance theory, to the corresponding dataset point to be superior to the ensemble machine learning algorithms. The results of this study are outperforming of the SVM algorithms when modifying and combining with the right theory/technique, especially the chance theory over other modern ensemble learning algorithms.},
   author = {Nabil Mohamed Eldakhly and Magdy Aboul-Ela and Areeg Abdalla},
   doi = {10.1142/S1469026818500013},
   issn = {14690268},
   issue = {1},
   journal = {International Journal of Computational Intelligence and Applications},
   keywords = {PM 1 0 pollutant,Single machine learning algorithm,chance theory,ensemble machine learning algorithms,weighted learning algorithms},
   pages = {1-29},
   title = {A Novel Approach of Weighted Support Vector Machine with Applied Chance Theory for Forecasting Air Pollution Phenomenon in Egypt},
   volume = {17},
   year = {2018}
}
@article{Bellinger2017,
   abstract = {Background: Data measuring airborne pollutants, public health and environmental factors are increasingly being stored and merged. These big datasets offer great potential, but also challenge traditional epidemiological methods. This has motivated the exploration of alternative methods to make predictions, find patterns and extract information. To this end, data mining and machine learning algorithms are increasingly being applied to air pollution epidemiology. Methods: We conducted a systematic literature review on the application of data mining and machine learning methods in air pollution epidemiology. We carried out our search process in PubMed, the MEDLINE database and Google Scholar. Research articles applying data mining and machine learning methods to air pollution epidemiology were queried and reviewed. Results: Our search queries resulted in 400 research articles. Our fine-grained analysis employed our inclusion/exclusion criteria to reduce the results to 47 articles, which we separate into three primary areas of interest: 1) source apportionment; 2) forecasting/prediction of air pollution/quality or exposure; and 3) generating hypotheses. Early applications had a preference for artificial neural networks. In more recent work, decision trees, support vector machines, k-means clustering and the APRIORI algorithm have been widely applied. Our survey shows that the majority of the research has been conducted in Europe, China and the USA, and that data mining is becoming an increasingly common tool in environmental health. For potential new directions, we have identified that deep learning and geo-spacial pattern mining are two burgeoning areas of data mining that have good potential for future applications in air pollution epidemiology. Conclusions: We carried out a systematic review identifying the current trends, challenges and new directions to explore in the application of data mining methods to air pollution epidemiology. This work shows that data mining is increasingly being applied in air pollution epidemiology. The potential to support air pollution epidemiology continues to grow with advancements in data mining related to temporal and geo-spacial mining, and deep learning. This is further supported by new sensors and storage mediums that enable larger, better quality data. This suggests that many more fruitful applications can be expected in the future.},
   author = {Colin Bellinger and Mohomed Shazan Mohomed Jabbar and Osmar Zaïane and Alvaro Osornio-Vargas},
   doi = {10.1186/s12889-017-4914-3},
   issn = {14712458},
   issue = {1},
   journal = {BMC Public Health},
   keywords = {Air pollution,Association mining,Big data,Data mining,Epidemiology,Exposure,Machine learning},
   pages = {1-19},
   publisher = {BMC Public Health},
   title = {A systematic review of data mining and machine learning for air pollution epidemiology},
   volume = {17},
   year = {2017}
}
@article{OMS2005,
   abstract = {Se considera que el aire limpio es un requisito básico de la salud y el bienestar humanos. Sin embargo, su contaminación sigue representando una amenaza importante para la salud en todo el mundo. Según una evaluación de la OMS de la carga de enfermedad debida a la contaminación del aire, son más de dos millones las muertes prematuras que se pueden atribuir cada año a los efectos de la contaminación del aire en espacios abiertos urbanos y en espacios cerrados (produ- cida por la quema de combustibles sólidos). Más de la mitad de esta carga de enfermedad recae en las poblaciones de los países en desarrollo.},
   author = {OMS},
   issue = {1},
   journal = {Guías de calidad del aire de la OMS relativas al material particulado, el ozono, el dióxido de nitrógeno y el dióxido de azufre Actualización},
   pages = {1-21},
   title = {Actualización mundial 2005},
   volume = {5},
   year = {2005}
}
@article{Papaleonidas2013,
   abstract = {Real time monitoring, forecasting and modeling air pollutants' concentrations in major urban centers is one of the top priorities of all local and national authorities globally. This paper studies and analyzes the parameters related to the problem, aiming in the design and development of an effective machine learning model and its corresponding system, capable of forecasting dangerous levels of ozone (O3) concentrations in the city center of Athens and more specifically in the "Athinas" air quality monitoring station. This is a multi parametric case, so an effort has been made to combine a vast number of data vectors from several operational nearby measurements' stations. The final result was the design and construction of a group of artificial neural networks capable of estimating O3 concentrations in real time mode and also having the capacity of forecasting the same values for future time intervals of 1, 2, 3 and 6 h, respectively. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {Antonios Papaleonidas and Lazaros Iliadis},
   doi = {10.1007/s12530-013-9078-5},
   issn = {18686478},
   issue = {4},
   journal = {Evolving Systems},
   keywords = {Artificial neural networks,Machine learning,Multi parametric ANN,Ozone estimation and forecasting,Pollution of the atmosphere},
   pages = {221-233},
   title = {Neurocomputing techniques to dynamically forecast spatiotemporal air pollution data},
   volume = {4},
   year = {2013}
}
@article{Martinez-Espana2018,
   abstract = {Air-pollution is one of the main threats for developed societies. According to the World Health Organization (WHO), pollution is the main cause of deaths among children aged under five. Smart cities are called to play a decisive role to improve such pollution by first collecting, in real-time, different parameters such as SO2, NOx, O3, NH3, CO, PM10, just to mention a few, and then performing the subsequent data analysis and prediction. However, some machine learning techniques may be more well-suited than others to predict pollution-like variables. In this paper several machine learning methods are analyzed to predict the ozone level (O3) in the Region of Murcia (Spain). O3 is one of the main hazards to health when it reaches certain levels. Indeed, having accurate air-quality prediction models is a previous step to take mitigation activities that may benefit people with respiratory disease like Asthma, Bronchitis or Pneumonia in intelligent cities. Moreover, here it is identified the most-significant variables to monitor the air-quality in cities. Our results indicate an adjustment for the proposed O3 prediction models from 90% and a root mean square error less than 11 µ/m3 for the cities of the Region of Murcia involved in the study.},
   author = {Raquel Martínez-España and Andrés Bueno-Crespo and Isabel Timón and Jesús Soto and Andrés Muñoz and José M. Cecilia},
   issn = {09486968},
   issue = {3},
   journal = {Journal of Universal Computer Science},
   keywords = {Air-pollution monitoring,Hierarchical clustering,Machine learning,Ozone,Random forest,Smart cities},
   pages = {261-276},
   title = {Air-pollution prediction in smart cities through machine learning methods: A case of study in Murcia, Spain},
   volume = {24},
   year = {2018}
}
@article{Delavar2019,
   abstract = {Environmental pollution has mainly been attributed to urbanization and industrial developments across the globe. Air pollution has been marked as one of the major problems of metropolitan areas around the world, especially in Tehran, the capital of Iran, where its administrators and residents have long been struggling with air pollution damage such as the health issues of its citizens. As far as the study area of this research is concerned, a considerable proportion of Tehran air pollution is attributed to PM10 and PM2.5 pollutants. Therefore, the present study was conducted to determine the prediction models to determine air pollutions based on PM10 and PM2.5 pollution concentrations in Tehran. To predict the air-pollution, the data related to day of week, month of year, topography, meteorology, and pollutant rate of two nearest neighbors as the input parameters and machine learning methods were used. These methods include a regression support vector machine, geographically weighted regression, artificial neural network and auto-regressive nonlinear neural network with an external input as the machine learning method for the air pollution prediction. A prediction model was then proposed to improve the afore-mentioned methods, by which the error percentage has been reduced and improved by 57%, 47%, 47% and 94%, respectively. The most reliable algorithm for the prediction of air pollution was autoregressive nonlinear neural network with external input using the proposed prediction model, where its one-day prediction error reached 1.79 µg/m3. Finally, using genetic algorithm, data for day of week, month of year, topography, wind direction, maximum temperature and pollutant rate of the two nearest neighbors were identified as the most effective parameters in the prediction of air pollution.},
   author = {Mahmoud Reza Delavar and Amin Gholami and Gholam Reza Shiran and Yousef Rashidi and Gholam Reza Nakhaeizadeh and Kurt Fedra and Smaeil Hatefi Afshar},
   doi = {10.3390/ijgi8020099},
   issn = {22209964},
   issue = {2},
   journal = {ISPRS International Journal of Geo-Information},
   keywords = {Air pollution,Artificial neural network,Auto-regressive nonlinear neural,Genetic algorithm,Geographically weighted regression,Interpolation,Machine learning,Prediction,Regression SVM},
   title = {A novel method for improving air pollution prediction based on machine learning approaches: A case study applied to the capital city of Tehran},
   volume = {8},
   year = {2019}
}
@article{,
   title = {CM0868 GUÍA DEL PROYECTO Trabajo 2 : Ejecución del plan inicial}
}
@article{Qi2008,
   author = {Min Qi and G Peter Zhang},
   issue = {5},
   pages = {808-816},
   title = {Trend Time – Series Modeling and Forecasting With Neural Networks},
   volume = {19},
   year = {2008}
}
@article{Krzysko2020,
   abstract = {In this paper, we consider the discriminant coordinates for multivariate functional data and their application to classification. We present more general construction of the multivariate functional discriminant coordinates than that known in the literature. The construction is based on basis expansion of functional data. To overcome non-robustness of classical estimators, we also propose robust estimation methods of unknown parameters. The constructed classification rules for multivariate functional data based on the linear discriminant analysis and standard and robust discriminant coordinates are compared on simulation study and on real data set. The results indicate possible usefulness of proposed methods in practice.},
   author = {Mirosław Krzyśko and Łukasz Smaga},
   doi = {10.1080/03610918.2019.1580731},
   issn = {15324141},
   issue = {3},
   journal = {Communications in Statistics: Simulation and Computation},
   keywords = {Discriminant coordinates,Linear discriminant analysis,Multivariate functional data analysis,Robust location and scatter estimation},
   pages = {717-733},
   publisher = {Taylor \& Francis},
   title = {Robust multivariate functional discriminant coordinates},
   volume = {49},
   url = {https://doi.org/10.1080/03610918.2019.1580731},
   year = {2020}
}
@article{Gorecki2020,
   abstract = {In the case of vector data, Gretton et al. (Algorithmic learning theory. Springer, Berlin, pp 63–77, 2005) defined Hilbert–Schmidt independence criterion, and next Cortes et al. (J Mach Learn Res 13:795–828, 2012) introduced concept of the centered kernel target alignment (KTA). In this paper we generalize these measures of dependence to the case of multivariate functional data. In addition, based on these measures between two kernel matrices (we use the Gaussian kernel), we constructed independence test and nonlinear canonical variables for multivariate functional data. We show that it is enough to work only on the coefficients of a series expansion of the underlying processes. In order to provide a comprehensive comparison, we conducted a set of experiments, testing effectiveness on two real examples and artificial data. Our experiments show that using functional variants of the proposed measures, we obtain much better results in recognizing nonlinear dependence.},
   author = {Tomasz Górecki and Mirosław Krzyśko and Waldemar Wołyński},
   doi = {10.1007/s10462-018-9666-7},
   issn = {15737462},
   issue = {1},
   journal = {Artificial Intelligence Review},
   keywords = {Canonical correlation analysis,Correlation analysis,Functional data analysis,Multivariate functional data},
   pages = {475-499},
   title = {Independence test and canonical correlation analysis based on the alignment between kernel matrices for multivariate functional data},
   volume = {53},
   year = {2020}
}
@article{Dai2019,
   abstract = {The direction of outlyingness is crucial to describing the centrality of multivariate functional data. Motivated by this idea, classical depth is generalized to directional outlyingness for functional data. Theoretical properties of functional directional outlyingness are investigated and the total outlyingness can be naturally decomposed into two parts: magnitude outlyingness and shape outlyingness which represent the centrality of a curve for magnitude and shape, respectively. This decomposition serves as a visualization tool for the centrality of curves. Furthermore, an outlier detection procedure is proposed based on functional directional outlyingness. This criterion applies to both univariate and multivariate curves and simulation studies show that it outperforms competing methods. Weather and electrocardiogram data demonstrate the practical application of our proposed framework.},
   author = {Wenlin Dai and Marc G. Genton},
   doi = {10.1016/j.csda.2018.03.017},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Centrality visualization,Directional outlyingness,Multivariate function data,Outlier detection,Outlyingness decomposition},
   note = {What if not?},
   pages = {50-65},
   publisher = {Elsevier B.V.},
   title = {Directional outlyingness for multivariate functional data},
   volume = {131},
   url = {https://doi.org/10.1016/j.csda.2018.03.017},
   year = {2019}
}
@article{Dai2018,
   abstract = {This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate nonoutlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data. Supplementary material for this article is available online.},
   author = {Wenlin Dai and Marc G. Genton},
   doi = {10.1080/10618600.2018.1473781},
   issn = {15372715},
   issue = {4},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Data visualization,Directional outlyingness,Functional data,Graphical tool,Magnitude and shape,Outlier detection},
   pages = {923-934},
   publisher = {Taylor \& Francis},
   title = {Multivariate Functional Data Visualization and Outlier Detection},
   volume = {27},
   url = {https://doi.org/10.1080/10618600.2018.1473781},
   year = {2018}
}
@article{Hubert2015a,
   abstract = {Functional data are occurring more and more often in practice, and various statistical techniques have been developed to analyze them. In this paper we consider multivariate functional data, where for each curve and each time point a $$p$$p-dimensional vector of measurements is observed. For functional data the study of outlier detection has started only recently, and was mostly limited to univariate curves $$(p=1)$$(p=1). In this paper we set up a taxonomy of functional outliers, and construct new numerical and graphical techniques for the detection of outliers in multivariate functional data, with univariate curves included as a special case. Our tools include statistical depth functions and distance measures derived from them. The methods we study are affine invariant in $$p$$p-dimensional space, and do not assume elliptical or any other symmetry.},
   author = {Mia Hubert and Peter J. Rousseeuw and Pieter Segaert},
   doi = {10.1007/s10260-015-0297-8},
   issn = {1613981X},
   issue = {2},
   journal = {Statistical Methods and Applications},
   keywords = {Depth,Diagnostic plot,Functional data,Graphical display,Heatmap,Robustness},
   pages = {177-202},
   publisher = {Springer Berlin Heidelberg},
   title = {Multivariate functional outlier detection},
   volume = {24},
   url = {http://dx.doi.org/10.1007/s10260-015-0297-8},
   year = {2015}
}
@article{Kuhnt2016,
   abstract = {A measure especially designed for detecting shape outliers in functional data is presented. It is based on the tangential angles of the intersections of the centred data and can be interpreted like a data depth. Due to its theoretical properties we call it functional tangential angle (FUNTA) pseudo-depth. Furthermore we introduce a robustification (rFUNTA). The existence of intersection angles is ensured through the centring. Assuming that shape outliers in functional data follow a different pattern, the distribution of intersection angles differs. Furthermore we formulate a population version of FUNTA in the context of Gaussian processes. We determine sample breakdown points of FUNTA and compare its performance with respect to outlier detection in simulation studies and a real data example.},
   author = {Sonja Kuhnt and André Rehage},
   doi = {10.1016/j.jmva.2015.10.016},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Bootstrap,Data depth,Functional data,Robust estimate,Shape outlier detection},
   pages = {325-340},
   publisher = {Elsevier Inc.},
   title = {An angle-based multivariate functional pseudo-depth for shape outlier detection},
   volume = {146},
   url = {http://dx.doi.org/10.1016/j.jmva.2015.10.016},
   year = {2016}
}
@article{Lopez-Pintado2014,
   abstract = {We propose notions of simplicial band depth for multivariate functional data that extend the univariate functional band depth. The proposed simplicial band depths provide simple and natural criteria to measure the centrality of a trajectory within a sample of curves. Based on these depths, a sample of multivariate curves can be ordered from the center outward and order statistics can be defined. Properties of the proposed depths, such as invariance and consistency, can be established. A simulation study shows the robustness of this new definition of depth and the advantages of using a multivariate depth versus the marginal depths for detecting outliers. Real data examples from growth curves and signature data are used to illustrate the performance and usefulness of the proposed depths. © 2014 Springer-Verlag Berlin Heidelberg.},
   author = {Sara López-Pintado and Ying Sun and Juan K. Lin and Marc G. Genton},
   doi = {10.1007/s11634-014-0166-6},
   issn = {18625355},
   issue = {3},
   journal = {Advances in Data Analysis and Classification},
   keywords = {Band depth,Functional and image data,Functional boxplot,Modified band depth,Multivariate,Simplicial},
   pages = {321-338},
   title = {Simplicial band depth for multivariate functional data},
   volume = {8},
   year = {2014}
}
@article{Tarabelloni2015,
   abstract = {In this paper we develop statistical methods to compare two independent samples of multivariate functional data that differ in terms of covariance operators. In particular we generalize the concept of depth measure to this kind of data, exploiting the role of the covariance operators in weighting the components that define the depth. Two simulation studies are carried out to validate the robustness of the proposed methods and to test their effectiveness in some settings of interest. We present an application to Electrocardiographic (ECG) signals aimed at comparing physiological subjects and patients affected by Left Bundle Branch Block. The proposed depth measures computed on data are then used to perform a nonparametric comparison test among these two populations. They are also introduced into a generalized regression model aimed at classifying the ECG signals.},
   author = {Nicholas Tarabelloni and Francesca Ieva and Rachele Biasi and Anna Maria Paganoni},
   doi = {10.1515/ijb-2014-0041},
   issn = {15574679},
   issue = {2},
   journal = {International Journal of Biostatistics},
   keywords = {ECG signals,covariance operators,depth measures,generalized linear models,multivariate functional data},
   pages = {189-201},
   pmid = {26110484},
   title = {Use of Depth Measure for Multivariate Functional Data in Disease Prediction: An Application to Electrocardiograph Signals},
   volume = {11},
   year = {2015}
}
@article{Febrero-Bande2012a,
   abstract = {This paper is devoted to the R package fda.usc which includes some utilities for functional data analysis. This package carries out exploratory and descriptive analysis of functional data analyzing its most important features such as depth measurements or functional outliers detection, among others. The R package fda.usc also includes functions to compute functional regression models, with a scalar response and a functional explanatory data via non-parametric functional regression, basis representation or functional principal components analysis. There are natural extensions such as functional linear models and semi-functional partial linear models, which allow non-functional covariates and factors and make predictions. The functions of this package complement and incorporate the two main references of functional data analysis: The R package fda and the functions implemented by Ferraty and Vieu (2006).},
   author = {Manuel Febrero-Bande and Manuel Oviedo de la Fuente},
   doi = {10.18637/jss.v051.i04},
   issn = {15487660},
   issue = {4},
   journal = {Journal of Statistical Software},
   keywords = {Depth measures,Functional data regression,Non-parametric kernel estimation,Outlier,Representation of functional data},
   title = {Statistical computing in functional data analysis: The R package fda.usc},
   volume = {51},
   year = {2012}
}
@article{Gorecki2018,
   abstract = {Data in the form of a continuous vector function on a given interval are referred to as multivariate functional data. These data are treated as realizations of multivariate random processes. The paper is devoted to three statistical dimension reduction techniques for multivariate data. For the first one, principal components analysis, the authors present a review of a recent paper (Jacques and Preda in, Comput Stat Data Anal, 71:92–106, 2014). For two others one, canonical variables and discriminant coordinates, the authors extend existing works for univariate functional data to multivariate. These methods for multivariate functional data are presented, illustrated and discussed in the context of analyzing real data sets. Each of these techniques is applied on real data set.},
   author = {Tomasz Górecki and Mirosław Krzyśko and Łukasz Waszak and Waldemar Wołyński},
   doi = {10.1007/s00362-016-0757-8},
   issn = {09325026},
   issue = {1},
   journal = {Statistical Papers},
   keywords = {Canonical correlation analysis,Discriminant coordinates,Functional data analysis,Multivariate functional data,Principal component analysis},
   pages = {153-182},
   title = {Selected statistical methods of data analysis for multivariate functional data},
   volume = {59},
   year = {2018}
}
@article{Virta2020,
   abstract = {We extend two methods of independent component analysis, fourth order blind identification and joint approximate diagonalization of eigen-matrices, to vector-valued functional data. Multivariate functional data occur naturally and frequently in modern applications, and extending independent component analysis to this setting allows us to distill important information from this type of data, going a step further than the functional principal component analysis. To allow the inversion of the covariance operator we make the assumption that the dependency between the component functions lies in a finite-dimensional subspace. In this subspace we define fourth cross-cumulant operators and use them to construct the two novel, Fisher consistent methods for solving the independent component problem for vector-valued functions. Both simulations and an application on a hand gesture data set show the usefulness and advantages of the proposed methods over functional principal component analysis.},
   author = {Joni Virta and Bing Li and Klaus Nordhausen and Hannu Oja},
   doi = {10.1016/j.jmva.2019.104568},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Covariance operator,Dimension reduction,Fourth order blind identification,Functional principal component analysis,Hilbert space,Joint approximate diagonalization of eigenmatrices},
   pages = {104568},
   publisher = {Elsevier Inc.},
   title = {Independent component analysis for multivariate functional data},
   volume = {176},
   url = {https://doi.org/10.1016/j.jmva.2019.104568},
   year = {2020}
}
@article{Ferger2000,
   abstract = {In this paper we consider the classical problem of testing whether two samples of observations are from the same distribution. Since in many situations the data are multivariate or even of functional type, classical methodology is not applicable. In our approach we conceive a difference in distribution as the occurrence of a change-point problem, where the change-point is known in advance. This point of view enables us to construct new tests which are distribution-free under the null hypothesis for general sample spaces. The power function of the tests is studied under local and global alternatives. Finally some optimality results are provided. © 2000 Academic Press.},
   author = {Dietmar Ferger},
   doi = {10.1006/jmva.1999.1879},
   issn = {0047259X},
   issue = {1},
   journal = {Journal of Multivariate Analysis},
   keywords = {Two-sample test, contiguous alternatives, local po},
   pages = {1-35},
   title = {Optimal Tests for the General Two-Sample Problem},
   volume = {74},
   year = {2000}
}
@article{Hall2007,
   abstract = {One of the ways in which functional data analysis differs from other areas of statistics is in the extent to which data are pre-processed prior to analysis. Functional data are invariably recorded discretely, although they are generally substantially smoothed as a prelude even to viewing by statisticians, let alone further analysis. This has a potential to interfere with the performance of two-sample statistical tests, since the use of different tuning parameters for the smoothing step, or different observation times or subsample sizes (i.e., numbers of observations per curve), can mask the differences between distributions that a test is trying to locate. In this paper, and in the context of two-sample tests, we take up this issue. Ways of pre-processing the data, so as to minimise the effects of smoothing, are suggested. We show theoretically and numerically that, by employing exactly the same tuning parameter (e.g. bandwidth) to produce each curve from its raw data, significant loss of power can be avoided. Provided a common tuning parameter is used, it is often satisfactory to choose that parameter along conventional lines, as though the target was estimation of the continuous functions themselves, rather than testing hypotheses about them. Moreover, in this case, using a second-order smoother (such as a local-linear method), the subsample sizes can be almost as small as the square root of sample sizes before the effects of smoothing have any first-order impact on the results of a two-sample test.},
   author = {Peter Hall and Ingrid Van Keilegom},
   issn = {10170405},
   issue = {4},
   journal = {Statistica Sinica},
   keywords = {Bandwidth,Bootstrap,Cramér-von mises test,Curve estimation,Hypothesis testing,Kernel,Local-linear methods,Local-polynomial methods,Nonparametric regression,Smoothing},
   pages = {1511-1531},
   title = {Two-sample tests in functional data analysis starting from discrete data},
   volume = {17},
   year = {2007}
}
@article{Jiang2019,
   abstract = {We consider two-sample tests for functional data with observations which may be uni- or multi-dimensional. The new methods are formulated as L2-type criteria based on empirical characteristic functions and are convenient from the computational point of view. Asymptotic properties are presented. Simulations and two real data applications are conducted in order to evaluate the performance of the proposed tests vis-à-vis other methods.},
   author = {Qing Jiang and Marie Hušková and Simos G. Meintanis and Lixing Zhu},
   doi = {10.1016/j.jmva.2018.09.002},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Empirical characteristic function,Functional data,Two-sample problem},
   pages = {202-220},
   publisher = {Elsevier Inc.},
   title = {Asymptotics, finite-sample comparisons and applications for two-sample tests with functional data},
   volume = {170},
   url = {https://doi.org/10.1016/j.jmva.2018.09.002},
   year = {2019}
}
@book{Palumbo2018,
   abstract = {This edited volume on the latest advances in data science covers
a wide range of topics in the context of data analysis and
classification. In particular, it includes contributions on
classification methods for high-dimensional data, clustering
methods, multivariate statistical methods, and various
applications. The book gathers a selection of peer-reviewed
contributions presented at the Fifteenth Conference of the
International Federation of Classification Societies (IFCS2015),
which was hosted by the Alma Mater Studiorum, University of
Bologna, from July 5 to 8, 2015.},
   author = {Francesco Palumbo and Angela Montanari and Maurizio Vichi},
   doi = {10.30748/soi.2018.153.08},
   isbn = {978-3-319-55722-9},
   issn = {16817710},
   issue = {2(153)},
   pages = {69-74},
   title = {Data Science: Innovative Developments in Data Analysis and Clustering},
   url = {http://link.springer.com/10.1007/978-3-319-55723-6%0Ahttp://www.hups.mil.gov.ua/periodic-app/article/18780/soi_2018_2_10.pdf},
   year = {2018}
}
@article{Zhang2015,
   abstract = {Motivated by the need to statistically quantify the difference between two spatio-temporal datasets that arise in climate downscaling studies, we propose new tests to detect the differences of the covariance operators and their associated characteristics of two functional time series. Our two sample tests are constructed on the basis of functional principal component analysis and self-normalization, the latter of which is a new studentization technique recently developed for the inference of a univariate time series. Compared to the existing tests, our SN-based tests allow for weak dependence within each sample and it is robust to the dependence between the two samples in the case of equal sample sizes. Asymptotic properties of the SNbased test statistics are derived under both the null and local alternatives. Through extensive simulations, our SN-based tests are shown to outperform existing alternatives in size and their powers are found to be respectable. The tests are then applied to the gridded climate model outputs and interpolated observations to detect the difference in their spatial dynamics.},
   author = {Xianyang Zhang and Xiaofeng Shao},
   doi = {10.3150/13-BEJ592},
   issn = {13507265},
   issue = {2},
   journal = {Bernoulli},
   keywords = {Climate downscaling,Functional data analysis,Long run variance matrix,Self-normalization,Time series,Two sample problem},
   pages = {909-929},
   title = {Two sample inference for the second-order property of temporally dependent functional data},
   volume = {21},
   year = {2015}
}
@article{Febrero-Bande2007,
   abstract = {Five notions of data depth are considered. They are mostly designed for functional data but they can be also adapted to the standard multivariate case. The performance of these depth notions, when used as auxiliary tools in estimation and classification, is checked through a Monte Carlo study. © 2007 Springer-Verlag.},
   author = {Manuel Febrero-Bande and Pedro Galeano and Wenceslao González-Manteiga},
   doi = {10.1007/s00180-007-0053-0},
   isbn = {0018000700530},
   issn = {09434062},
   issue = {3},
   journal = {Reports in statistics and operations research},
   keywords = {Depth measures,Functional data,Projections method,Supervised classification},
   pages = {481-496},
   title = {A functional analysis of NOx levels: location and scale estimation and outlier detection},
   volume = {22},
   year = {2007}
}
@article{Fraiman2001,
   abstract = {In practice, the use of functional data is often preferable to that of large finite-dimensional vectors obtained by discrete approximations of functions. In this paper a new concept of data depth is introduced for functional data. The aim is to measure the centrality of a given curve within a group of curves. This concept is used to define ranks and trimmed means for functional data. Some theoretical and practical aspects are discussed and a simulation study is given. The results show a good performance of our method, in terms of efficiency and robustness, when compared with the mean. Finally, a real-data example based on the Nasdaq 100 index is discussed.},
   author = {Ricardo Fraiman and Graciela Muniz},
   doi = {10.1007/BF02595706},
   issn = {11330686},
   issue = {2},
   journal = {Test},
   keywords = {Data depth,Functional data,Trimmed means estimates},
   pages = {419-440},
   title = {Trimmed means for functional data},
   volume = {10},
   year = {2001}
}
@article{Gorecki2019,
   abstract = {Functional data, i.e., observations represented by curves or functions, frequently arise in various fields. The theory and practice of statistical methods for such data is referred to as functional data analysis (FDA) which is one of major research fields in statistics. The practical use of FDA methods is made possible thanks to availability of specialized and usually free software. In particular, a number of R packages is devoted to these methods. In the paper, we introduce a new R package fdANOVA which provides an access to a broad range of global analysis of variance methods for univariate and multivariate functional data. The implemented testing procedures mainly for homoscedastic case are briefly overviewed and illustrated by examples on a well known functional data set. To reduce the computation time, parallel implementation is developed and its efficiency is empirically evaluated. Since some of the implemented tests have not been compared in terms of size control and power yet, appropriate simulations are also conducted. Their results can help in choosing proper testing procedures in practice.},
   author = {Tomasz Górecki and Łukasz Smaga},
   doi = {10.1007/s00180-018-0842-7},
   issn = {16139658},
   issue = {2},
   journal = {Computational Statistics},
   keywords = {Analysis of variance,Functional data,R,fdANOVA},
   pages = {571-597},
   publisher = {Springer Berlin Heidelberg},
   title = {fdANOVA: an R software package for analysis of variance for univariate and multivariate functional data},
   volume = {34},
   url = {https://doi.org/10.1007/s00180-018-0842-7},
   year = {2019}
}
@article{Hanusz2019,
   abstract = {One of the basic statistical methods of dimensionality reduction is analysis of discriminant coordinates given by Fisher (1936) and Rao (1948). The space of discriminant coordinates is a space convenient for presenting multidimensional data originating from multiple groups and for the use of various classification methods (methods of discriminant analysis). In the present paper, we adapt the classical discriminant coordinates analysis to multivariate functional data. The theory has been applied to analysis of textural properties of apples of six varieties, measured over a period of 180 days, stored in two types of refrigeration chamber.},
   author = {Zofia Hanusz and Mirosław Krzyśko and Rafał Nadulski and Łukasz Waszak},
   doi = {10.1080/03610926.2019.1602650},
   issn = {1532415X},
   issue = {0},
   journal = {Communications in Statistics - Theory and Methods},
   keywords = {Multivariate functional data,discriminant coordinates,multivariate analysis,textural properties of apples},
   pages = {1-14},
   publisher = {Taylor \& Francis},
   title = {Discriminant coordinates analysis for multivariate functional data},
   volume = {0},
   url = {https://doi.org/10.1080/03610926.2019.1602650},
   year = {2019}
}
@article{Tsukada2019,
   abstract = {The multivariate two-sample problem has been extensively investigated, and various methods have been proposed. However, most two-sample tests perform poorly when applied to high-dimensional data, and many of them are not applicable when the dimension of the data exceeds the sample size. We reconsider two previously reported tests (Baringhaus and Franz in Stat Sin 20:1333–1361, 2010; Biswas and Ghosh in J Multivar Anal 123:160–171, 2014), and propose two new criteria. Simulations demonstrate that the power of the proposed test is stable for high-dimensional data and large samples, and the power of our test is equivalent to that of the test by Biswas and Ghosh when the covariance matrices are different. We also investigate the theoretical properties of our test when the dimension tends to infinity and the sample size is fixed, and when the dimension is fixed and the sample size tends to infinity. In these cases, the proposed test is asymptotically distribution-free and consistent.},
   author = {Shin ichi Tsukada},
   doi = {10.1007/s00180-017-0777-4},
   isbn = {0018001707774},
   issn = {16139658},
   issue = {2},
   journal = {Computational Statistics},
   keywords = {High-dimensional data,Inter-point distance,Nonparametric test,Two-sample},
   pages = {599-615},
   publisher = {Springer Berlin Heidelberg},
   title = {High dimensional two-sample test based on the inter-point distance},
   volume = {34},
   year = {2019}
}
@article{Pomann,
   abstract = {A nonparametric testing procedure is proposed for testing the hypothesis that two samples of curves observed at discrete grids and with noise have the same underlying distribution. Our method relies on representing the curves using a common orthogonal basis expansion. The approach reduces the dimension of the testing problem in a way that enables the application of traditional non-parametric univariate testing procedures. This procedure is computationally inexpensive, can be easily implemented, and accommodates different sampling designs across the samples. Numerical studies confirm the size and power prop-erties of the test in many realistic scenarios, and furthermore show that the proposed test is more powerful than alternative testing procedures. The pro-posed methodology is illustrated on a state-of-the art diffusion tensor imaging study, where the objective is to compare white matter tract profiles in healthy individuals and multiple sclerosis patients.},
   author = {Gina-Maria Pomann and Ana-Maria Staicu and Sujit Ghosh},
   issue = {1986},
   keywords = {Anderson Darling test,Diffusion tensor imaging,Functional data,Functional principal component analysis,Hypothesis testing,Multiple Sclerosis},
   title = {Two Sample Hypothesis Testing for Functional Data}
}
@article{Berrendero2011,
   abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
   author = {J. R. Berrendero and A. Justel and M. Svarc},
   doi = {10.1016/j.csda.2011.03.011},
   issn = {01679473},
   issue = {9},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Dimension reduction,Eigenvalue functions,Explained variability},
   pages = {2619-2634},
   title = {Principal components for multivariate functional data},
   volume = {55},
   year = {2011}
}
@article{Chakraborty2015,
   abstract = {TheWilcoxon-Mann-Whitney test is a robust competitor of the t test in the univariate setting. For finitedimensional multivariate non-Gaussian data, several extensions of theWilcoxon-Mann-Whitney test have been shown to outperformHotelling's T 2 test. In this paper, we study aWilcoxon-Mann-Whitney-type test based on spatial ranks in infinite-dimensional spaces, we investigate its asymptotic properties and compare it with several existing tests. The proposed test is shown to be robust with respect to outliers and to have better power than some competitors for certain distributions with heavy tails. We study its performance using real and simulated data.},
   author = {Anirvan Chakraborty and Probal Chaudhuri},
   doi = {10.1093/biomet/asu072},
   issn = {14643510},
   issue = {1},
   journal = {Biometrika},
   keywords = {Contaminated data,Functional data,Separable Hilbert space,Spatial rank,Standard Brownian motion,T process,Two-sample problem},
   pages = {239-246},
   title = {A Wilcoxon-Mann-Whitney-type test for infinite-dimensional data},
   volume = {102},
   year = {2015}
}
@article{Cuesta-Albertos2007,
   abstract = {The possibility of considering random projections to identify probability distributions belonging to parametric families is explored. The results are based on considerations involving invariance properties of the family of distributions as well as on the random way of choosing the projections. In particular, it is shown that if a one-dimensional (suitably) randomly chosen projection is Gaussian, then the distribution is Gaussian. In order to show the applicability of the methodology some goodness-of-fit tests based on these ideas are designed. These tests are computationally feasible through the bootstrap setup, even in the functional framework. Simulations providing power comparisons of these projections-based tests with other available tests of normality, as well as to test the Black-Scholes model for a stochastic process are presented. © 2006 Elsevier B.V. All rights reserved.},
   author = {J. A. Cuesta-Albertos and E. del Barrio and R. Fraiman and C. Matrán},
   doi = {10.1016/j.csda.2006.09.007},
   issn = {01679473},
   issue = {10},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Black-Scholes,Families of distributions,Gaussian distributions,Goodness-of-fit tests,Random projections,Stochastic processes},
   pages = {4814-4831},
   title = {The random projection method in goodness of fit for functional data},
   volume = {51},
   year = {2007}
}
@article{Cuevas2006,
   abstract = {The bootstrap methodology for functional data and functional estimation target is considered. A Monte Carlo study analyzing the performance of the bootstrap confidence bands (obtained with different resampling methods) of several functional estimators is presented. Some of these estimators (e.g., the trimmed functional mean) rely on the use of depth notions for functional data and do not have received yet much attention in the literature. A real data example in cardiology research is also analyzed. In a more theoretical aspect, a brief discussion is given providing some insights on the asymptotic validity of the bootstrap methodology when functional data, as well as a functional parameter, are involved. © 2005 Elsevier B.V. All rights reserved.},
   author = {Antonio Cuevas and Manuel Febrero and Ricardo Fraiman},
   doi = {10.1016/j.csda.2005.10.012},
   issn = {01679473},
   issue = {2},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Bootstrap,Bootstrap validity,Functional data,Functional median,Smoothed bootstrap,Trimmed functional mean},
   pages = {1063-1074},
   title = {On the use of the bootstrap for estimating functions with functional data},
   volume = {51},
   year = {2006}
}
@article{Rousseeuw1999,
   abstract = {Tukey (1975) introduced the notion of halfspace depth in a data analytic context, as a multivariate analog of rank relative to a finite data set. Here we focus on the depth function of an arbitrary probability distribution on ℝp, and even of a non-probability measure. The halfspace depth of any point θ in ℝp is the smallest measure of a closed halfspace that contains θ. We review the properties of halfspace depth, enriched with some new results. For various measures, uniform as well as non-uniform, we derive an expression for the depth function. We also compute the Tukey median, which is the θ in which the depth function attains its maximal value. Various interesting phenomena occur. For the uniform distribution on a triangle, a square or any regular polygon, the depth function has ridges that correspond to an 'inversion' of depth contours. And for a product of Cauchy distributions, the depth contours are squares. We also consider an application of the depth function to voting theory.},
   author = {Peter J. Rousseeuw and Ida Ruts},
   doi = {10.1007/PL00020903},
   issn = {00261335},
   issue = {3},
   journal = {Metrika},
   keywords = {Depth contour,Halfspace depth,Location depth,Tukey median,Voting},
   pages = {213-244},
   title = {The depth function of a population distribution},
   volume = {49},
   year = {1999}
}
@article{Cuesta-Albertos2008,
   abstract = {The computation of the Tukey depth, also called halfspace depth, is very demanding, even in low dimensional spaces, because it requires that all possible one-dimensional projections be considered. A random depth which approximates the Tukey depth is proposed. It only takes into account a finite number of one-dimensional projections which are chosen at random. Thus, this random depth requires a reasonable computation time even in high dimensional spaces. Moreover, it is easily extended to cover the functional framework. Some simulations indicating how many projections should be considered depending on the kind of problem, sample size and dimension of the sample space among others are presented. It is noteworthy that the random depth, based on a very low number of projections, obtains results very similar to those obtained with the Tukey depth. © 2008 Elsevier B.V. All rights reserved.},
   author = {J. A. Cuesta-Albertos and A. Nieto-Reyes},
   doi = {10.1016/j.csda.2008.04.021},
   issn = {01679473},
   issue = {11},
   journal = {Computational Statistics and Data Analysis},
   keywords = {1980 subject classification,62g07,62g35,a,and phrases,functional data,homogeneity test,m,multidimen-,one-dimensional projections,primary 62h05,random tukey depth,s,secondary,sional data,supervised classification},
   pages = {4979-4988},
   title = {The random Tukey depth},
   volume = {52},
   year = {2008}
}
@article{Sguera2014,
   abstract = {We enlarge the number of available functional depths by introducing the kernelized functional spatial depth (KFSD). KFSD is a local-oriented and kernel-based version of the recently proposed functional spatial depth (FSD) that may be useful for studying functional samples that require an analysis at a local level. In addition, we consider supervised functional classification problems, focusing on cases in which the differences between groups are not extremely clear-cut or the data may contain outlying curves. We perform classification by means of some available robust methods that involve the use of a given functional depth, including FSD and KFSD, among others. We use the functional k-nearest neighbor classifier as a benchmark procedure. The results of a simulation study indicate that the KFSD-based classification approach leads to good results. Finally, we consider two real classification problems, obtaining results that are consistent with the findings observed with simulated curves.},
   author = {Carlo Sguera and Pedro Galeano and Rosa Lillo},
   doi = {10.1007/s11749-014-0379-1},
   isbn = {1174901403},
   issn = {11330686},
   issue = {4},
   journal = {Test},
   keywords = {Functional depths,Functional outliers,Functional spatial depth,Kernelized functional spatial depth,Supervised functional classification},
   pages = {725-750},
   title = {Spatial depth-based classification for functional data},
   volume = {23},
   year = {2014}
}
@article{Liang2020,
   abstract = {Abstract–Severe air pollution affects billions of people around the world, particularly in developing countries such as China. Effective emission control policies rely primarily on a proper assessment of air pollutants and accurate spatial clustering outcomes. Unfortunately, emission patterns are difficult to observe as they are highly confounded by many meteorological and geographical factors. In this study, we propose a novel approach for modeling and clustering PM (Formula presented.) concentrations across China. We model observed concentrations from monitoring stations as spatially dependent functional data and assume latent emission processes originate from a functional mixture model with each component as a spatio-temporal process. Cluster memberships of monitoring stations are modeled as a Markov random field, in which confounding effects are controlled through energy functions. The superior performance of our approach is demonstrated using extensive simulation studies. Our method is effective in dividing China and the Beijing-Tianjin-Hebei region into several regions based on PM (Formula presented.) concentrations, suggesting that separate local emission control policies are needed. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
   author = {Decai Liang and Haozhe Zhang and Xiaohui Chang and Hui Huang},
   doi = {10.1080/01621459.2020.1764363},
   issn = {1537274X},
   issue = {0},
   journal = {Journal of the American Statistical Association},
   keywords = {Environmental policies,Latent emission process,Markov random field,Model-based clustering},
   pages = {1-70},
   publisher = {Taylor \& Francis},
   title = {Modeling and Regionalization of China’s PM2.5 Using Spatial-Functional Mixture Models},
   volume = {0},
   url = {https://doi.org/10.1080/01621459.2020.1764363},
   year = {2020}
}
@article{Wang2020,
   abstract = {In recent years, with rapid industrialization and massive energy consumption, ground-level ozone (O3) has become one of the most severe air pollutants. In this paper, we propose a functional spatio-temporal statistical model to analyze air quality data. Firstly, since the pollutant data from the monitoring network usually have a strong spatial and temporal correlation, the spatio-temporal statistical model is a reasonable method to reveal spatial correlation structure and temporal dynamic mechanism in data. Secondly, effects from the covariates are introduced to explore the formation mechanism of ozone pollution. Thirdly, considering the obvious diurnal pattern of ozone data, we explore the diurnal cycle of O3 pollution using the functional data analysis approach. The spatio-temporal model shows great applicational potential by comparison with other models. With application to O3 pollution data of 36 stations in Beijing, China, we give explanations of the covariate effects on ozone pollution, such as other pollutants and meteorological variables, and meanwhile we discuss the diurnal cycle of ozone pollution.},
   author = {Yaqiong Wang and Ke Xu and Shaomin Li},
   doi = {10.3390/ijerph17093172},
   issn = {16604601},
   issue = {9},
   journal = {International Journal of Environmental Research and Public Health},
   keywords = {Functional data analysis,O3 pollution,Spatio-temporal statistical model},
   pmid = {32370183},
   title = {The functional spatio-temporal statistical model with application to O3 pollution in Beijing, China},
   volume = {17},
   year = {2020}
}
@article{Torres2020,
   abstract = {Ground level concentrations of nitrogen oxide (NOx) can act as an indicator of air quality in the urban environment. In cities with relatively good air quality, and where NOx concentrations rarely exceed legal limits, adverse health effects on the population may still occur. Therefore, detecting small deviations in air quality and deriving methods of controlling air pollution are challenging. This study presents different data analytical methods which can be used to monitor and effectively evaluate policies or measures to reduce nitrogen oxide (NOx) emissions through the detection of pollution episodes and the removal of outliers. This method helps to identify the sources of pollution more effectively, and enhances the value of monitoring data and exceedances of limit values. It will detect outliers, changes and trend deviations in NO2 concentrations at ground level, and consists of four main steps: classical statistical description techniques, statistical process control techniques, functional analysis and a functional control process. To demonstrate the effectiveness of the outlier detection methodology proposed, it was applied to a complete one-year NO2 dataset for a sub-urban site in Dublin, Ireland in 2013. The findings demonstrate how the functional data approach improves the classical techniques for detecting outliers, and in addition, how this new methodology can facilitate a more thorough approach to defining effect air pollution control measures.},
   author = {Javier Martínez Torres and Jorge Pastor Pérez and Joaquín Sancho Val and Aonghus McNabola and Miguel Martínez Comesaña and John Gallagher},
   doi = {10.3390/math8020225},
   issn = {22277390},
   issue = {2},
   journal = {Mathematics},
   keywords = {Air pollution,Functional data analysis,Non-normal data,Outlier,Statistical process control},
   title = {A functional data analysis approach for the detection of air pollution episodes and outliers: A case study in Dublin, Ireland},
   volume = {8},
   year = {2020}
}
@article{Melendez2020,
   abstract = {Los datos recopilados en el monitoreo de la contaminación del aire, como PM10, se obtienen en estaciones automatizadas que generalmente contenían valores faltantes debido a fallas de la máquina, mantenimiento de rutina o errores humanos. Los conjuntos de datos incompletos pueden causar sesgo de información, por lo tanto, es importante encontrar la mejor manera de estimar estos valores faltantes para garantizar la calidad de los datos analizados. En este trabajo se evaluaron los datos de partículas PM10consideradas en el tiempo como un objeto funcional, para este caso se utilizó la base de datos de la red de monitoreo ambiental de la Corporación Ambiental de La Guajira (Corpoguajira). En este estudio hemos implementado la metodología de Jeng-Min Chiou,(2014) para imputar datos funcionales. La detección de valores atípicos de contaminantes es muy importante para el monitoreo y control de la calidad del aire. Además, hemos implementado el método de imputación de datos faltantes y detección de valores atípicos para datos funcionales. Consideramos las concentraciones de partículas PM10en las estaciones de monitoreo ambiental sobre el área de influencia de la mina de carbón a cielo abierto durante 2012. Para imputar datos faltantes funcionales, se basó en la aplicación de herramientas como el análisis de componentes principales funcional (ACPF) y los procedimientos gráficos para detectar curvas de valores atípicos como el bagplot funcional y el diagrama de caja funcional de la región de mayor densidad (HDR) por sus siglas en ingles. Los resultados indican que la estación de Barranca es una curva atípica y se observó que los intervalos imputados capturan la dinámica que se comparte con las otras trayectorias de las diferentes estaciones},
   author = {Rafael Meléndez and Stevenso N Bolívar and Roberto Rojano},
   doi = {10.18273/revuin.v19n2-2020001},
   issn = {16574583},
   issue = {2},
   journal = {Revista UIS Ingenierías},
   keywords = {análisis de componentes principales,datos funcionales,funcionales,valores atípicos funcionales},
   pages = {1-10},
   title = {Imputación de valores perdidos y detección de valores atípicos en datos funcionales: una aplicación con datos de PM10},
   volume = {19},
   year = {2020}
}
@article{Sun2011,
   abstract = {This article proposes an informative exploratory tool, the functional boxplot, for visualizing functional data, as well as its generalization, the enhanced functional boxplot. Based on the center outward ordering induced by band depth for functional data, the descriptive statistics of a functional boxplot are: the envelope of the 50% central region, the median curve, and the maximum non-outlying envelope. In addition, outliers can be detected in a functional boxplot by the 1.5 times the 50% central region empirical rule, analogous to the rule for classical boxplots. The construction of a functional boxplot is illustrated on a series of sea surface temperatures related to the El Niño phenomenon and its outlier detection performance is explored by simulations. As applications, the functional boxplot and enhanced functional boxplot are demonstrated on children growth data and spatio-temporal U.S. precipitation data for nine climatic regions, respectively. This article has supplementary material online. © 2011 American Statistical Association.},
   author = {Ying Sun and Marc G. Genton},
   doi = {10.1198/jcgs.2011.09224},
   issn = {10618600},
   issue = {2},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Depth,Functional data,Growth data,Precipitation data,Space-time data,Visualization},
   pages = {316-334},
   title = {Functional boxplots},
   volume = {20},
   year = {2011}
}
@article{Ieva2019,
   abstract = {The focus of this paper is on the open-source R package roahd (RObust Analysis of High dimensional Data), see Tarabelloni et al. (2017). roahd has been developed to gather recently proposed statistical methods that deal with the robust inferential analysis of univariate and multivariate functional data. In particular, efficient methods for outlier detection and related graphical tools, methods to represent and simulate functional data, as well as inferential tools for testing differences and dependency among families of curves will be discussed, and the associated functions of the package will be described in details.},
   author = {Francesca Ieva and Anna Maria Paganoni and Juan Romo and Nicholas Tarabelloni},
   doi = {10.32614/RJ-2019-032},
   issn = {20734859},
   issue = {2},
   journal = {R Journal},
   pages = {291-307},
   title = {Roahd package: Robust analysis of high dimensional data},
   volume = {11},
   year = {2019}
}
@article{Hyndman2010,
   abstract = {We propose new tools for visualizing large amounts of functional data in the form of smooth curves. The proposed tools include functional versions of the bagplot and boxplot, which make use of the first two robust principal component scores, Tukey's data depth and highest density regions. By-products of our graphical displays are outlier detection methods for functional data. We compare these new outlier detection methods with existing methods for detecting outliers in functional data, and show that our methods are better able to identify outliers. An R-package containing computer code and datasets is available in the online supplements. Copyright © 2010 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
   author = {Rob J. Hyndman and Han Lin Shang},
   doi = {10.1198/jcgs.2009.08158},
   issn = {10618600},
   issue = {1},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Highest density regions,Kernel density estimation,Outlier detection,Robust principal component analysis,Tukey's halfspace location depth},
   pages = {29-45},
   title = {Rainbow plots, bagplots, and boxplots for functional data},
   volume = {19},
   year = {2010}
}
@article{Martinez2014,
   abstract = {Polluted air of cities is a harmful factor to health that may eventually cause respiratory problems and cardiovascular disease. The monitoring and control of pollutants is an essential activity in order to protect the environment and the health by minimizing pollution levels through the detection of contaminants. Contaminants are emissions of substances to the atmosphere (mainly gases and particulate matter) whose values are greater than the limits allowed by the environmental legislation (they are anomalous values). Thus they are considered as vector samples where each component represents the gas concentration value in the air. In this sense, a model based on functional analysis has been implemented for the outliers detection in air quality samples in this research work. This model transforms the vectorial sample by creating a new functional sample in order to determine functional outliers by adjusting the concept of depth to the functional event. This method has been compared to classical outliers analysis from a vectorial point of view, emphasizing the power of use of such functional techniques over the traditional ones. The main aim of this research work is to compare the results corresponding to the classical and the functional methods and to obtain the most appropriate methodology to analyze this type of dataset in order to reach a better solution for the air quality control. © 2014 Elsevier Inc. All rights reserved.},
   author = {J. Martínez and Á Saavedra and P. J. García-Nieto and J. I. Piñeiro and C. Iglesias and J. Taboada and J. Sancho and J. Pastor},
   doi = {10.1016/j.amc.2014.05.004},
   issn = {00963003},
   issue = {2},
   journal = {Applied Mathematics and Computation},
   keywords = {Air quality control,Functional analysis,Gas pollution,Outliers detection},
   pages = {1-10},
   title = {Air quality parameters outliers detection using functional data analysis in the Langreo urban area (Northern Spain)},
   volume = {241},
   year = {2014}
}
@article{Paredes2019,
   author = {Daniela Paredes},
   issue = {34},
   pages = {2-3},
   title = {Mecanismos de pago en salud},
   year = {2019}
}
@article{Torres2011,
   abstract = {In this work a solution for the problem of the detection of outliers in gas emissions in urban areas that uses functional data analysis is described. Different methodologies for outlier identification have been applied in air pollution studies, with gas emissions considered as vectors whose components are gas concentration values for each observation made. In our methodology we consider gas emissions over time as curves, with outliers obtained by a comparison of curves instead of vectors. The methodology, which is based on the concept of functional depth, was applied to the detection of outliers in gas omissions in the city of Oviedo and results were compared with those obtained using a conventional method based on a comparison of vectors. Finally, the advantages of the functional method are reported. © 2010 Elsevier B.V.},
   author = {J. Martínez Torres and P. J.Garcia Nieto and L. Alejano and A. N. Reyes},
   doi = {10.1016/j.jhazmat.2010.10.091},
   issn = {03043894},
   issue = {1},
   journal = {Journal of Hazardous Materials},
   keywords = {Air pollution,Functional data analysis,Functional depth,Gas emissions,Outliers},
   pages = {144-149},
   pmid = {21112150},
   title = {Detection of outliers in gas emissions from urban areas using functional data analysis},
   volume = {186},
   year = {2011}
}
@article{Quintela-del-Rio2011,
   abstract = {The study of extreme values and prediction of ozone data is an important topic of research when dealing with environmental problems. Classical extreme value theory is usually used in air-pollution studies. It consists in fitting a parametric generalised extreme value (GEV) distribution to a data set of extreme values, and using the estimated distribution to compute return levels and other quantities of interest. Here, we propose to estimate these values using nonparametric functional data methods. Functional data analysis is a relatively new statistical methodology that generally deals with data consisting of curves or multi-dimensional variables. In this paper, we use this technique, jointly with nonparametric curve estimation, to provide alternatives to the usual parametric statistical tools. The nonparametric estimators are applied to real samples of maximum ozone values obtained from several monitoring stations belonging to the Automatic Urban and Rural Network (AURN) in the UK. The results show that nonparametric estimators work satisfactorily, outperforming the behaviour of classical parametric estimators. Functional data analysis is also used to predict stratospheric ozone concentrations. We show an application, using the data set of mean monthly ozone concentrations in Arosa, Switzerland, and the results are compared with those obtained by classical time series (ARIMA) analysis. © 2010 Elsevier Ltd.},
   author = {Alejandro Quintela-del-Río and Mario Francisco-Fernández},
   doi = {10.1016/j.chemosphere.2010.11.025},
   issn = {00456535},
   issue = {6},
   journal = {Chemosphere},
   keywords = {Air-pollution,Extreme values,Nonparametric functional estimation,Ozone,Prediction},
   pages = {800-808},
   pmid = {21144549},
   publisher = {Elsevier Ltd},
   title = {Nonparametric functional data estimation applied to ozone data: Prediction and extreme value analysis},
   volume = {82},
   url = {http://dx.doi.org/10.1016/j.chemosphere.2010.11.025},
   year = {2011}
}
@article{Shaadan2012,
   abstract = {This study highlights the advantage of functional data approach in assessing and comparing the PM 10 pollutant behaviour as an alternative statistical approach during and between the two extreme haze years (1997 and 2005) that have been reported in Selangor, state of Malaysia. The aim of the study was to improvise the current conventional methods used in air quality assessment so that any unforeseen implicit information can be revealed and the previous research findings can be justified. An analysis based on the daily diurnal curves in place of discrete point values was performed. The analysis results provided evidences of the influence of the change in the climate (due to the El-Nino event), the different levels of different emission sources and meteorological conditions on the severity of the PM 10 problem. By means of the cummulative exceedence index and the functional depth method, most of the monitoring stations for the year 2005 experienced the worst day of critical exceedences on the 10 th of August, while for the year 1997 it occurred between 13 th and 26 th September inclusively at different dates among the stations.},
   author = {Norshahida Shaadan and Sayang Mohd Deni and Abd Aziz Jemain},
   issn = {01266039},
   issue = {11},
   journal = {Sains Malaysiana},
   keywords = {Air quality,Behaviour,Exceedences,Functional data,PM 10 pollutant},
   pages = {1335-1344},
   title = {Assessing and comparing PM10 pollutant behaviour using functional data approach},
   volume = {41},
   year = {2012}
}
@article{Shaadan2015,
   abstract = {In environmental data sets, the occurrence of a high concentration of an unusual pollutant, more formally known as an anomaly, may indicate air quality problems. Thus, a critical understanding of the behavior of anomalies is increasingly becoming very important for air pollution investigations. This study was conducted to detect anomalies in daily PM10 functional data, to investigate the patterns of behavior as well as to identify possible factors that determine PM10 anomalies at three selected air quality monitoring stations (Klang, Kuala Selangor and Petaling Jaya) in the Klang Valley, Malaysia. The statistical method employed to detect these anomalies consisted of a combination of the robust projection pursuit and the robust Mahalanobis distance methods using air quality data recorded from 2005 to 2010. Analysis of obtained anomalous PM10 profiles showed that data recorded during El Nino years (2005, 2006 and 2009) contained the highest frequency of anomalies. More frequent anomalies appeared during the southwest (SW) monsoon which occurs in the months of July and August as well as during the northeast (NE) monsoon in February. A lesser number of anomalies were also observed during weekends compared to weekdays. The weekend and monsoonal effect phenomena were shown to be significantly existent at all stations while wind speed was positively associated with extreme PM10 anomalies at the Klang and Petaling Jaya stations. In conclusion, anomalies detection was found useful for air pollution investigation in this study. The findings of this study imply that the location and background of a station, as well as wind speed, seasonal (monsoon) and weekdays-weekend variations play important role in influencing PM10 anomalies.},
   author = {Norshahida Shaadan and Abdul Aziz Jemain and Mohd Talib Latif and Sayang Mohd Deni},
   doi = {10.5094/APR.2015.040},
   issn = {13091042},
   issue = {2},
   journal = {Atmospheric Pollution Research},
   keywords = {Air quality monitoring,Anomaly detection,Functional data,PM10},
   pages = {365-375},
   publisher = {Elsevier},
   title = {Anomaly detection and assessment of PM10 functional data at several locations in the Klang Valley, Malaysia},
   volume = {6},
   url = {http://dx.doi.org/10.5094/APR.2015.040},
   year = {2015}
}
@article{Sanchez-Lasheras2020,
   abstract = {For more than a century, air pollution has been one of the most important environmental problems in cities. Pollution is a threat to human health and is responsible for many deaths every year all over the world. This paper deals with the topic of functional outlier detection. Functional analysis is a novel mathematical tool employed for the recognition of outliers. This methodology is applied here to the emissions of a coal-fired power plant. This research uses two different methods, called functional high-density region (HDR) boxplot and functional bagplot. Please note that functional bagplots were developed using bivariate bagplots as a starting point. Indeed, they are applied to the first two robust principal component scores. Both methodologies were applied for the detection of outliers in the time pollutant emission curves that were built using, as inputs, the discrete information available from an air quality monitoring data record station and the subsequent smoothing of this dataset for each pollutant. In this research, both new methodologies are tested to detect outliers in pollutant emissions performed over a long period of time in an urban area. These pollutant emissions have been treated in order to use them as vectors whose components are pollutant concentration values for each observation made. Note that although the recording of pollutant emissions is made in a discrete way, these methodologies use pollutants as curves, identifying the outliers by a comparison of curves rather than vectors. Then, the concept of outlier goes from being a point to a curve that employs the functional depth as the indicator of curve distance. In this study, it is applied to the detection of outliers in pollutant emissions from a coal-fired power plant located on the outskirts of the city of Oviedo, located in the north of Spain and capital of the Principality of Asturias. Also, strengths of the functional methods are explained.},
   author = {Fernando Sánchez-Lasheras and Celestino Ordóñez-Galán and Paulino José García-Nieto and Esperanza García-Gonzalo},
   doi = {10.1007/s11356-019-04435-4},
   issn = {16147499},
   issue = {1},
   journal = {Environmental Science and Pollution Research},
   keywords = {Air pollution,Functional bagplot,Functional data analysis,Functional high-density region (HDR) boxplot,Gas emissions,Outlier detection},
   pages = {8-20},
   pmid = {30771125},
   title = {Detection of outliers in pollutant emissions from the Soto de Ribera coal-fired power plant using functional data analysis: a case study in northern Spain},
   volume = {27},
   year = {2020}
}
@article{Liu1990,
   author = {Regina Y. Liu},
   issue = {1},
   journal = {Annals of Statistics},
   pages = {1403-405-414},
   title = {On a notion of data depth based on random simplices},
   volume = {18},
   year = {1990}
}
@article{Lopez-Pintado2011,
   abstract = {A new definition of depth for functional observations is introduced based on the notion of "half-region" determined by a curve. The half-region depth provides a simple and natural criterion to measure the centrality of a function within a sample of curves. It has computational advantages relative to other concepts of depth previously proposed in the literature which makes it applicable to the analysis of high-dimensional data. Based on this depth a sample of curves can be ordered from the center-outward and order statistics can be defined. The properties of the half-region depth, such as consistency and uniform convergence, are established. A simulation study shows the robustness of this new definition of depth when the curves are contaminated. Finally, real data examples are analyzed. © 2010 Published by Elsevier B.V.},
   author = {Sara López-Pintado and Juan Romo},
   doi = {10.1016/j.csda.2010.10.024},
   issn = {01679473},
   issue = {4},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Data depth,Functional data,High-dimensional data,Order statistics},
   pages = {1679-1695},
   publisher = {Elsevier B.V.},
   title = {A half-region depth for functional data},
   volume = {55},
   url = {http://dx.doi.org/10.1016/j.csda.2010.10.024},
   year = {2011}
}
@article{Hubert2020,
   author = {Mia Hubert and Peter Rousseeuw and Jakob Raymaekers},
   title = {Package ‘ mrfDepth ’},
   year = {2020}
}
@article{Rousseeuw1999a,
   author = {Peter J. Rousseeuw and Ida Ruts and John W. Tukey},
   issue = {4},
   journal = {Statistical Computing and Graphics},
   pages = {382-387},
   title = {The Bagplot: A Bivariate Boxplot},
   volume = {53},
   year = {1999}
}
@article{Hubert2008,
   abstract = {The boxplot is a very popular graphical tool for visualizing the distribution of continuous unimodal data. It shows information about the location, spread, skewness as well as the tails of the data. However, when the data are skewed, usually many points exceed the whiskers and are often erroneously declared as outliers. An adjustment of the boxplot is presented that includes a robust measure of skewness in the determination of the whiskers. This results in a more accurate representation of the data and of possible outliers. Consequently, this adjusted boxplot can also be used as a fast and automatic outlier detection tool without making any parametric assumption about the distribution of the bulk of the data. Several examples and simulation results show the advantages of this new procedure. © 2007 Elsevier B.V. All rights reserved.},
   author = {M. Hubert and E. Vandervieren},
   doi = {10.1016/j.csda.2007.11.008},
   issn = {01679473},
   issue = {12},
   journal = {Computational Statistics and Data Analysis},
   pages = {5186-5201},
   title = {An adjusted boxplot for skewed distributions},
   volume = {52},
   year = {2008}
}
@article{Pez-Pintado2009a,
   abstract = {The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the "centrality" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect "shape" outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls. © 2009 American Statistical Association.},
   author = {Sara LÓ Pez-Pintado and Juan Romo},
   doi = {10.1198/jasa.2009.0108},
   issn = {01621459},
   issue = {486},
   journal = {Journal of the American Statistical Association},
   keywords = {Data depth,Functional data,Rank test for functions},
   pages = {718-734},
   title = {On the concept of depth for functional data},
   volume = {104},
   year = {2009}
}
@article{Card2019,
   author = {Reference Card and Group Math},
   issue = {Cv},
   title = {Inner product and norm for FD fdata class objects Functional Non-Linear Models RPubs document Functional Non-Supervised Classification},
   year = {2019}
}
@article{Jentzsch2015,
   abstract = {Anti-insurgent militias and states attempt to erode insurgent groups’ capacities and co-opt insurgent fighters by promising and providing benefits. They do so to create a perception that the insurgency is unraveling and to harness inside information to prosecute more effective counterinsurgency campaigns. Why do some insurgents defect to a paramilitary group and others exit the war by demobilizing, while still others remain loyal to their group? This article presents the first empirical analysis of these questions, connecting insurgents’ motivations for joining, wartime experiences, and organizational behavior with decisions to defect. A survey of ex-combatants in Colombia shows that individuals who joined for ideological reasons are less likely to defect overall but more likely to side-switch or demobilize when their group deviates from its ideological precepts. Among fighters who joined for economic reasons, political indoctrination works to decrease their chances of demobilization and defection to paramilitaries, while opportunities for looting decrease economically motivated combatants’ odds of defection.},
   author = {Ben Oppenheim and Abbey Steele and Juan F. Vargas and Michael Weintraub},
   doi = {10.1177/0022002715576750},
   isbn = {0022002715},
   issn = {15528766},
   issue = {5},
   journal = {Journal of Conflict Resolution},
   keywords = {Colombia,civil wars,defection,demobilization,ideology,insurgency,internal armed conflict,militias,side switching},
   pages = {794-823},
   title = {True Believers, Deserters, and Traitors: Who Leaves Insurgent Groups and Why},
   volume = {59},
   year = {2015}
}
@article{Crisp2002,
   abstract = {Representation can vary depending on whether legislators view constituents as best represented by aggregated, programmatic universal policies or by parochial, particularistic policies. In 1991 Colombia adopted a major institutional reform intended to change the "electoral connection" between voters and senators, encouraging members of the upper chamber to adopt a more national, programmatic vision. We explain variations in geographic patterns of electoral support in the post-reform era and show how the spatial pattern of votes for a senator influences his or her "hill style" in terms of bill-initiation priorities. Although reformers created the option of a more dispersed pattern of support, it is still possible for senate candidates to gain election with geographically concentrated constituencies. These senators have a higher probability of initiating bills with a pork-barrel propensity.},
   author = {Brian Crisp and Rachael E. Ingall},
   doi = {10.2307/3088430},
   issn = {00925853},
   issue = {4},
   journal = {American Journal of Political Science},
   pages = {733},
   title = {Institutional Engineering and the Nature of Representation: Mapping the Effects of Electoral Reform in Colombia},
   volume = {46},
   year = {2002}
}
@article{Gutierrez2004,
   abstract = {El artículo plantea que el conflicto colombiano parece ser un ejemplo típico de una “guerra codiciosa”, y exhibe lazos fuertes entre actividades económicas ilícitas y organizaciones insurgentes. A la vez, argumenta que la interpretación de la guerrilla como una simple expresión criminal ― tal como lo propone Paul Collier ― es errónea, incluso en un conflicto tan criminalizado como el colombiano. Para esto, se discuten algunas de las inconsistencias de las tesis de Collier, y las razones por las cuales son inaplicables al país.},
   author = {Francisco Gutiérrez},
   issn = {0121-5167},
   issue = {24},
   journal = {Estudios Políticos},
   pages = {37-71},
   title = {Criminales y rebeldes: una discusión de la economía política del conflicto armado desde el caso colombiano.},
   volume = {0},
   url = {http://aprendeenlinea.udea.edu.co/revistas/index.php/estudiospoliticos/article/view/1362},
   year = {2004}
}
@article{Salazar2011,
   author = {José de Jesús Salazar and José Polendo and Jorge Ibarra},
   isbn = {1332109500},
   issue = {II Semestre},
   journal = {Gestión y Política Pública},
   keywords = {Convenios intermunicipales,Policía Metropolitana,seguridad pública,área conurbana Almatam.},
   pages = {433-457},
   title = {Convenios intermunicipales: El efecto de la Policía Metropolitana del área conurbada Almatam},
   volume = {XX},
   year = {2011}
}
@article{PinoMontoya2015,
   abstract = {En todo proceso de investigación social el investigador debe de optar por un enfoque que le permita explicarde forma adecuada el fenómeno particular que va a estudiar. Este artículo pretende exponer el paradigmaempírico como un enfoque acorde con los estudios realizados por las ciencias políticas. A través dela metodología se privilegió el enfoque cualitativo de investigación, el rastreo y el análisis documental conel fin de indagar sobre las perspectivas metodológicas utilizadas por las ciencias políticas. En los resultadosse encuentra que el paradigma empírico analítico como metodología de investigación es pertinente paraser utilizado en los estudios políticos. Finalmente en la conclusión las ciencias políticas, además de utilizarel paradigma empírico analítico de investigación debe utilizar otros enfoques investigativos que amplíen suvisión, para evitar el simplismo y llegar a una explicación más objetiva e integral.},
   author = {José Wilmar Pino Montoya},
   doi = {10.21501/23823410.1671},
   issn = {2382-3410},
   issue = {2},
   journal = {Revista Fundación Universitaria Luis Amigó},
   keywords = {ciencia,ciencia política,empírico analítico,experimento,revolución},
   pages = {185},
   title = {Metodología De Investigación En La Ciencia Política: La Mirada Empírico Analítica},
   volume = {2},
   year = {2015}
}
@article{Burgos2020,
   author = {Preciado Burgos and Victoria Amalia},
   issn = {2477-9555},
   issue = {1},
   journal = {Utopía y Praxis Latinoamericana},
   keywords = {Estado,Resocialización,centro carcelario,derechos fundamentales,fundamental rights,prison center,recidivism.,reincidencia. / Resocialization,state},
   pages = {139-153},
   title = {Educación o resocialización: Problemática abordada desde la administración penitenciaria en Colombia.},
   volume = {25},
   year = {2020}
}
@article{GomezGomez2001,
   abstract = {Gomez, C.M., (2001). Economía y Violencia en Colombia. En A. Martinez Ortiz (Ed.) Economía Crimen Conflicto. Bogotá, Colombia: Universidad Nacional de Colombia, 41–58.},
   author = {Carlos Mario Gómez Gómez},
   issn = {1575-4227},
   issue = {2},
   journal = {Quórum: revista de pensamiento iberoamericano},
   keywords = {CIENCIAS SOCIALES,Political science,Política,SOCIAL SCIENCES},
   pages = {159-173},
   title = {Economía y violencia en Colombia},
   year = {2001}
}
@book{Rincon2007,
   abstract = {Una versión actualizada del presente texto se encuentra disponible en formato electrónico en la dirección http://www.matematicas.unam.mx/lars Prólogo El presente texto constituye el material completo del curso semestral de Proba-bilidad y Estadística, impartido por el autor a alumnos de la licenciatura en ciencias de la computación en la Facultad de Ciencias de la UNAM. Contiene el te-mario básico para un curso elemental e introductorio a algunos temas tradicionales de la probabilidad y la estadística, así como una colección de ejercicios. Algunos de estos ejercicios aparecen a lo largo del texto como parte de la lectura, y una colección más extensa aparece al final del libro incluyendo algunas soluciones o sugerencias para resolverlos. El texto está dirigido de manera general a alumnos de las distintas carreras de ingeniería, ciencias de la computación, y otras carreras científicas similares, cuyos programas de estudio contemplan un semestre introductorio a estos temas. Como es natural en este tipo de cursos, no se hacé enfasis en el rigor matemático de la demostración de los resultados, sino en el uso, interpretación y aplicación dé estos. Como prerequisitos para una lectura provechosa de este material, se requiere, en determinados momentos, tener cierta familiaridad con algunos conceptos elementa-les dé algebra y del cálculo diferencial e integral. El texto fue escrito en el sistema L A T E X, y la mayoría de las ilustraciones fueron elaboradas usando el paquete ps-tricks. El autor agradece cualquier comentario, sugerencia o corrección enviada al correo electrónico que aparece abajo.},
   author = {Luis Rincón},
   isbn = {978-607-02-4092-8/pbk},
   journal = {Notas de Matemáticas},
   pages = {1 - 175},
   title = {Curso elemental de probabilidad y estadística},
   url = {http://www.cimat.mx/~pabreu/LuisRinconI.pdf},
   year = {2007}
}
@article{Restrepo2004,
   abstract = {We present a detailed, high-frequency data set on the civil conflict in Colombia during the period 1988-2002. We briefly introduce the Colombian case and the methodological issues that hinder data collection in civil wars, before presenting the pattern over time of conflict actions and intensity for all sides involved in the confrontation. We also describe the pattern of victimisation by group and the victimisation of civilians out of clashes.},
   author = {J Restrepo and M Spagat and J Vargas},
   isbn = {3892650381},
   issue = {2},
   journal = {Homo Oeconomicus},
   keywords = {Colombia,Conflict,Faculty of History and Social Science\Economics,Guerrilla,Warfare},
   pages = {396-428},
   title = {The Dynamics of the Colombian Civil Conflict: A New Data Set},
   volume = {21},
   url = {http://personal.rhul.ac.uk/uhte/014/Research.htm},
   year = {2004}
}
@book{AnduizaPerea2009,
   author = {Eva Anduiza Perea},
   pages = {149},
   title = {Metodología de la ciencia política},
   year = {2009}
}
@book{Rincon2017,
   author = {Luis Rincón},
   isbn = {9788578110796},
   issn = {1098-6596},
   keywords = {icle},
   pmid = {25246403},
   title = {Estadística Descriptiva},
   year = {2017}
}
@book{Rinc,
   author = {Luis Rincón},
   isbn = {9786073024327},
   pages = {416},
   title = {Una introducción a la estadística inferencial},
   url = {http://lya.fciencias.unam.mx/lars/Publicaciones/ei2019.pdf},
   year = {2019}
}
@article{Fergusson2016,
   author = {Leopoldo Fergusson and Nelson A Ruiz-guarin and Juan F Vargas},
   issue = {4},
   journal = {American Journal of Political Science},
   keywords = {adriana camacho,angulo,catherine boone,clau-,democracy,elections,emilio depetris,ernesto dal-b,for their helpful comments,guadalupe dorna,juan dubra,laura bronner,marcela eslava,o,political inclusion,regression discontinuity,tim besley,violence,we thank juan carlos},
   pages = {1-65},
   title = {The Real Winner ’ s Curse},
   year = {2020}
}
@book{VariosAutores2014,
   author = {Varios Autores},
   city = {Madrid},
   publisher = {Ariel},
   title = {Diez Textos Básicos de Ciencia Política},
   year = {2014}
}
@article{Elster2005,
   abstract = {Some issues have also a distinctive title.},
   author = {Jon Elster},
   issn = {0187-0173},
   issue = {57},
   journal = {Sociológica},
   keywords = {anthropology,social science,sociology},
   note = {No eisten leyes generales en las ciencias sociales. <br/><br/>Las disciplinas no se limitan a la descripción y narración de los fenómenos.},
   pages = {239-273},
   title = {En favor de los mecanismos},
   volume = {20},
   year = {2005}
}
@article{MariaEncarnacaoBeltraoSposito2013,
   abstract = {ZnSn(OH)6 hierarchical cubes and Zn2SnO4 octahedra have been synthesized through a rapid, template-free, one-pot hydrothermal approach using zinc acetate, tin chloride and sodium hydroxide. ZnSn(OH)6 aggregates with cubic morphology and uniform size distribution have been successfully synthesized via aggregation-mediated crystallization. Through adjusting the hydrothermal parameters, Zn 2SnO4 octahedra were obtained at a higher temperature. The formation of Zn2SnO4 octahedra undergone a transformation from ZnSn(OH) 6 cubes. The as-synthesized products were characterized by powder X-ray diffraction (XRD), scanning electron microscopy (SEM) and differential scanning calorimetric analysis (DSC) and thermogravimetric analysis (TG). © (2011) Trans Tech Publications.},
   author = {Eda Maria Góes Maria Encarnação Beltrão Sposito},
   isbn = {978-85-232-0700-7},
   issn = {03601315},
   journal = {A psicanalise dos contos de fadas. Tradução Arlene Caetano},
   keywords = {Cidades,Ciências Sociais,Geografia Urbana,História Social},
   pages = {466},
   pmid = {470195},
   title = {Scanned by CamScanner ﯼﺭﺍﺰﻤﮐ},
   year = {2013}
}
@article{Package2020,
   author = {Type Package},
   doi = {10.18637/jss.v093.i05>.URL},
   title = {Package ‘ MFPCA ’},
   volume = {3},
   year = {2020}
}
@article{Chiou2014,
   abstract = {Missing values and outliers are frequently encountered in traffic monitoring data. We approach these problems by sampling the daily traffic flow rate trajectories from random functions and taking advantage of the data features using functional data analysis. We propose to impute missing values by using the conditional expectation approach to functional principal component analysis (FPCA). Our simulation study shows that the FPCA approach performs better than two commonly discussed methods in the literature, the probabilistic principal component analysis (PCA) and the Bayesian PCA, which have been shown to perform better than many conventional approaches. Based on the FPCA approach, the functional principal component scores can be applied to the functional bagplot and functional highest density region boxplot, which makes outlier detection possible for incomplete functional data. Our numerical results indicate that these two outlier detection approaches coupled with the proposed missing value imputation method can perform reasonably well. Although motivated by traffic flow data application, the proposed functional data methods for missing value imputation and outlier detection can be used in many applications with longitudinally recorded functional data.},
   author = {Jeng Min Chiou and Yi Chen Zhang and Wan Hui Chen and Chiung Wen Chang},
   doi = {10.1080/21680566.2014.892847},
   issn = {21680582},
   issue = {2},
   journal = {Transportmetrica B},
   keywords = {functional data,functional principal component analysis,intelligent transportation system,traffic flow rate,vehicle detector},
   pages = {106-129},
   title = {A functional data approach to missing value imputation and outlier detection for traffic flow data},
   volume = {2},
   url = {https://doi.org/10.1080/21680566.2014.892847},
   year = {2014}
}
@article{Junger2015,
   abstract = {Missing data are major concerns in epidemiological studies of the health effects of environmental air pollutants. This article presents an imputation-based method that is suitable for multivariate time series data, which uses the EM algorithm under the assumption of normal distribution. Different approaches are considered for filtering the temporal component. A simulation study was performed to assess validity and performance of proposed method in comparison with some frequently used methods. Simulations showed that when the amount of missing data was as low as 5%, the complete data analysis yielded satisfactory results regardless of the generating mechanism of the missing data, whereas the validity began to degenerate when the proportion of missing values exceeded 10%. The proposed imputation method exhibited good accuracy and precision in different settings with respect to the patterns of missing observations. Most of the imputations obtained valid results, even under missing not at random. The methods proposed in this study are implemented as a package called mtsdi for the statistical software system R.},
   author = {W. L. Junger and A. Ponce de Leon},
   doi = {10.1016/j.atmosenv.2014.11.049},
   issn = {18732844},
   journal = {Atmospheric Environment},
   keywords = {Air pollution,Data imputation,EM algorithm,Environmental epidemiology,Missing data,Particulate matter,Time series},
   pages = {96-104},
   publisher = {Elsevier Ltd},
   title = {Imputation of missing data in time series for air pollutants},
   volume = {102},
   url = {http://dx.doi.org/10.1016/j.atmosenv.2014.11.049},
   year = {2015}
}
@techReport{WorldHealthOrganization2006,
   author = {World Health Organization},
   doi = {10.1007/s12011-019-01864-7},
   issn = {15590720},
   pmid = {31407216},
   title = {WHO Air quality guidelines for particulate matter, ozone, nitrogen dioxide and sulfur dioxide - Global Update 2005},
   year = {2006}
}
@misc{SIATA2021,
   author = {SIATA},
   title = {Información de calidad del aire},
   url = {https://siata.gov.co/descarga_siata/index.php/index2/calidad_aire/},
   year = {2021}
}
@techReport{Calidad2012,
   author = {SIATA},
   city = {Medellín},
   institution = {Área Metropolitana del Valle de Aburrá},
   pages = {1-3},
   title = {Generalidades de la información Red de Calidad del Aire del Valle de Aburrá},
   url = {https://siata.gov.co/descarga_siata/index.php/info/aire/},
   year = {2019}
}
@article{Dai2020,
   abstract = {Functional data analysis can be seriously impaired by abnormal observations, which can be classified as either magnitude or shape outliers based on their way of deviating from the bulk of data. Identifying magnitude outliers is relatively easy, while detecting shape outliers is much more challenging. We propose turning the shape outliers into magnitude outliers through data transformation and detecting them using the functional boxplot. Besides easing the detection procedure, applying several transformations sequentially provides a reasonable taxonomy for the flagged outliers. A joint functional ranking, which consists of several transformations, is also defined here. Simulation studies are carried out to evaluate the performance of the proposed method using different functional depth notions. Interesting results are obtained in several practical applications.},
   author = {Wenlin Dai and Tomáš Mrkvička and Ying Sun and Marc G. Genton},
   issn = {23318422},
   issue = {11901573},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Data transformation,Functional boxplot,Magnitude outliers,Multivariate functional data,Shape outliers},
   title = {Functional outlier detection and taxonomy by sequential transformations},
   volume = {149},
   year = {2020}
}
@article{Cuesta-Albertos2017,
   abstract = {The maximum depth classifier was the first attempt to use data depths instead of multivariate raw data in classification problems. Recently, the DD-classifier has addressed some of the serious limitations of this classifier but issues still remain. This paper aims to extend the DD-classifier as follows: first, by enabling it to handle more than two groups; second, by applying regular classification methods (such as kNN, linear or quadratic classifiers, recursive partitioning, etc) to DD-plots, which is particularly useful, because it gives insights based on the diagnostics of these methods; and third, by integrating various sources of information (data depths, multivariate functional data, etc) in the classification procedure in a unified way. This paper also proposes an enhanced revision of several functional data depths and it provides a simulation study and applications to some real data sets.},
   author = {J. A. Cuesta-Albertos and M. Febrero-Bande and M. Oviedo de la Fuente},
   doi = {10.1007/s11749-016-0502-6},
   issn = {11330686},
   issue = {1},
   journal = {Test},
   keywords = {DD-classifier,Functional data analysis,Functional depths},
   pages = {119-142},
   title = {The DD G -classifier in the functional setting},
   volume = {26},
   year = {2017}
}
@misc{AbGhani2023,
   abstract = {Clustering high dimensional data is challenging as data dimensionality increases the distance between data points, resulting in sparse regions that degrade clustering performance. Subspace clustering is a common approach for processing high-dimensional data by finding relevant features for each cluster in the data space. Subspace clustering methods extend traditional clustering to account for the constraints imposed by data streams. Data streams are not only high-dimensional, but also unbounded and evolving. This necessitates the development of subspace clustering algorithms that can handle high dimensionality and adapt to the unique characteristics of data streams. Although many articles have contributed to the literature review on data stream clustering, there is currently no specific review on subspace clustering algorithms in high-dimensional data streams. Therefore, this article aims to systematically review the existing literature on subspace clustering of data streams in high-dimensional streaming environments. The review follows a systematic methodological approach and includes 18 articles for the final analysis. The analysis focused on two research questions related to the general clustering process and dealing with the unbounded and evolving characteristics of data streams. The main findings relate to six elements: clustering process, cluster search, subspace search, synopsis structure, cluster maintenance, and evaluation measures. Most algorithms use a two-phase clustering approach consisting of an initialization stage, a refinement stage, a cluster maintenance stage, and a final clustering stage. The density-based top-down subspace clustering approach is more widely used than the others because it is able to distinguish true clusters and outliers using projected micro-clusters. Most algorithms implicitly adapt to the evolving nature of the data stream by using a time fading function that is sensitive to outliers. Future work can focus on the clustering framework, parameter optimization, subspace search techniques, memory-efficient synopsis structures, explicit cluster change detection, and intrinsic performance metrics. This article can serve as a guide for researchers interested in high-dimensional subspace clustering methods for data streams.},
   author = {Nur Laila Ab Ghani and Izzatdin Abdul Aziz and Said Jadid AbdulKadir},
   doi = {10.32604/cmc.2023.035987},
   issn = {15462226},
   issue = {2},
   journal = {Computers, Materials and Continua},
   keywords = {Clustering,concept drift,data stream,evolving data stream,high dimensionality,projected clustering,stream clustering,subspace clustering},
   pages = {4649-4668},
   publisher = {Tech Science Press},
   title = {Subspace Clustering in High-Dimensional Data Streams: A Systematic Literature Review},
   volume = {75},
   year = {2023}
}
@techReport{Xu2018a,
   abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
   author = {Xiaodan Xu and Huawen Liu and Li Li and Minghai Yao},
   keywords = {data mining,evaluation measurement,high-dimensional data,outlier detection},
   title = {A Comparison of Outlier Detection Techniques for High-Dimensional Data},
   year = {2018}
}
@article{Li2024a,
   abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
   author = {Jia Li and Jiangwei Li and Chenxu Wang and Fons J. Verbeek and Tanja Schultz and Hui Liu},
   doi = {10.1088/2632-2153/ad2492},
   issn = {26322153},
   issue = {1},
   journal = {Machine Learning: Science and Technology},
   keywords = {clustering,data mining,machine learning,medical data,medoid selection,minimum spanning tree,outlier detection},
   month = {3},
   publisher = {Institute of Physics},
   title = {MS2OD: outlier detection using minimum spanning tree and medoid selection},
   volume = {5},
   year = {2024}
}
@article{Aleman-Gomez2022,
   abstract = {Functional magnetic resonance imaging (fMRI) is a non-invasive technique that facilitates the study of brain activity by measuring changes in blood flow. Brain activity signals can be recorded during the alternate performance of given tasks, that is, task fMRI (tfMRI), or during resting-state, that is, resting-state fMRI (rsfMRI), as a measure of baseline brain activity. This contributes to the understanding of how the human brain is organized in functionally distinct subdivisions. fMRI experiments from high-resolution scans provide hundred of thousands of longitudinal signals for each individual, corresponding to brain activity measurements over each voxel of the brain along the duration of the experiment. In this context, we propose novel visualization techniques for high-dimensional functional data relying on depth-based notions that enable computationally efficient 2-dim representations of fMRI data, which elucidate sample composition, outlier presence, and individual variability. We believe that this previous step is crucial to any inferential approach willing to identify neuroscientific patterns across individuals, tasks, and brain regions. We present the proposed technique via an extensive simulation study, and demonstrate its application on a motor and language tfMRI experiment.},
   author = {Yasser Alemán-Gómez and Ana Arribas-Gil and Manuel Desco and Antonio Elías and Juan Romo},
   doi = {10.1002/sim.9342},
   issn = {10970258},
   issue = {11},
   journal = {Statistics in Medicine},
   keywords = {FMRI,data visualization,dimensionality reduction,functional depth,multidimensional outliers},
   month = {5},
   pages = {2005-2024},
   pmid = {35118686},
   publisher = {John Wiley and Sons Ltd},
   title = {Depthgram: Visualizing outliers in high-dimensional functional data with application to fMRI data exploration},
   volume = {41},
   year = {2022}
}
@techReport{Dua,
   abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
   author = {Xuefeng Du and Yiyou Sun and Xiaojin Zhu and Yixuan Li},
   title = {Dream the Impossible: Outlier Imagination with Diffusion Models},
   url = {https://github.com/deeplearning-wisc/dream-ood.}
}
@article{Dang2010,
   abstract = {In extending univariate outlier detection methods to higher dimension, various issues arise: limited visualization methods, inadequacy of marginal methods, lack of a natural order, limited parametric modeling, and, when using Mahalanobis distance, restriction to ellipsoidal contours. To address and overcome such limitations, we introduce nonparametric multivariate outlier identifiers based on multivariate depth functions, which can generate contours following the shape of the data set. Also, we study masking robustness, that is, robustness against misidentification of outliers as nonoutliers. In particular, we define a masking breakdown point (MBP), adapting to our setting certain ideas of Davies and Gather [1993. The identification of multiple outliers (with discussion). Journal of the American Statistical Association 88, 782-801] and Becker and Gather [1999. The masking breakdown point of multivariate outlier identification rules. Journal of the American Statistical Association 94, 947-955] based on the Mahalanobis distance outlyingness. We then compare four affine invariant outlier detection procedures, based on Mahalanobis distance, halfspace or Tukey depth, projection depth, and "Mahalanobis spatial" depth. For the goal of threshold type outlier detection, it is found that the Mahalanobis distance and projection procedures are distinctly superior in performance, each with very high MBP, while the halfspace approach is quite inferior. When a moderate MBP suffices, the Mahalanobis spatial procedure is competitive in view of its contours not constrained to be elliptical and its computational burden relatively mild. A small sampling experiment yields findings completely in accordance with the theoretical comparisons. While these four depth procedures are relatively comparable for the purpose of robust affine equivariant location estimation, the halfspace depth is not competitive with the others for the quite different goal of robust setting of an outlyingness threshold. © 2009 Elsevier B.V. All rights reserved.},
   author = {Xin Dang and Robert Serfling},
   doi = {10.1016/j.jspi.2009.07.004},
   issn = {03783758},
   issue = {1},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Depth functions,Multivariate analysis,Nonparametric,Outlier identification,Robust},
   month = {1},
   pages = {198-213},
   title = {Nonparametric depth-based multivariate outlier identifiers, and masking robustness properties},
   volume = {140},
   year = {2010}
}
@techReport{Sikdera,
   author = {Nazmul Kabir Sikder and Feras A Batarseh},
   title = {Outlier Detection using AI: A Survey}
}
@techReport{Chandola2009,
   abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
   author = {Varun Chandola},
   journal = {ACM Computing Surveys},
   keywords = {Algorithms Additional Key Words and Phrases,Anomaly Detection,Categories and Subject Descriptors,Database Applications-Data Mining General Terms,H28 [Database Management],Outlier Detection},
   title = {Anomaly Detection : A Survey},
   year = {2009}
}
@article{Chen2025a,
   author = {Bo Chen and Feifei Chen and Junxin Wang and Tao Qiu},
   doi = {10.1016/j.csda.2024.108123},
   issn = {01679473},
   journal = {Computational Statistics \& Data Analysis},
   month = {6},
   pages = {108123},
   title = {An efficient and distribution-free symmetry test for high-dimensional data based on energy statistics and random projections},
   volume = {206},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S016794732400207X},
   year = {2025}
}
@article{Roldan-Alzate2022,
   abstract = {Identification of critical episodes of environmental pollution, both as a outlier identification problem and as a classification problem, is a usual application of multivariate functional data analysis. This article addresses the effects of robustifying multivariate functional samples on the identification of critical pollution episodes in Medellín, Colombia. To do so, it compares 18 depth-based outlier identification methods and highlights the best options in terms of precision through simulation. It then applies the two methods with the best performance to robustify a real dataset of air pollution (PM2.5 concentration) in the Metropolitan Area of Medellín, Colombia and compares the effects of robustifying the samples on the accuracy of supervised classification through the multivariate functional DD-classifier. Our results show that 10 out of 20 methods revised perform better in at least one kind outliers. Nevertheless, no clear positive effects of robustification were identified with the real dataset.},
   author = {Luis Miguel Roldán-Alzate and Francisco Zuluaga},
   doi = {10.1007/s10651-022-00544-5},
   issn = {15733009},
   issue = {4},
   journal = {Environmental and Ecological Statistics},
   keywords = {Air pollution,Multivariate functional outlier detection,Sequential transformations,α-trimming},
   month = {12},
   pages = {801-825},
   publisher = {Springer},
   title = {Assessing the effects of multivariate functional outlier identification and sample robustification on identifying critical PM2.5 air pollution episodes in Medellín, Colombia},
   volume = {29},
   year = {2022}
}
@techReport{Bickel,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger},
   title = {Springer Series in Statistics}
}
@article{Amovin-Assagba2022,
   abstract = {In an industrial context, the activity of sensors is recorded at a high frequency. A challenge is to automatically detect abnormal measurement behavior. Considering the sensor measures as functional data, the problem can be formulated as the detection of outliers in a multivariate functional data set. Due to the heterogeneity of this data set, the proposed contaminated mixture model both clusters the multivariate functional data into homogeneous groups and detects outliers. The main advantage of this procedure over its competitors is that it does not require to specify the proportion of outliers. Model inference is performed through an Expectation-Conditional Maximization algorithm, and the BIC is used to select the number of clusters. Numerical experiments on simulated data demonstrate the high performance achieved by the inference algorithm. In particular, the proposed model outperforms the competitors. Its application on the real data which motivated this study allows to correctly detect abnormal behaviors.},
   author = {Martial Amovin-Assagba and Irène Gannaz and Julien Jacques},
   doi = {10.1016/j.csda.2022.107496},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Contaminated Gaussian mixture,EM algorithm,Model-based clustering,Multivariate functional data,Outlier detection},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Outlier detection in multivariate functional data through a contaminated mixture model},
   volume = {174},
   year = {2022}
}
@article{Zheng2022a,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@article{Zhu2023a,
   abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
   author = {Pengyun Zhu and Chaowei Zhang and Xiaofeng Li and Jifu Zhang and Xiao Qin},
   doi = {10.1109/TKDE.2022.3172167},
   issn = {15582191},
   issue = {6},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {High-dimensional outlier detection,local outlier coulomb force,neighborhood outlier factor,outlier coulomb resultant force,similarity metric},
   month = {6},
   pages = {5506-5520},
   publisher = {IEEE Computer Society},
   title = {A High-Dimensional Outlier Detection Approach Based on Local Coulomb Force},
   volume = {35},
   year = {2023}
}
@article{Zhang2025,
   author = {Chi Zhang and Peijun Sang and Yingli Qin},
   doi = {10.1214/25-EJS2348},
   issn = {1935-7524},
   issue = {1},
   journal = {Electronic Journal of Statistics},
   month = {1},
   title = {Two-sample inference for sparse functional data},
   volume = {19},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-19/issue-1/Two-sample-inference-for-sparse-functional-data/10.1214/25-EJS2348.full},
   year = {2025}
}
@techReport{Bivand2015,
   abstract = {The integrated nested Laplace approximation (INLA) provides an interesting way of approximating the posterior marginals of a wide range of Bayesian hierarchical models. This approximation is based on conducting a Laplace approximation of certain functions and numerical integration is extensively used to integrate some of the models parameters out. The R-INLA package offers an interface to INLA, providing a suitable framework for data analysis. Although the INLA methodology can deal with a large number of models, only the most relevant have been implemented within R-INLA. However, many other important models are not available for R-INLA yet. In this paper we show how to fit a number of spatial models with R-INLA, including its interaction with other R packages for data analysis. Secondly, we describe a novel method to extend the number of latent models available for the model parameters. Our approach is based on conditioning on one or several model parameters and fit these conditioned models with R-INLA. Then these models are combined using Bayesian model averaging to provide the final approximations to the posterior marginals of the model. Finally, we show some examples of the application of this technique in spatial statistics. It is worth noting that our approach can be extended to a number of other fields, and not only spatial statistics.},
   author = {Roger S Bivand and Virgilio Gómez-Rubio and Håvard Rue},
   journal = {JSS Journal of Statistical Software},
   keywords = {INLA,R,spatial statistics},
   title = {Spatial Data Analysis with R-INLA with Some Extensions},
   volume = {63},
   url = {http://www.jstatsoft.org/},
   year = {2015}
}
@book{Lawson2021,
   author = {Andrew B Lawson},
   city = {Oxon},
   keywords = {Bayesian Hierarchical Models,Count Models,Disease Mapping Models,Infectious Disease Modeling,Nimble,Spatial Health Data,Survival Modeling},
   publisher = {CRC Press},
   title = {Using R for Bayesian Spatial and Spatio-Temporal Health Modeling},
   year = {2021}
}
@book{Blangiardo2015,
   abstract = {Spatial and Spatio-Temporal Bayesian Models with R-INLA provides a muchneeded, practically oriented & innovative presentation of the combination ofBayesian methodology and spatial statistics. The authors combine an introduction toBayesian theory and methodology with a focus on the spatial and spatio­-temporal modelsused within the Bayesian framework and a series of practical examples which allowthe reader to link the statistical theory presented to real data problems. The numerousexamples from the fields of epidemiology, biostatistics and social science all arecoded in the R package R-INLA, which has proven to be a valid alternative to the commonlyused Markov Chain Monte Carlo simulations. Title Page; Copyright; Table of Contents; Dedication; Preface; Chapter 1: Introduction; 1.1 Why spatial and spatio-temporal statistics?; 1.2 Why do we use Bayesian methods for modeling spatial and spatio-temporal structures?; 1.3 Why INLA?; 1.4 Datasets; References; Chapter 2: Introduction to R; 2.1 The R language; 2.2 R objects; 2.3 Data and session management; 2.4 Packages; 2.5 Programming in R; 2.6 Basic statistical analysis with R; References; Chapter 3: Introduction to Bayesian methods; 3.1 Bayesian philosophy; 3.2 Basic probability elements; 3.3 Bayes theorem.},
   author = {Marta. Blangiardo and Michela. Cameletti},
   isbn = {9781118326558},
   publisher = {John Wiley and Sons, Inc.},
   title = {Spatial and spatio-temporal Bayesian models with R-INLA},
   year = {2015}
}
@article{Post2018,
   abstract = {Dissolved oxygen is a critical component of river water quality. This study investigated average weekly dissolved oxygen (AWDO) and average weekly water temperature (AWT) in the Savannah River during 2015 and 2016 using data from the Intelligent River® sensor network. Weekly data and seasonal summary statistics revealed distinct seasonal patterns that impact both AWDO and AWT regardless of location along the river. Within seasons, spatial patterns of AWDO and AWT along the river are also evident. Linear mixed effects models indicate that AWT and low and high river flow conditions had a significant impact on AWDO, but added little predictive information to the models. Low and high river flow conditions had a significant impact on AWT, but also added little predictive information to the models. Spatial linear mixed effects models yielded parameter estimates that were effectively the same as non-spatial linear mixed effects models. However, components of variance from spatial linear mixed effects models indicate that 23–32% of the total variance in AWDO and that 12–18% of total variance in AWT can be apportioned to the effect of spatial covariance. These results indicate that location, week, and flow-directional spatial relationships are critically important considerations for investigating relationships between space- and time-varying water quality metrics.},
   author = {Christopher J. Post and Michael P. Cope and Patrick D. Gerard and Nicholas M. Masto and Joshua R. Vine and Roxanne Y. Stiglitz and Jason O. Hallstrom and Jillian C. Newman and Elena A. Mikhailova},
   doi = {10.1007/s10661-018-6646-y},
   issn = {15732959},
   issue = {5},
   journal = {Environmental Monitoring and Assessment},
   keywords = {Geographic information systems (GIS),Intelligent River®,Spatial stream networks,Water quality monitoring},
   month = {5},
   pmid = {29637320},
   publisher = {Springer International Publishing},
   title = {Monitoring spatial and temporal variation of dissolved oxygen and water temperature in the Savannah River using a sensor network},
   volume = {190},
   year = {2018}
}
@article{Chen2024,
   abstract = {Background: Clinical trials are increasingly using Bayesian methods for their design and analysis. Inference in Bayesian trials typically uses simulation-based approaches such as Markov Chain Monte Carlo methods. Markov Chain Monte Carlo has high computational cost and can be complex to implement. The Integrated Nested Laplace Approximations algorithm provides approximate Bayesian inference without the need for computationally complex simulations, making it more efficient than Markov Chain Monte Carlo. The practical properties of Integrated Nested Laplace Approximations compared to Markov Chain Monte Carlo have not been considered for clinical trials. Using data from a published clinical trial, we aim to investigate whether Integrated Nested Laplace Approximations is a feasible and accurate alternative to Markov Chain Monte Carlo and provide practical guidance for trialists interested in Bayesian trial design. Methods: Data from an international Bayesian multi-platform adaptive trial that compared therapeutic-dose anticoagulation with heparin to usual care in non-critically ill patients hospitalized for COVID-19 were used to fit Bayesian hierarchical generalized mixed models. Integrated Nested Laplace Approximations was compared to two Markov Chain Monte Carlo algorithms, implemented in the software JAGS and stan, using packages available in the statistical software R. Seven outcomes were analysed: organ-support free days (an ordinal outcome), five binary outcomes related to survival and length of hospital stay, and a time-to-event outcome. The posterior distributions for the treatment and sex effects and the variances for the hierarchical effects of age, site and time period were obtained. We summarized these posteriors by calculating the mean, standard deviations and the 95% equitailed credible intervals and presenting the results graphically. The computation time for each algorithm was recorded. Results: The average overlap of the 95% credible interval for the treatment and sex effects estimated using Integrated Nested Laplace Approximations was 96% and 97.6% compared with stan, respectively. The graphical posterior densities for these effects overlapped for all three algorithms. The posterior mean for the variance of the hierarchical effects of age, site and time estimated using Integrated Nested Laplace Approximations are within the 95% credible interval estimated using Markov Chain Monte Carlo but the average overlap of the credible interval is lower, 77%, 85.6% and 91.3%, respectively, for Integrated Nested Laplace Approximations compared to stan. Integrated Nested Laplace Approximations and stan were easily implemented in clear, well-established packages in R, while JAGS required the direct specification of the model. Integrated Nested Laplace Approximations was between 85 and 269 times faster than stan and 26 and 1852 times faster than JAGS. Conclusion: Integrated Nested Laplace Approximations could reduce the computational complexity of Bayesian analysis in clinical trials as it is easy to implement in R, substantially faster than Markov Chain Monte Carlo methods implemented in JAGS and stan, and provides near identical approximations to the posterior distributions for the treatment effect. Integrated Nested Laplace Approximations was less accurate when estimating the posterior distribution for the variance of hierarchical effects, particularly for the proportional odds model, and future work should determine if the Integrated Nested Laplace Approximations algorithm can be adjusted to improve this estimation.},
   author = {Ziming Chen and Jeffrey S. Berger and Lana A. Castellucci and Michael Farkouh and Ewan C. Goligher and Erinn M. Hade and Beverley J. Hunt and Lucy Z. Kornblith and Patrick R. Laweler and Eric S. Leifer and Elizabeth Lorenzi and Matthew D. Neal and Ryan Zarychanski and Anna Heath},
   doi = {10.1177/17407745241247334},
   issn = {17407753},
   journal = {Clinical Trials},
   keywords = {Bayesian clinical trial analysis,Integrated Nested Laplace Approximations,JAGS,Markov chain Monte Carlo,logistic regression,proportional odds model,stan,survival analysis},
   month = {12},
   pmid = {38752434},
   publisher = {SAGE Publications Ltd},
   title = {A comparison of computational algorithms for the Bayesian analysis of clinical trials},
   year = {2024}
}
@techReport{Jutras2023a,
   author = {Mathilde Jutras},
   title = {Physical and biogeochemical drivers of deoxygenation in the Gulf and Lower St. Lawrence Estuary},
   year = {2023}
}
@article{Contractor2021,
   abstract = {Ocean data timeseries are vital for a diverse range of stakeholders (ranging from government, to industry, to academia) to underpin research, support decision making, and identify environmental change. However, continuous monitoring and observation of ocean variables is difficult and expensive. Moreover, since oceans are vast, observations are typically sparse in spatial and temporal resolution. In addition, the hostile ocean environment creates challenges for collecting and maintaining data sets, such as instrument malfunctions and servicing, often resulting in temporal gaps of varying lengths. Neural networks (NN) have proven effective in many diverse big data applications, but few oceanographic applications have been tested using modern frameworks and architectures. Therefore, here we demonstrate a “proof of concept” neural network application using a popular “off-the-shelf” framework called “TensorFlow” to predict subsurface ocean variables including dissolved oxygen and nutrient (nitrate, phosphate, and silicate) concentrations, and temperature timeseries and show how these models can be used successfully for gap filling data products. We achieved a final prediction accuracy of over 96% for oxygen and temperature, and mean squared errors (MSE) of 2.63, 0.0099, and 0.78, for nitrates, phosphates, and silicates, respectively. The temperature gap-filling was done with an innovative contextual Long Short-Term Memory (LSTM) NN that uses data before and after the gap as separate feature variables. We also demonstrate the application of a novel dropout based approach to approximate the Bayesian uncertainty of these temperature predictions. This Bayesian uncertainty is represented in the form of 100 monte carlo dropout estimates of the two longest gaps in the temperature timeseries from a model with 25% dropout in the input and recurrent LSTM connections. Throughout the study, we present the NN training process including the tuning of the large number of NN hyperparameters which could pose as a barrier to uptake among researchers and other oceanographic data users. Our models can be scaled up and applied operationally to provide consistent, gap-free data to all data users, thus encouraging data uptake for data-based decision making.},
   author = {Steefan Contractor and Moninya Roughan},
   doi = {10.3389/fmars.2021.637759},
   issn = {22967745},
   journal = {Frontiers in Marine Science},
   keywords = {East Australian Current,coastal oceanography,depth profile observations,machine learning,nitrate,phosphate,silicate,statistical modeling},
   month = {5},
   publisher = {Frontiers Media S.A.},
   title = {Efficacy of Feedforward and LSTM Neural Networks at Predicting and Gap Filling Coastal Ocean Timeseries: Oxygen, Nutrients, and Temperature},
   volume = {8},
   year = {2021}
}
@article{Styles2024,
   abstract = {Ocean ventilation translates atmospheric forcing into the ocean interior. The Southern Ocean is an important ventilation site for heat and carbon and is likely to influence the outcome of anthropogenic climate change. We conduct an extensive backwards-in-time trajectory experiment to identify spatial and temporal patterns of ventilation. Temporally, almost all ventilation occurs between August and November. Spatially, “hotspots” of ventilation account for 60% of open-ocean ventilation on a 30 years timescale; the remaining 40% ventilates in a circumpolar pattern. The densest waters ventilate on the Antarctic shelf, primarily near the Antarctic Peninsula (40%) and the west Ross sea (20%); the remaining 40% is distributed across East Antarctica. Shelf-ventilated waters experience significant densification outside of the mixed layer.},
   author = {Andrew F. Styles and Graeme A. MacGilchrist and Michael J. Bell and David P. Marshall},
   doi = {10.1029/2023GL106716},
   issn = {19448007},
   issue = {4},
   journal = {Geophysical Research Letters},
   keywords = {Stommel's demon,hotspots,southern ocean,trajectories,ventilation},
   month = {2},
   publisher = {John Wiley and Sons Inc},
   title = {Spatial and Temporal Patterns of Southern Ocean Ventilation},
   volume = {51},
   year = {2024}
}
@article{Bourbonnais2023,
   abstract = {Nitrous oxide (N2O) is a potent greenhouse gas and ozone depleting substance, with the ocean accounting for about one third of global emissions. In marine environments, a significant amount of N2O is produced by biological processes in Oxygen Deficient Zones (ODZs). While recent technological advances are making surface N2O concentration more available, high temporal and spatial resolution water-column N2O concentration data are relatively scarce, limiting global N2O ocean models’ predictive capability. We present a N2O concentration, stable isotopic composition and isotopomer dataset of unprecedently large spatial coverage and depth resolution in the broader Pacific, crossing both the eastern tropical South and North Pacific Ocean ODZs collected as part of the GO-SHIP P18 repeat hydrography program in 2016/2017. We complement these data with dissolved gases (nitrogen, oxygen, argon) and nitrate isotope data to investigate the pathways controlling N2O production in relation to apparent oxygen utilization and fixed nitrogen loss. N2O yield significantly increased under low oxygen conditions near the ODZs. Keeling plot analysis revealed different N2O sources above the ODZs under different oxygen regimes. Our stable isotopic data and relationships between the N2O added by microbial processes (ΔN2O) and dissolved inorganic nitrogen (DIN) deficit confirm increased N2O production by denitrification under low oxygen conditions near the oxycline where the largest N2O accumulations were observed. The slope for δ18O-N2O versus site preference (SP, the difference between the central (α) and outer (β) N atoms in the linear N2O molecule) in the eastern tropical North Pacific ODZ was lower than expected for pure N2O reduction, likely because of the observed decrease in δ15Nβ. This trend is consistent with prior ODZ studies and attributed to concurrent production of N2O from nitrite with a low δ15N or denitrification with a SP >0‰. We estimated apparent isotope effects for N2O consumption in the ETNP ODZ of 3.6‰ for 15Nbulk, 9.4‰ for 15Nα, -2.3‰ for 15Nβ, 12.0‰ for 18O, and 11.7‰ for SP. These values were generally within ranges previously reported for previous laboratory and field experiments.},
   author = {Annie Bourbonnais and Bonnie X. Chang and Rolf E. Sonnerup and Scott C. Doney and Mark A. Altabet},
   doi = {10.3389/fmars.2023.1137064},
   issn = {22967745},
   journal = {Frontiers in Marine Science},
   keywords = {Southern Ocean,eastern North Pacific Ocean,eastern South Pacific Ocean,greenhouse gas,isotopomers,nitrous oxide,oxygen deficient zones,stable isotopes},
   publisher = {Frontiers Media S.A.},
   title = {Marine N2O cycling from high spatial resolution concentration, stable isotopic and isotopomer measurements along a meridional transect in the eastern Pacific Ocean},
   volume = {10},
   year = {2023}
}
@article{,
   title = {scotia_20180720_87_delayed_corrected_v4}
}
@article{Liu2018,
   abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter $\{k\}$ of $\{k\}$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
   author = {Huawen Liu and Xuelong Li and Jiuyong Li and Shichao Zhang},
   doi = {10.1109/TSMC.2017.2718220},
   issn = {21682232},
   issue = {12},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
   keywords = {Dimension reduction,high-dimensional data,k nearest neighbors (kNN),low-rank approximation,outlier detection},
   month = {12},
   pages = {2451-2461},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Efficient Outlier Detection for High-Dimensional Data},
   volume = {48},
   year = {2018}
}
@misc{Mirzaie2023a,
   abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
   author = {Mostafa Mirzaie and Behshid Behkamal and Mohammad Allahbakhsh and Samad Paydar and Elisa Bertino},
   doi = {10.1016/j.cosrev.2023.100554},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data quality,Data streams,Quality framework,Systematic literature review},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {State of the art on quality control for data streams: A systematic literature review},
   volume = {48},
   year = {2023}
}
@article{Pronello2023a,
   abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
   author = {Nicola Pronello and Rosaria Ignaccolo and Luigi Ippoliti and Sara Fontanella},
   doi = {10.1007/s11222-023-10288-2},
   issn = {15731375},
   issue = {6},
   journal = {Statistics and Computing},
   keywords = {Functional zoning,Manifold data,Mixture models,Shape analysis,Spatial clustering,Surface data},
   month = {12},
   publisher = {Springer},
   title = {Penalized model-based clustering of complex functional data},
   volume = {33},
   year = {2023}
}
@article{Porreca2024a,
   abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
   author = {Annamaria Porreca and Fabrizio Maturo},
   doi = {10.1007/s11135-024-01876-z},
   issn = {15737845},
   journal = {Quality and Quantity},
   keywords = {Biodiversity,Diversity,FDA,Functional outlier detection,Hill’s numbers,Normalized Hill’s functions,Standardized Hill’s functions},
   publisher = {Springer Science and Business Media B.V.},
   title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized Hill’s numbers and their integral functions},
   year = {2024}
}
@article{Todorov2024a,
   abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
   author = {Valentin Todorov},
   doi = {10.1002/wics.70007},
   issn = {19390068},
   issue = {6},
   journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
   keywords = {R,high dimensions,multivariate,outlier,robust},
   month = {11},
   publisher = {John Wiley and Sons Inc},
   title = {The R Package Ecosystem for Robust Statistics},
   volume = {16},
   year = {2024}
}
@article{Ruff2021a,
   abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
   author = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Gregoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus Robert Muller},
   doi = {10.1109/JPROC.2021.3052449},
   issn = {15582256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   keywords = {Anomaly detection (AD),deep learning,explainable artificial intelligence,interpretability,kernel methods,neural networks,novelty detection,one-class classification,out-of-distribution (OOD) detection,outlier detection,unsupervised learning.},
   month = {5},
   pages = {756-795},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Unifying Review of Deep and Shallow Anomaly Detection},
   volume = {109},
   year = {2021}
}
@article{,
   title = {Untitled}
}
@article{,
   title = {case_study_ssc}
}
@misc{Souiden2022a,
   abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
   author = {Imen Souiden and Mohamed Nazih Omri and Zaki Brahmi},
   doi = {10.1016/j.cosrev.2022.100463},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data streams,High dimensional data,Outlier detection},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {A survey of outlier detection in high dimensional data streams},
   volume = {44},
   year = {2022}
}
@article{Lavine1999,
   abstract = {The National Oceanic Data Center (NODC) contains historical records from approximately 144,000 hydrographic stations in the North Atlantic. This data has been used by oceanographers to construct maps of point estimates of pressure, temperature, salinity and oxygen in the North Atlantic (Levitus (1994); Lozier et al. (1995)). Because data from any particular year are scarce, the previous maps have been for time-averaged values only. In addition, the maps have been reported without uncertainty estimates. This paper presents a Markov random ®eld (MRF) analysis that can generate maps for speci®c time periods along with associated uncertainties. To estimate changes in oceanic properties over time previous oceanographic work has focused on differences between a few time periods each having many observations. Due to data scarcity this poses a severe restriction for both spatial and temporal coverage of climatic change. The MRF analysis provides a means for temporal modeling that does not require high data density at each time period. To demonstrate the usefulness of a MRF analysis of oceanic data we investigate the temporal variability along 24.5 N in the North Atlantic. Our results are compared to an earlier analysis (Parrilla et al. (1994)) where data from only three time periods was used. We obtain a more thorough understanding of the temperature change found by this previous study.},
   author = {Michael Lavine and Susan Lozier},
   issue = {6},
   journal = {Environmental and Ecological Statistics},
   keywords = {Academic,Bayesian analysis 1352-8505 # 1999,Kluwer,Publishers},
   title = {A Markov random field spatio-temporal analysis of ocean temperature},
   year = {1999}
}
@techReport{Ferreira,
   abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
   author = {Marco AR Ferreira},
   keywords = {Bayesian,Markov random fields,areal data,dynamic linear models,ensembles of models,hierarchical models,model selection paradox,multiscale models,objective Bayes,spatio-temporal modeling},
   title = {Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models}
}
@article{Skalvik2023,
   abstract = {In this paper we give an overview of factors and limitations impairing deep-sea sensor data, and we show how automatic tests can give sensors self-validation and self-diagnostic capabilities. This work is intended to lay a basis for sophisticated use of smart sensors in long-term autonomous operation in remote deep-sea locations. Deep-sea observation relies on data from sensors operating in remote, harsh environments which may affect sensor output if uncorrected. In addition to the environmental impact, sensors are subject to limitations regarding power, communication, and limitations on recalibration. To obtain long-term measurements of larger deep-sea areas, fixed platform sensors on the ocean floor may be deployed for several years. As for any observation systems, data collected by deep-sea observation equipment are of limited use if the quality or accuracy (closeness of agreement between the measurement and the true value) is not known. If data from a faulty sensor are used directly, this may result in an erroneous understanding of deep water conditions, or important changes or conditions may not be detected. Faulty sensor data may significantly weaken the overall quality of the combined data from several sensors or any derived model. This is particularly an issue for wireless sensor networks covering large areas, where the overall measurement performance of the network is highly dependent on the data quality from individual sensors. Existing quality control manuals and initiatives for best practice typically recommend a selection of (near) real-time automated checks. These are mostly limited to basic and straight forward verification of metadata and data format, and data value or transition checks against pre-defined thresholds. Delayed-mode inspection is often recommended before a final data quality stamp is assigned.},
   author = {Astrid Marie Skålvik and Camilla Saetre and Kjell Eivind Frøysa and Ranveig N. Bjørk and Anders Tengberg},
   doi = {10.3389/fmars.2023.1152236},
   issn = {22967745},
   journal = {Frontiers in Marine Science},
   keywords = {calibration,data quality,in-situ,self-validation,sensor},
   publisher = {Frontiers Media S.A.},
   title = {Challenges, limitations, and measurement strategies to ensure data quality in deep-sea sensors},
   volume = {10},
   year = {2023}
}
@article{Jutras2023,
   abstract = {Persistent hypoxic bottom waters have developed in the Lower St Lawrence Estuary (LSLE) and have impacted fish and benthic species distributions. Minimum dissolved oxygen concentrations decreased from ∼ 125 μmol L-1 (38 % saturation) in the 1930s to ∼ 65 μmol L-1 (21 % saturation) in 1984. Minimum dissolved oxygen concentrations remained at hypoxic levels (< 62.5 μM = 2 mg L-1 or 20 % saturation) between 1984 and 2019, but in 2020, they suddenly decreased to ∼ 35 μmol L-1. Concurrently, bottom-water temperatures in the LSLE have increased progressively from ∼ 3 °C in the 1930s to nearly 7 °C in 2021. The main driver of deoxygenation and warming in the bottom waters of the Gulf of St Lawrence and St Lawrence Estuary is a change in the circulation pattern in the western North Atlantic, more specifically a decrease in the relative contribution of younger, well-oxygenated and cold Labrador Current Waters to the waters of the Laurentian Channel, a deep valley that extends from the continental shelf edge, through Cabot Strait, the gulf and to the head of the LSLE. Hence, the warmer, oxygen-depleted North Atlantic Central Waters carried by the Gulf Stream now make up nearly 100 % of the waters entering the Laurentian Channel. The areal extent of the hypoxic zone in the LSLE has varied since 1993 when it was first estimated at 1300 km2. In 2021, it reached 9400 km2, extending well into the western Gulf of St Lawrence. Severely hypoxic waters are now also found at the end of the two deep channels that branch out from the Laurentian Channel, namely, the Esquiman Channel and Anticosti Channel. Copyright:},
   author = {Mathilde Jutras and Alfonso Mucci and Gwenaëlle Chaillou and William A. Nesbitt and Douglas W.R. Wallace},
   doi = {10.5194/bg-20-839-2023},
   issn = {17264189},
   issue = {4},
   journal = {Biogeosciences},
   month = {2},
   pages = {839-849},
   publisher = {Copernicus Publications},
   title = {Temporal and spatial evolution of bottom-water hypoxia in the St Lawrence estuarine system},
   volume = {20},
   year = {2023}
}
@article{Hull2021,
   abstract = {The continental shelf seas are important at a global scale for ecosystem services. These highly dynamic regions are under a wide range of stresses, and as such future management requires appropriate monitoring measures. A key metric to understanding and predicting future change are the rates of biological production. We present here the use of an autonomous underwater glider with an oxygen (O2) and a wet-chemical microfluidic total oxidised nitrogen (NOx-NO3-+NO2-) sensor during a spring bloom as part of a 2019 pilot autonomous shelf sea monitoring study. We find exceptionally high rates of net community production using both O2 and NOx-water column inventory changes, corrected for air-sea gas exchange in case of O2. We compare these rates with 2007 and 2008 mooring observations finding similar rates of NOx-consumption. With these complementary methods we determine the O2:N amount ratio of the newly produced organic matter (7.8g±0.4) and the overall O2:N ratio for the total water column (5.7g±0.4). The former is close to the canonical Redfield O2:N ratio of 8.6g±1.0, whereas the latter may be explained by a combination of new organic matter production and preferential remineralisation of more reduced organic matter at a higher O2:N ratio below the euphotic zone.},
   author = {Tom Hull and Naomi Greenwood and Antony Birchill and Alexander Beaton and Matthew Palmer and Jan Kaiser},
   doi = {10.5194/bg-18-6167-2021},
   issn = {17264189},
   issue = {23},
   journal = {Biogeosciences},
   month = {12},
   pages = {6167-6180},
   publisher = {Copernicus GmbH},
   title = {Simultaneous assessment of oxygen-and nitrate-based net community production in a temperate shelf sea from a single ocean glider},
   volume = {18},
   year = {2021}
}
@article{Chakraborty2015a,
   abstract = {This paper examines the spatio-temporal dynamics of a marine ecosystem. The system is described by two reaction–diffusion equations. We consider a phytoplankton–zooplankton system with Ivlev-type grazing function. The dynamics of the reaction–diffusion system of phytoplankton–zooplankton interaction has been studied with both constant and variable diffusion coefficients. Periodic oscillations of the phytoplankton and zooplankton populations are shown with constant and variable diffusion coefficients. In order to obtain spatio-temporal patterns, we perform numerical simulations of the coupled system describing phytoplankton–zooplankton dynamics in the presence of diffusive forces. We explain how the concentration of species changes due to local reactions and diffusion. Our results suggest that patchiness is one of the basic characteristics of the functioning of an ecological system. Two-dimensional spatial patterns of phytoplankton–zooplankton dynamics are self-organized and, therefore, can be considered to provide a theoretical framework to understand patchiness in marine environments.},
   author = {Kunal Chakraborty and Vamsi Manthena},
   doi = {10.1007/s11071-015-2114-1},
   issn = {1573269X},
   issue = {4},
   journal = {Nonlinear Dynamics},
   keywords = {Diffusion-driven instability,Marine ecosystem,Patchiness,Reaction–diffusion equations,Spatio-temporal dynamics},
   month = {9},
   pages = {1895-1906},
   publisher = {Kluwer Academic Publishers},
   title = {Modelling and analysis of spatio-temporal dynamics of a marine ecosystem},
   volume = {81},
   year = {2015}
}
@article{Saba2019,
   abstract = {Coastal and ocean acidification can alter ocean biogeochemistry, with ecological consequences that may result in economic and cultural losses. Yet few time series and high resolution spatial and temporal measurements exist to track the existence and movement of water low in pH and/or carbonate saturation. Past acidification monitoring efforts have either low spatial resolution (mooring) or high cost and low temporal and spatial resolution (research cruises). We developed the first integrated glider platform and sensor system for sampling pH throughout the water column of the coastal ocean. A deep ISFET (Ion Sensitive Field Effect Transistor)-based pH sensor system was modified and integrated into a Slocum glider, tank tested in natural seawater to determine sensor conditioning time under different scenarios, and validated in situ during deployments in the U.S. Northeast Shelf (NES). Comparative results between glider pH and pH measured spectrophotometrically from discrete seawater samples indicate that the glider pH sensor is capable of accuracy of 0.011 pH units or better for several weeks throughout the water column in the coastal ocean, with a precision of 0.005 pH units or better. Furthermore, simultaneous measurements from multiple sensors on the same glider enabled salinity-based estimates of total alkalinity (AT) and aragonite saturation state (ΩArag). During the Spring 2018 Mid-Atlantic deployment, glider pH and derived AT/ΩArag data along the cross-shelf transect revealed higher pH and ΩArag associated with the depth of chlorophyll and oxygen maxima and a warmer, saltier water mass. Lowest pH and ΩArag occurred in bottom waters of the middle shelf and slope, and nearshore following a period of heavy precipitation. Biofouling was revealed to be the primary limitation of this sensor during a summer deployment, whereby offsets in pH and AT increased dramatically. Advances in anti-fouling coatings and the ability to routinely clean and swap out sensors can address this challenge. The data presented here demonstrate the ability for gliders to routinely provide high resolution water column data on regional scales that can be applied to acidification monitoring efforts in other coastal regions.},
   author = {Grace K. Saba and Elizabeth Wright-Fairbanks and Baoshan Chen and Wei Jun Cai and Andrew H. Barnard and Clayton P. Jones and Charles W. Branham and Kui Wang and Travis Miles},
   doi = {10.3389/fmars.2019.00664},
   issn = {22967745},
   journal = {Frontiers in Marine Science},
   keywords = {Mid-Atlantic,U.S. Northeast Shelf,glider,monitoring,ocean acidification,pH},
   month = {10},
   publisher = {Frontiers Media S.A.},
   title = {The Development and Validation of a Profiling Glider Deep ISFET-Based pH Sensor for High Resolution Observations of Coastal and Ocean Acidification},
   volume = {6},
   year = {2019}
}
@article{Burke2023,
   abstract = {Dissolved oxygen (DO) in the global ocean is on the decline, resulting in the degradation of coastal habitats. As aquaculture production occurs in these regions, proper understanding of coastal DO dynamics is important for improved farm management (e.g. site selection). The main objective of this study was to quantify along-shore and cross-shore variability in DO dynamics, as well as onshore advection of offshore waters to the bays that could contain aquaculture farms. For that purpose, a Slocum underwater glider was deployed between September 25 and October 12, 2020 to collect high-resolution data of temperature, salinity and DO along a transect between Shelburne Bay and St. Margarets Bay, Nova Scotia, Canada, with an average distance of about 15 km from shore. The observations revealed the variable nature of cross-shore water properties. Shoreward bottom currents transport offshore waters to the coast, and combined with longer residence times, mixing of these waters with those present created a mosaic of differing water properties, with warmer, fresher, and less oxygenated shoreward waters. The glider intercepted an upwelling event due to strong and persistent southwesterly winds, which cooled the upper water layers by 6 °C and increased DO by 1.4 mg L−1. This strong upwelling event detected at 10 km from the coast was also captured 30 h later within St. Margarets Bay, depicting a potential offshore-inshore interaction. Therefore, bay-wide ecosystems and aquaculture production could be affected by intrusions of offshore waters.},
   author = {Meredith Burke and Jon Grant and Ramon Filgueira and Jinyu Sheng},
   doi = {10.1016/j.csr.2022.104908},
   issn = {18736955},
   journal = {Continental Shelf Research},
   keywords = {Along-/cross-shelf variability,Aquaculture,Dissolved oxygen,Scotian shelf,Slocum glider,Temperature and salinity},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Temporal and spatial variability in hydrography and dissolved oxygen along southwest Nova Scotia using glider observations},
   volume = {254},
   year = {2023}
}
@article{Wikle2025,
   abstract = {Deep neural network models have become ubiquitous in recent years and have been applied to nearly all areas of science, engineering, and industry. These models are particularly useful for data that have strong dependencies in space (e.g., images) and time (e.g., sequences). Indeed, deep models have also been extensively used by the statistical community to model spatial and spatiotemporal data through, for example, the use of multilevel Bayesian hierarchical models and deep Gaussian processes. In this review, we first present an overview of traditional statistical and machine learning perspectives for modeling spatial and spatiotemporal data, and then focus on a variety of hybrid models that have recently been developed for latent process, data, and parameter specifications. These hybrid models integrate statistical modeling ideas with deep neural network models in order to take advantage of the strengths of each modeling paradigm. We conclude by giving an overview of computational technologies that have proven useful for these hybrid models, and with a brief discussion on future research directions.},
   author = {Christopher K Wikle and Andrew Zammit-Mangion},
   doi = {10.1146/annurev-statistics-033021},
   keywords = {Bayesian hierarchical models,convolutional neural networks,deep Gaussian processes,recurrent neural networks,reinforcement learning,warping},
   pages = {5},
   title = {Annual Review of Statistics and Its Application Statistical Deep Learning for Spatial and Spatiotemporal Data},
   volume = {14},
   url = {https://doi.org/10.1146/annurev-statistics-033021-},
   year = {2025}
}
@article{Jutras2020,
   abstract = {Oxygen concentrations in the deep waters of the Lower St. Lawrence Estuary have decreased by 50% over the past century. The drivers of this decrease are investigated by applying an extended Optimum Multiparameter analysis to a time series of physical and biogeochemical observations of the St. Lawrence Estuarine System in the 1970s and from late 1990s to 2018. This method reconstructs the relative contributions of the two major water masses feeding the system, the Labrador Current Waters (LCW) and the North Atlantic Central Waters (NACW), as well as oxygen utilization, and accounts for diapycnal mixing. The causes of the oxygen decline varied over the last 5 decades. Between the 1970s and late 1990s, the decrease was mainly driven by biogeochemical changes through an increase in microbial oxygen utilization in the St. Lawrence Estuary in response to warmer temperatures and eutrophication and lower oxygen concentrations in LCW and NACW. Between 2008 and 2018, the decrease was mainly driven by circulation changes in the western North Atlantic associated with a reduced inflow of high-oxygenated LCW to the deep waters of the system in favor of low-oxygenated NACW, reaching a historical minimum in 2016. The LCW:NACW ratio is strongly correlated with the volume transport of the Scotian shelf-break current, an extension of the Labrador Current. These results highlight the primary role of the Labrador Current in determining the oxygen concentration and other water properties of the St. Lawrence Estuarine System and on the western North Atlantic continental shelf and slope.},
   author = {M. Jutras and C. O. Dufour and A. Mucci and F. Cyr and D. Gilbert},
   doi = {10.1029/2020JC016577},
   issn = {21699291},
   issue = {12},
   journal = {Journal of Geophysical Research: Oceans},
   keywords = {Labrador Current,St. Lawrence Estuary,deoxygenation,eutrophication,oxygen,slope waters},
   month = {12},
   publisher = {Blackwell Publishing Ltd},
   title = {Temporal Changes in the Causes of the Observed Oxygen Decline in the St. Lawrence Estuary},
   volume = {125},
   year = {2020}
}
@techReport{Rue2009,
   abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (gener-alized) additive models, smoothing spline models, state space models, semiparametric regression , spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of struc-tured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality , which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
   author = {Håvard Rue and Sara Martino and Nicolas Chopin},
   issue = {2},
   journal = {J. R. Statist. Soc. B},
   keywords = {Approximate Bayesian inference,Gaussian Markov random fields,Generalized additive mixed models,Laplace approximation,Parallel computing,Sparse matrices,Structured additive regression models},
   pages = {319-392},
   title = {Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations},
   volume = {71},
   year = {2009}
}
@article{Cameletti2013,
   abstract = {In this work, we consider a hierarchical spatio-temporal model for particulate matter (PM) concentration in the North-Italian region Piemonte. The model involves a Gaussian Field (GF), affected by a measurement error, and a state process characterized by a first order autoregressive dynamic model and spatially correlated innovations. This kind of model is well discussed and widely used in the air quality literature thanks to its flexibility in modelling the effect of relevant covariates (i. e. meteorological and geographical variables) as well as time and space dependence. However, Bayesian inference-through Markov chain Monte Carlo (MCMC) techniques-can be a challenge due to convergence problems and heavy computational loads. In particular, the computational issue refers to the infeasibility of linear algebra operations involving the big dense covariance matrices which occur when large spatio-temporal datasets are present. The main goal of this work is to present an effective estimating and spatial prediction strategy for the considered spatio-temporal model. This proposal consists in representing a GF with Matérn covariance function as a Gaussian Markov Random Field (GMRF) through the Stochastic Partial Differential Equations (SPDE) approach. The main advantage of moving from a GF to a GMRF stems from the good computational properties that the latter enjoys. In fact, GMRFs are defined by sparse matrices that allow for computationally effective numerical methods. Moreover, when dealing with Bayesian inference for GMRFs, it is possible to adopt the Integrated Nested Laplace Approximation (INLA) algorithm as an alternative to MCMC methods giving rise to additional computational advantages. The implementation of the SPDE approach through the R-library INLA (www.r-inla.org) is illustrated with reference to the Piemonte PM data. In particular, providing the step-by-step R-code, we show how it is easy to get prediction and probability of exceedance maps in a reasonable computing time. © 2012 Springer-Verlag.},
   author = {Michela Cameletti and Finn Lindgren and Daniel Simpson and Håvard Rue},
   doi = {10.1007/s10182-012-0196-3},
   issn = {18638171},
   issue = {2},
   journal = {AStA Advances in Statistical Analysis},
   keywords = {Covariance functions,Gaussian Markov random fields,Gaussian fields,Hierarchical models,Integrated Nested Laplace Approximation},
   month = {4},
   pages = {109-131},
   publisher = {Springer Verlag},
   title = {Spatio-temporal modeling of particulate matter concentration through the SPDE approach},
   volume = {97},
   year = {2013}
}
@techReport{Lindgren2011,
   abstract = {Continuously indexed Gaussian fields (GFs) are the most important ingredient in spatial statistical modelling and geostatistics. The specification through the covariance function gives an intuitive interpretation of the field properties. On the computational side, GFs are hampered with the big n problem, since the cost of factorizing dense matrices is cubic in the dimension. Although computational power today is at an all time high, this fact seems still to be a computational bottleneck in many applications. Along with GFs, there is the class of Gauss-ian Markov random fields (GMRFs) which are discretely indexed. The Markov property makes the precision matrix involved sparse, which enables the use of numerical algorithms for sparse matrices, that for fields in R 2 only use the square root of the time required by general algorithms. The specification of a GMRF is through its full conditional distributions but its marginal properties are not transparent in such a parameterization. We show that, using an approximate stochastic weak solution to (linear) stochastic partial differential equations, we can, for some GFs in the Matérn class, provide an explicit link , for any triangulation of R d , between GFs and GMRFs, formulated as a basis function representation. The consequence is that we can take the best from the two worlds and do the modelling by using GFs but do the computations by using GMRFs. Perhaps more importantly, our approach generalizes to other covariance functions generated by SPDEs, including oscillating and non-stationary GFs, as well as GFs on manifolds. We illustrate our approach by analysing global temperature data with a non-stationary model defined on a sphere.},
   author = {Finn Lindgren and Håvard Rue and Johan Lindström},
   journal = {J. R. Statist. Soc. B},
   keywords = {Approximate Bayesian inference,Covariance functions,Gaussian Markov random fields,Gaussian fields,Latent Gaussian models,Sparse matrices,Stochastic partial differential equations},
   pages = {423-498},
   title = {An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach},
   volume = {73},
   year = {2011}
}
@article{Baxevani2009,
   abstract = {In this paper, we construct a homogeneous spatio-temporal model to describe the variability of significant wave height over small regions of the sea and over short periods of time. Then, the model is extended to a nonhomogeneous one that is valid over larger areas of the sea and for time periods of up to 10 h. To validate the proposed model, we reconstruct the significant wave height surface under different scenarios and then compare it to satellite measurements and the C-ERA-40 field. Copyright © 2008 John Wiley & Sons, Ltd.},
   author = {A. Baxevani and S. Caires and I. Rychlik},
   doi = {10.1002/env.908},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Gaussian random fields,Random surface,Satellite data,Significant wave height,Stationary},
   month = {2},
   pages = {14-31},
   title = {Spatio-temporal statistical modelling of significant wave height},
   volume = {20},
   year = {2009}
}
@article{Olita2017,
   abstract = {Bio-physical glider measurements from a unique process-oriented experiment in the Eastern Alboran Sea (AlborEx) allowed us to observe the distribution of the deep chlorophyll maximum (DCM) across an intense density front, with a resolution (∼ 400 m) suitable for investigating sub-mesoscale dynamics. This front, at the interface between Atlantic and Mediterranean waters, had a sharp density gradient (Δρ ∼ 1 kg/m3 in ∼ 10 km) and showed imprints of (sub-)mesoscale phenomena on tracer distributions. Specifically, the chlorophyll-a concentration within the DCM showed a disrupted pattern along isopycnal surfaces, with patches bearing a relationship to the stratification (buoyancy frequency) at depths between 30 and 60 m. In order to estimate the primary production (PP) rate within the chlorophyll patches observed at the sub-surface, we applied the Morel and Andrè (J Geophys Res 96:685–698 1991) bio-optical model using the photosynthetic active radiation (PAR) from Argo profiles collected simultaneously with glider data. The highest production was located concurrently with domed isopycnals on the fresh side of the front, suggestive that (sub-)mesoscale upwelling is carrying phytoplankton patches from less to more illuminated levels, with a contemporaneous delivering of nutrients. Integrated estimations of PP (1.3 g C m−2d−1) along the glider path are two to four times larger than the estimations obtained from satellite-based algorithms, i.e., derived from the 8-day composite fields extracted over the glider trip path. Despite the differences in spatial and temporal sampling between instruments, the differences in PP estimations are mainly due to the inability of the satellite to measure DCM patches responsible for the high production. The deepest (depth > 60 m) chlorophyll patches are almost unproductive and probably transported passively (subducted) from upper productive layers. Finally, the relationship between primary production and oxygen is also investigated. The logarithm of the primary production in the DCM interior (chlorophyll (Chl) > 0.5 mg/m3) shows a linear negative relationship with the apparent oxygen utilization, confirming that high chlorophyll patches are productive. The slope of this relationship is different for Atlantic, mixed interface waters and Mediterranean waters, suggesting the presence of differences in planktonic communities (whether physiological, population, or community level should be object of further investigation) on the different sides of the front. In addition, the ratio of optical backscatter to Chl is high within the intermediate (mixed) waters, which is suggestive of large phytoplankton cells, and lower within the core of the Atlantic and Mediterranean waters. These observations highlight the relevance of fronts in triggering primary production at DCM level and shaping the characteristic patchiness of the pelagic domain. This gains further relevance considering the inadequacy of optical satellite sensors to observe DCM concentrations at such fine scales.},
   author = {Antonio Olita and Arthur Capet and Mariona Claret and Amala Mahadevan and Pierre Marie Poulain and Alberto Ribotti and Simón Ruiz and Joaquín Tintoré and Antonio Tovar-Sánchez and Ananda Pascual},
   doi = {10.1007/s10236-017-1058-z},
   issn = {16167228},
   issue = {6},
   journal = {Ocean Dynamics},
   keywords = {AOU,Fronts,Glider,Mediterranean sea,Primary production,sub-mesoscale},
   month = {6},
   pages = {767-782},
   publisher = {Springer Verlag},
   title = {Frontal dynamics boost primary production in the summer stratified Mediterranean sea},
   volume = {67},
   year = {2017}
}
@article{Coppola2023,
   abstract = {Intense glider monitoring was conducted in the Ligurian Sea for five months to capture the Net Community Production (NCP) variability in one of the most dynamic and productive regions of the Mediterranean Sea. Using the SeaExplorer glider technology, we were able to observe continuously from January to the end of May 2018 the physical and biogeochemical variables during the last period of intense convection observed in this region. High-frequency measurements from these gliders provided valuable information for determining dissolved O2 (DO) concentrations between coastal and open sea waters. Our DO balance approach provided an estimate of NCP fluxes complemented by the prediction of air-sea CO2 fluxes based on a neural network adapted to the Mediterranean Sea (CANYON-MED). Based on our NCP calculation method, our results show that the air-sea O2 flux and DO inventory have contributed largely to the NCP variability. The NCP values also suggest that heterotrophic conditions were predominant in winter and became autotrophic in spring, with strong variability in coastal waters due to the occurrence of sub-mesoscale structures. Finally, CO2 fluxes at the air-sea interface reveal that during the convection period, the central zone of the Ligurian Sea acted as a CO2 sink from January to March with little impact on NCP fluxes counterbalanced by a thermal effect of seawater.},
   author = {Laurent Coppola and Marine Fourrier and Orens Pasqueron de Fommervault and Antoine Poteau and Emilie Diamond Riquier and Laurent Béguery},
   doi = {10.3389/fmars.2023.1233845},
   issn = {22967745},
   journal = {Frontiers in Marine Science},
   keywords = {Mediterranean Sea,air-sea CO2 flux,dissolved oxygen,net community production,ocean gliders},
   publisher = {Frontiers Media SA},
   title = {High-resolution study of the air-sea CO2 flux and net community oxygen production in the Ligurian Sea by a fleet of gliders},
   volume = {10},
   year = {2023}
}
@article{Verzelen2010,
   abstract = {This paper studies the estimation of a large covariance matrix. We introduce a novel procedure called ChoSelect based on the Cholesky factor of the inverse covariance. This method uses a dimension reduction strategy by selecting the pattern of zero of the Cholesky factor. Alternatively, ChoSelect can be interpreted as a graph estimation procedure for directed Gaussian graphical models. Our approach is particularly relevant when the variables under study have a natural ordering (e.g. time series) or more generally when the Cholesky factor is approximately sparse. ChoSelect achieves non-asymptotic oracle inequalities with respect to the Kullback-Leibler entropy. Moreover, it satisfies various adaptive properties from a minimax point of view. We also introduce and study a two-stage procedure that combines ChoSelect with the Lasso. This last method enables the practitioner to choose his own trade-off between statistical efficiency and computational complexity. Moreover, it is consistent under weaker assumptions than the Lasso. The practical performances of the different procedures are assessed on numerical examples. © 2010, Institute of Mathematical Statistics. All rights reserved.},
   author = {Nicolas Verzelen},
   doi = {10.1214/10-EJS580},
   issn = {19357524},
   journal = {Electronic Journal of Statistics},
   keywords = {Banding,Cholesky decomposition,Covariance matrix,Directed graphical models,Minimax rate of estimation,Penalized criterion},
   pages = {1113-1150},
   title = {Adaptive estimation of covariance matrices via cholesky decomposition},
   volume = {4},
   year = {2010}
}
@article{Sang2017,
   abstract = {Functional principal component analysis (FPCA) is a popular approach in functional data analysis to explore major sources of variation in a sample of random curves. These major sources of variation are represented by functional principal components (FPCs). Most existing FPCA approaches use a set of flexible basis functions such as B-spline basis to represent the FPCs, and control the smoothness of the FPCs by adding roughness penalties. However, the flexible representations pose difficulties for users to understand and interpret the FPCs. In this article, we consider a variety of applications of FPCA and find that, in many situations, the shapes of top FPCs are simple enough to be approximated using simple parametric functions. We propose a parametric approach to estimate the top FPCs to enhance their interpretability for users. Our parametric approach can also circumvent the smoothing parameter selecting process in conventional nonparametric FPCA methods. In addition, our simulation study shows that the proposed parametric FPCA is more robust when outlier curves exist. The parametric FPCA method is demonstrated by analyzing several datasets from a variety of applications.},
   author = {Peijun Sang and Liangliang Wang and Jiguo Cao},
   doi = {10.1111/biom.12641},
   issn = {15410420},
   issue = {3},
   journal = {Biometrics},
   keywords = {Curve Variation,Eigenfuntions,Functional Data Analysis,Robust Estimation},
   month = {9},
   pages = {802-810},
   pmid = {28295173},
   title = {Parametric functional principal component analysis},
   volume = {73},
   year = {2017}
}
@article{Wang2022,
   abstract = {Multidimensional function data arise from many fields nowadays. The covariance function plays an important role in the analysis of such increasingly common data. In this article, we propose a novel nonparametric covariance function estimation approach under the framework of reproducing kernel Hilbert spaces (RKHS) that can handle both sparse and dense functional data. We extend multilinear rank structures for (finite-dimensional) tensors to functions, which allow for flexible modeling of both covariance operators and marginal structures. The proposed framework can guarantee that the resulting estimator is automatically semipositive definite, and can incorporate various spectral regularizations. The trace-norm regularization in particular can promote low ranks for both covariance operator and marginal structures. Despite the lack of a closed form, under mild assumptions, the proposed estimator can achieve unified theoretical results that hold for any relative magnitudes between the sample size and the number of observations per sample field, and the rate of convergence reveals the phase-transition phenomenon from sparse to dense functional data. Based on a new representer theorem, an ADMM algorithm is developed for the trace-norm regularization. The appealing numerical performance of the proposed estimator is demonstrated by a simulation study and the analysis of a dataset from the Argo project. Supplementary materials for this article are available online.},
   author = {Jiayi Wang and Raymond K.W. Wong and Xiaoke Zhang},
   doi = {10.1080/01621459.2020.1820344},
   issn = {1537274X},
   issue = {538},
   journal = {Journal of the American Statistical Association},
   keywords = {Functional data analysis,Multilinear ranks,Tensor product space,Unified theory},
   pages = {809-822},
   publisher = {American Statistical Association},
   title = {Low-Rank Covariance Function Estimation for Multidimensional Functional Data},
   volume = {117},
   year = {2022}
}
@article{Schmutz2020,
   abstract = {With the emergence of numerical sensors in many aspects of everyday life, there is an increasing need in analyzing multivariate functional data. This work focuses on the clustering of such functional data, in order to ease their modeling and understanding. To this end, a novel clustering technique for multivariate functional data is presented. This method is based on a functional latent mixture model which fits the data into group-specific functional subspaces through a multivariate functional principal component analysis. A family of parsimonious models is obtained by constraining model parameters within and between groups. An Expectation Maximization algorithm is proposed for model inference and the choice of hyper-parameters is addressed through model selection. Numerical experiments on simulated datasets highlight the good performance of the proposed methodology compared to existing works. This algorithm is then applied to the analysis of the pollution in French cities for 1 year.},
   author = {Amandine Schmutz and Julien Jacques and Charles Bouveyron and Laurence Chèze and Pauline Martin},
   doi = {10.1007/s00180-020-00958-4},
   issn = {16139658},
   issue = {3},
   journal = {Computational Statistics},
   keywords = {EM algorithm,Model-based clustering,Multivariate functional curves,Multivariate functional principal component analys},
   month = {9},
   pages = {1101-1131},
   publisher = {Springer},
   title = {Clustering multivariate functional data in group-specific functional subspaces},
   volume = {35},
   year = {2020}
}
@article{Porcu2023,
   abstract = {The Mat\'ern model has been a cornerstone of spatial statistics for more than half a century. More recently, the Mat\'ern model has been central to disciplines as diverse as numerical analysis, approximation theory, computational statistics, machine learning, and probability theory. In this article we take a Mat\'ern-based journey across these disciplines. First, we reflect on the importance of the Mat\'ern model for estimation and prediction in spatial statistics, establishing also connections to other disciplines in which the Mat\'ern model has been influential. Then, we position the Mat\'ern model within the literature on big data and scalable computation: the SPDE approach, the Vecchia likelihood approximation, and recent applications in Bayesian computation are all discussed. Finally, we review recent devlopments, including flexible alternatives to the Mat\'ern model, whose performance we compare in terms of estimation, prediction, screening effect, computation, and Sobolev regularity properties.},
   author = {Emilio Porcu and Moreno Bevilacqua and Robert Schaback and Chris J. Oates},
   month = {3},
   title = {The Mat\'ern Model: A Journey through Statistics, Numerical Analysis and Machine Learning},
   url = {http://arxiv.org/abs/2303.02759},
   year = {2023}
}
@article{Pulido2025,
   abstract = {With the rapid growth of data generation, advancements in functional data analysis have become essential, especially for approaches that handle multiple variables at the same time. This paper introduces a novel formulation of the epigraph and hypograph indices, along with their generalized expressions, specifically designed for multivariate functional data (MFD). These new definitions account for interrelationships between variables, enabling effective clustering of MFD based on the original data curves and their first two derivatives. The methodology developed here has been tested on simulated datasets, demonstrating strong performance compared to state-of-the-art methods. Its practical utility is further illustrated with two environmental datasets: the Canadian weather dataset and a 2023 air quality study in Madrid. These applications highlight the potential of the method as a great tool for analyzing complex environmental data, offering valuable insights for researchers and policymakers in climate and environmental research.},
   author = {Belén Pulido and Alba M. Franco-Pereira and Rosa E. Lillo},
   doi = {10.1007/s00477-025-02986-2},
   issn = {14363259},
   journal = {Stochastic Environmental Research and Risk Assessment},
   keywords = {Clustering,EHyClus,Environmental data analysis,Epigraph,Hypograph,Multivariate functional data},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Clustering multivariate functional data with the epigraph and hypograph indices: a case study on Madrid air quality},
   year = {2025}
}
@article{Yao2005a,
   abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
   author = {Fang Yao and Hans Georg Müller and Jane Ling Wang},
   doi = {10.1198/016214504000001745},
   issn = {01621459},
   issue = {470},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotics,Conditioning,Confidence band,Measurement error,Principal components,Simultaneous inference,Smoothing},
   month = {6},
   pages = {577-590},
   title = {Functional data analysis for sparse longitudinal data},
   volume = {100},
   year = {2005}
}
@misc{Zimek2012a,
   abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
   author = {Arthur Zimek and Erich Schubert and Hans Peter Kriegel},
   doi = {10.1002/sam.11161},
   issn = {19321864},
   issue = {5},
   journal = {Statistical Analysis and Data Mining},
   keywords = {Anomalies in high-dimensional data,Approximate outlier detection,Correlation outlier detection,Curse of dimensionality,Outlier detection in high-dimensional data,Subspace outlier detection},
   pages = {363-387},
   publisher = {John Wiley and Sons Inc},
   title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
   volume = {5},
   year = {2012}
}
@article{Evans2015,
   abstract = {In model-based clustering based on normal-mixture models, a few outlying observations can influence the cluster structure and number. This paper develops a method to identify these, however it does not attempt to identify clusters amidst a large field of noisy observations. We identify outliers as those observations in a cluster with minimal membership proportion or for which the cluster-specific variance with and without the observation is very different. Results from a simulation study demonstrate the ability of our method to detect true outliers without falsely identifying many non-outliers and improved performance over other approaches, under most scenarios. We use the contributed R package MCLUST for model-based clustering, but propose a modified prior for the cluster-specific variance which avoids degeneracies in estimation procedures. We also compare results from our outlier method to published results on National Hockey League data.},
   author = {Katie Evans and Tanzy Love and Sally W. Thurston},
   doi = {10.1007/s00357-015-9171-5},
   issn = {14321343},
   issue = {1},
   journal = {Journal of Classification},
   keywords = {Influential points,MCLUST,National Hockey League,Normal-mixture models,Prior},
   month = {4},
   pages = {63-84},
   publisher = {Springer New York LLC},
   title = {Outlier Identification in Model-Based Cluster Analysis},
   volume = {32},
   year = {2015}
}
@techReport{Pang,
   abstract = {The large proportion of irrelevant or noisy features in real-life high-dimensional data presents a significant challenge to subspace/feature selection-based high-dimensional out-lier detection (a.k.a. outlier scoring) methods. These methods often perform the two dependent tasks: relevant feature subset search and outlier scoring independently, consequently retaining features/subspaces irrelevant to the scoring method and downgrading the detection performance. This paper introduces a novel sequential ensemble-based framework SEMSE and its instance CINFO to address this issue. SEMSE learns the sequential ensembles to mutually refine feature selection and outlier scoring by iterative sparse modeling with outlier scores as the pseudo target feature. CINFO instantiates SEMSE by using three successive recurrent components to build such sequential ensembles. Given outlier scores output by an existing outlier scoring method on a feature subset , CINFO first defines a Cantelli's inequality-based out-lier thresholding function to select outlier candidates with a false positive upper bound. It then performs lasso-based sparse regression by treating the outlier scores as the target feature and the original features as predictors on the out-lier candidate set to obtain a feature subset that is tailored for the outlier scoring method. Our experiments show that two different outlier scoring methods enabled by CINFO (i) perform significantly better on 11 real-life high-dimensional data sets, and (ii) have much better resilience to noisy features , compared to their bare versions and three state-of-the-art competitors. The source code of CINFO is available at https://sites.google.com/site/gspangsite/sourcecode.},
   author = {Guansong Pang and Longbing Cao and Ling Chen and Defu Lian and Huan Liu},
   keywords = {Machine Learning Methods Track},
   title = {Sparse Modeling-Based Sequential Ensemble Learning for Effective Outlier Detection in High-Dimensional Numeric Data},
   url = {www.aaai.org}
}
@article{AsirAntonyGnanaSingh2016,
   abstract = {Reliability, lack of error, and security are important improvements to quality of service. Outlier detection is a process of detecting the erroneous parts or abnormal objects in defined populations, and can contribute to secured and error-free services. Outlier detection approaches can be categorized into four types: statistic-based, unsupervised, supervised, and semi-supervised. A model-based outlier detection system with statistical preprocessing is proposed, taking advantage of the statistical approach to preprocess training data and using unsupervised learning to construct the model. The robustness of the proposed system is evaluated using the performance evaluation metrics sum of squared error (SSE) and time to build model (TBM). The proposed system performs better for detecting outliers regardless of the application domain.},
   author = {D. Asir Antony Gnana Singh and E. Jebalamar Leavline},
   doi = {10.22237/jmasm/1462077480},
   issn = {15389472},
   issue = {1},
   journal = {Journal of Modern Applied Statistical Methods},
   keywords = {Anomaly detection,Inter-quartile range,Outlier,Preprocessing},
   pages = {789-801},
   publisher = {Wayne State University},
   title = {Model-based outlier detection system with statistical preprocessing},
   volume = {15},
   year = {2016}
}
@techReport{Angiulli,
   abstract = {In this paper, a new definition of distance-based outlier and an algorithm, called HilOut, designed to efficiently detect the top n outliers of a large and high-dimensional data set are proposed. Given an integer k, the weight of a point is defined as the sum of the distances separating it from its k nearest-neighbors. Outlier are those points scoring the largest values of weight. The algorithm HilOut makes use of the notion of space-filling curve to linearize the data set, and it consists of two phases. The first phase provides an approximate solution, within a rough factor, after the execution of at most d þ 1 sorts and scans of the data set, with temporal cost quadratic in d and linear in N and in k, where d is the number of dimensions of the data set and N is the number of points in the data set. During this phase, the algorithm isolates points candidate to be outliers and reduces this set at each iteration. If the size of this set becomes n, then the algorithm stops reporting the exact solution. The second phase calculates the exact solution with a final scan examining further the candidate outliers that remained after the first phase. Experimental results show that the algorithm always stops, reporting the exact solution, during the first phase after much less than d þ 1 steps. We present both an in-memory and disk-based implementation of the HilOut algorithm and a thorough scaling analysis for real and synthetic data sets showing that the algorithm scales well in both cases.},
   author = {Fabrizio Angiulli and Clara Pizzuti},
   keywords = {Index Terms-Outlier mining,space-filling curves},
   title = {Outlier Mining in Large High-Dimensional Data Sets}
}
@inproceedings{Klerx2014,
   abstract = {Model-based anomaly detection in technical systems is an important application field of artificial intelligence. We consider discrete event systems, which is a system class to which a wide range of relevant technical systems belong and for which no comprehensive model-based anomaly detection approach exists so far. The original contributions of this paper are threefold: First, we identify the types of anomalies that occur in discrete event systems and we propose a tailored behavior model that captures all anomaly types, called probabilistic deterministic timed-transition automata (PDTTA). Second, we present a new algorithm to learn a PDTTA from sample observations of a system. Third, we describe an approach to detect anomalies based on a learned PDTTA. An empirical evaluation in a practical application, namely ATM fraud detection, shows promising results.},
   author = {Timo Klerx and Maik Anderka and Hans Kleine Buning and Steffen Priesterjahn},
   doi = {10.1109/ICTAI.2014.105},
   isbn = {9781479965724},
   issn = {10823409},
   booktitle = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
   keywords = {ATM Fraud Detection,Automatic Model Generation,Discrete Event Systems,Model-based Anomaly Detection},
   month = {12},
   pages = {665-672},
   publisher = {IEEE Computer Society},
   title = {Model-Based Anomaly Detection for Discrete Event Systems},
   volume = {2014-Decem},
   year = {2014}
}
@inproceedings{Moallemi2021,
   abstract = {Modern Structural Health Monitoring (SHM) systems are becoming of pervasive use in civil engineering because they can track the structural condition and detect damages of critical and civil infrastructures such as buildings, viaducts, and tunnels. Although noticeable work has been done to improve anomaly detection for ensuring public safety, algorithms that can be executed on low-cost hardware for long-term monitoring are still an open issue to the community. This paper presents a new framework that exploits compression techniques to identify anomalies in the structure, avoiding continuous streaming of raw data to the cloud. We used a real installation on a bridge in Italy to test the proposed anomaly detection algorithm. We trained three compression models, namely a Principal Component Analysis (PCA), a fully-connected autoencoder, and a convolutional autoencoder. Performance comparison is also provided through an ablation study that analyzes the impact of various parameters. Results demonstrate that the model-based approach, i.e., PCA, can reach a better accuracy whereas data-driven models, i.e., autoencoders, are limited by training set size.},
   author = {Amirhossein Moallemi and Alessio Burrello and Davide Brunelli and Luca Benini},
   doi = {10.1109/I2MTC50364.2021.9459999},
   isbn = {9781728195391},
   issn = {10915281},
   booktitle = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
   keywords = {Anomaly detection,Compression Techniques,Deep Learning,Edge computing,Structural Health Monitoring},
   month = {5},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Model-based vs. Data-driven Approaches for Anomaly Detection in Structural Health Monitoring: A Case Study},
   volume = {2021-May},
   year = {2021}
}
@article{Pigoli2014a,
   abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
   author = {Davide Pigoli and John A.D. Aston and Ian L. Dryden and Piercesare Secchi},
   doi = {10.1093/biomet/asu008},
   issn = {14643510},
   issue = {2},
   journal = {Biometrika},
   keywords = {Distance metric,Functional data analysis,Procrustes analysis,Shape analysis},
   pages = {409-422},
   publisher = {Oxford University Press},
   title = {Distances and inference for covariance operators},
   volume = {101},
   year = {2014}
}
@misc{Correia2024a,
   abstract = {Time-series anomaly detection plays an important role in engineering processes, like development, manufacturing and other operations involving dynamic systems. These processes can greatly benefit from advances in the field, as state-of-the-art approaches may aid in cases involving, for example, highly dimensional data. To provide the reader with understanding of the terminology, this survey introduces a novel taxonomy where a distinction between online and offline, and training and inference is made. Additionally, it presents the most popular data sets and evaluation metrics used in the literature, as well as a detailed analysis. Furthermore, this survey provides an extensive overview of the state-of-the-art model-based online semi- and unsupervised anomaly detection approaches for multivariate time-series data, categorising them into different model families and other properties. The biggest research challenge revolves around benchmarking, as currently there is no reliable way to compare different approaches against one another. This problem is two-fold: on the one hand, public data sets suffers from at least one fundamental flaw, while on the other hand, there is a lack of intuitive and representative evaluation metrics in the field. Moreover, the way most publications choose a detection threshold disregards real-world conditions, which hinders the application in the real world. To allow for tangible advances in the field, these issues must be addressed in future work.},
   author = {Lucas Correia and Jan Christoph Goos and Philipp Klein and Thomas Bäck and Anna V. Kononova},
   doi = {10.1016/j.engappai.2024.109323},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Anomaly detection,Model-based,Multivariate,Online,Survey,Time series},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Online model-based anomaly detection in multivariate time series: Taxonomy, survey, research challenges and future directions},
   volume = {138},
   year = {2024}
}
@article{Eiteneuer2020,
   abstract = {Anomaly detection is the task of detecting data which differs from the normal behaviour of a system in a given context. In order to approach this problem, data-driven models can be learned to predict current or future observations. Oftentimes, anomalous behaviour depends on the internal dynamics of the system and looks normal in a static context. To address this problem, the model should also operate depending on state. Long Short-Term Memory (LSTM) neural networks have been shown to be particularly useful to learn time sequences with varying length of temporal dependencies and are therefore an interesting general purpose approach to learn the behaviour of arbitrarily complex Cyber-Physical Systems. In order to perform anomaly detection, we slightly modify the standard norm 2 error to incorporate an estimate of model uncertainty. We analyse the approach on artificial and real data.},
   author = {Benedikt Eiteneuer and Oliver Niggemann},
   month = {10},
   title = {LSTM for Model-Based Anomaly Detection in Cyber-Physical Systems},
   url = {http://arxiv.org/abs/2010.15680},
   year = {2020}
}
@article{Olteanu2023a,
   abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
   author = {Madalina Olteanu and Fabrice Rossi and Florian Yger},
   doi = {10.1016/j.neucom.2023.126634},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Anomaly detection,Meta-survey,Outlier detection},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Meta-survey on outlier and anomaly detection},
   volume = {555},
   year = {2023}
}
@inproceedings{Zhang2022a,
   abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
   author = {Sean Zhang and Varun Ursekar and Leman Akoglu},
   doi = {10.1145/3534678.3539076},
   isbn = {9781450393850},
   booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Apache Spark,data-parallel algorithms,distributed outlier detection},
   month = {8},
   pages = {4530-4540},
   publisher = {Association for Computing Machinery},
   title = {Sparx: Distributed Outlier Detection at Scale},
   year = {2022}
}
@article{Tong2022,
   abstract = {The use of the multivariate contaminated normal (MCN) distribution in model-based clustering is recommended to cluster data characterized by mild outliers, the model can at the same time detect outliers automatically and produce robust parameter estimates in each cluster. However, one of the limitations of this approach is that it requires complete data, i.e. the MCN cannot be used directly on data with missing values. In this paper, we develop a framework for fitting a mixture of MCN distributions to incomplete data sets, i.e. data sets with some values missing at random. Parameter estimation is obtained using the expectation-conditional maximization algorithm—a variant of the expectation-maximization algorithm in which the traditional maximization steps are instead replaced by simpler conditional maximization steps. We perform a simulation study to compare the results of our model to a mixture of multivariate normal and Student’s t distributions for incomplete data. The simulation also includes a study on the effect of the percentage of missing data on the performance of the three algorithms. The model is then applied to the Automobile data set (UCI machine learning repository). The results show that, while the Student’s t distribution gives similar classification performance, the MCN works better in detecting outliers with a lower false positive rate of outlier detection. The performance of all the techniques decreases linearly as the percentage of missing values increases.},
   author = {Hung Tong and Cristina Tortora},
   doi = {10.1007/s11634-021-00476-1},
   issn = {18625355},
   issue = {1},
   journal = {Advances in Data Analysis and Classification},
   keywords = {Contaminated normal distribution,Data missing at random,Model-based clustering,Outliers},
   month = {3},
   pages = {5-30},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Model-based clustering and outlier detection with missing data},
   volume = {16},
   year = {2022}
}
@article{Claeskens2014,
   abstract = {This article defines and studies a depth for multivariate functional data. By the multivariate nature and by including a weight function, it acknowledges important characteristics of functional data, namely differences in the amount of local amplitude, shape, and phase variation. We study both population and finite sample versions. The multivariate sample of curves may include warping functions, derivatives, and integrals of the original curves for a better overall representation of the functional data via the depth.We present a simulation study and data examples that confirm the good performance of this depth function. Supplementary materials for this article are available online. © 2014 American Statistical Association.},
   author = {Gerda Claeskens and Mia Hubert and Leen Slaets and Kaveh Vakili},
   doi = {10.1080/01621459.2013.856795},
   issn = {1537274X},
   issue = {505},
   journal = {Journal of the American Statistical Association},
   keywords = {Functional data,Multivariate data,Statistical depth,Time warping},
   pages = {411-423},
   publisher = {American Statistical Association},
   title = {Multivariate functional halfspace depth},
   volume = {109},
   year = {2014}
}
@techReport{Li,
   abstract = {Recent advances in cDNA and oligonucleotide DNA arrays have made it possible to measure the abundance of mRNA transcripts for many genes simultaneously. The analysis of such experiments is nontrivial because of large data size and many levels of variation introduced at different stages of the experiments. The analysis is further complicated by the large differences that may exist among different probes used to interrogate the same gene. However, an attractive feature of high-density oligonucleotide arrays such as those produced by photolithography and inkjet technology is the standardization of chip manufacturing and hybridization process. As a result, probe-specific biases, although significant, are highly reproducible and predictable, and their adverse effect can be reduced by proper modeling and analysis methods. Here, we propose a statistical model for the probe-level data, and develop model-based estimates for gene expression indexes. We also present model-based methods for identifying and handling cross-hybridizing probes and contaminating array regions. Applications of these results will be presented elsewhere.},
   author = {Cheng Li and Wing Hung Wong},
   title = {Model-based analysis of oligonucleotide arrays: Expression index computation and outlier detection},
   url = {www.pnas.orgcgidoi10.1073pnas.011404098}
}
@article{Zheng2022b,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@article{Jimenez-Varon2024a,
   abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
   author = {Cristian F. Jiménez-Varón and Fouzi Harrou and Ying Sun},
   doi = {10.1002/env.2851},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {data depth,functional data,magnitude outliers,pairwise depth,pointwise depth,shape outliers,visualization},
   month = {8},
   publisher = {John Wiley and Sons Ltd},
   title = {Pointwise data depth for univariate and multivariate functional outlier detection},
   volume = {35},
   year = {2024}
}
@article{Li2020b,
   abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
   author = {Xiaojie Li and Jiancheng Lv and Zhang Yi},
   doi = {10.1109/TCYB.2018.2876615},
   issn = {21682275},
   issue = {5},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Discrimination,outlier detection,outlier factor,structural scores},
   month = {5},
   pages = {2302-2310},
   pmid = {30418896},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Outlier Detection Using Structural Scores in a High-Dimensional Space},
   volume = {50},
   year = {2020}
}
@article{Dietzel2024a,
   abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
   author = {Andreas Dietzel and Marco Moretti and Lauren M. Cook},
   doi = {10.1016/j.ecoinf.2024.102561},
   issn = {15749541},
   journal = {Ecological Informatics},
   keywords = {Bayesian projection predictive variable selection,Blue-green infrastructure,Nature-based solutions,Shrinkage prior,Species distribution model,Urban biodiversity},
   month = {7},
   publisher = {Elsevier B.V.},
   title = {Shrinkage-based Bayesian variable selection for species distribution modelling in complex environments: An application to urban biodiversity},
   volume = {81},
   year = {2024}
}
@article{Welbaum2025,
   abstract = {Misalignment often occurs in functional data and can severely impact their clustering results. A clustering algorithm for misaligned functional data is developed, by adapting the original mean shift algorithm in the Euclidean space. This mean shift algorithm is applied to the quotient space of the orbits of the square root velocity functions induced by the misaligned functional data, in which the elastic distance is equipped. Convergence properties of this algorithm are studied. The efficacy of the algorithm is demonstrated through simulations and various real data applications.},
   author = {Andrew Welbaum and Wanli Qiao},
   doi = {10.1016/j.csda.2024.108107},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Clustering,Elastic distance,Functional data,Gradient ascent,Mean shift,Misalignment},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Mean shift-based clustering for misaligned functional data},
   volume = {206},
   year = {2025}
}
@book{Becker2013,
   abstract = {This Festschrift in honour of Ursula Gather’s 60th birthday deals with modern topics in the field of robust statistical methods, especially for time series and regression analysis, and with statistical methods for complex data structures. The individual contributions of leading experts provide a textbook-style overview of the topic, supplemented by current research results and questions. The statistical theory and methods in this volume aim at the analysis of data which deviate from classical stringent model assumptions, which contain outlying values and/or have a complex structure. Written for researchers as well as master and PhD students with a good knowledge of statistics.},
   author = {Claudia Becker and Roland Fried and Sonja Kuhnt},
   doi = {10.1007/978-3-642-35494-6},
   isbn = {9783642354946},
   journal = {Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather},
   month = {1},
   pages = {1-379},
   publisher = {Springer Berlin Heidelberg},
   title = {Robustness and complex data structures: Festschrift in honour of Ursula Gather},
   year = {2013}
}
@techReport{Salehia,
   abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
   author = {Mohammadreza Salehi and Salehidehnavi@uva Nl and Hossein Mirzaei and Dan Hendrycks and Yixuan Li and Mohammad Hossein Rohban and Mohammad Sabokrou},
   title = {A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges},
   url = {https://github.com/taslimisina/osr-ood-ad-methods}
}
@article{Brault2024,
   author = {Vincent Brault and Émilie Devijver and Charlotte Laclau},
   doi = {10.1214/24-EJS2286},
   issn = {1935-7524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   month = {1},
   title = {Mixture of segmentation for heterogeneous functional data},
   volume = {18},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-18/issue-2/Mixture-of-segmentation-for-heterogeneous-functional-data/10.1214/24-EJS2286.full},
   year = {2024}
}
@article{James2003,
   abstract = {We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low-dimensional representations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite-dimensional covariates and show how it can be applied to standard finite-dimensional clustering problems involving missing data.},
   author = {Gareth M. James and Catherine A. Sugar},
   doi = {10.1198/016214503000189},
   issn = {01621459},
   issue = {462},
   journal = {Journal of the American Statistical Association},
   keywords = {Curve estimation,Discriminant functions,Functional clustering,High-dimensional data},
   month = {6},
   pages = {397-408},
   title = {Clustering for sparsely sampled functional data},
   volume = {98},
   year = {2003}
}
@article{Li2020a,
   abstract = {Covariance estimation is essential yet underdeveloped for analysing multivariate functional data. We propose a fast covariance estimation method for multivariate sparse functional data using bivariate penalized splines. The tensor-product B-spline formulation of the proposed method enables a simple spectral decomposition of the associated covariance operator and explicit expressions of the resulting eigenfunctions as linear combinations of B-spline bases, thereby dramatically facilitating subsequent principal component analysis. We derive a fast algorithm for selecting the smoothing parameters in covariance smoothing using leave-one-subject-out cross-validation. The method is evaluated with extensive numerical studies and applied to an Alzheimer's disease study with multiple longitudinal outcomes.},
   author = {Cai Li and Luo Xiao and Sheng Luo},
   doi = {10.1002/sta4.245},
   issn = {20491573},
   issue = {1},
   journal = {Stat},
   keywords = {bivariate smoothing,covariance function,functional principal component analysis,longitudinal data,multivariate functional data,prediction},
   month = {12},
   publisher = {Blackwell Publishing Ltd},
   title = {Fast covariance estimation for multivariate sparse functional data},
   volume = {9},
   year = {2020}
}
@article{Chiou2014b,
   abstract = {We propose an extended version of the classical Karhunen-Lòeve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
   author = {Jeng Min Chiou and Yu Ting Chen and Ya Fang Yang},
   doi = {10.5705/ss.2013.305},
   issn = {10170405},
   issue = {4},
   journal = {Statistica Sinica},
   keywords = {Karhunen-LòEve expansion,Mercer's theorem,Multivariate functional data,Normalization,Traffic flow},
   month = {10},
   pages = {1571-1596},
   publisher = {Institute of Statistical Science},
   title = {Multivariate functional principal component analysis: A normalization approach},
   volume = {24},
   year = {2014}
}
@article{Happ2018a,
   abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen–Loève Theorem. For the practically relevant case of a finite Karhunen–Loève representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
   author = {Clara Happ and Sonja Greven},
   doi = {10.1080/01621459.2016.1273115},
   issn = {1537274X},
   issue = {522},
   journal = {Journal of the American Statistical Association},
   keywords = {Dimension reduction,Functional data analysis,Image analysis,Multivariate functional data},
   month = {4},
   pages = {649-659},
   publisher = {American Statistical Association},
   title = {Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains},
   volume = {113},
   year = {2018}
}
@article{Diquigiovanni2022,
   abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.},
   author = {Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
   doi = {10.1016/j.jmva.2021.104879},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Conformal Prediction,Distribution-free prediction set,Exact prediction set,Finite-sample prediction set,Functional data,Prediction band},
   month = {5},
   publisher = {Academic Press Inc.},
   title = {Conformal prediction bands for multivariate functional data},
   volume = {189},
   year = {2022}
}
@article{Ojo2023,
   abstract = {We present definitions and properties of the fast massive unsupervised outlier detection (FastMUOD) indices, used for outlier detection (OD) in functional data. FastMUOD detects outliers by computing, for each curve, an amplitude, magnitude, and shape index meant to target the corresponding types of outliers. Some methods adapting FastMUOD to outlier detection in multivariate functional data are then proposed. These include applying FastMUOD on the components of the multivariate data and using random projections. Moreover, these techniques are tested on various simulated and real multivariate functional datasets. Compared with the state of the art in multivariate functional OD, the use of random projections showed the most effective results with similar, and in some cases improved, OD performance. Based on the proportion of random projections that flag each multivariate function as an outlier, we propose a new graphical tool, the magnitude-shape-amplitude (MSA) plot, useful for visualizing the magnitude, shape and amplitude outlyingness of multivariate functional data.},
   author = {Oluwasegun Taiwo Ojo and Antonio Fernández Anta and Marc G. Genton and Rosa E. Lillo},
   doi = {10.1002/sta4.567},
   issn = {20491573},
   issue = {1},
   journal = {Stat},
   keywords = {FastMUOD,functional data,functional outlier detection,multivariate functional data,outlier classification,video data},
   month = {1},
   publisher = {John Wiley and Sons Inc},
   title = {Multivariate functional outlier detection using the fast massive unsupervised outlier detection indices},
   volume = {12},
   year = {2023}
}
@article{Elias2023,
   abstract = {Partially observed functional data are frequently encountered in applications and are the object of an increasing interest by the literature. We here address the problem of measuring the centrality of a datum in a partially observed functional sample. We propose an integrated functional depth for partially observed functional data, dealing with the very challenging case where partial observability can occur systematically on any observation of the functional dataset. In particular, differently from many techniques for partially observed functional data, we do not request that some functional datum is fully observed, nor we require that a common domain exist, where all of the functional data are recorded. Because of this, our proposal can also be used in those frequent situations where reconstructions methods and other techniques for partially observed functional data are inapplicable. By means of simulation studies, we demonstrate the very good performances of the proposed depth on finite samples. Our proposal enables the use of benchmark methods based on depths, originally introduced for fully observed data, in the case of partially observed functional data. This includes the functional boxplot, the outliergram and the depth versus depth classifiers. We illustrate our proposal on two case studies, the first concerning a problem of outlier detection in German electricity supply functions, the second regarding a classification problem with data obtained from medical imaging. Supplementary materials for this article are available online.},
   author = {Antonio Elías and Raúl Jiménez and Anna M. Paganoni and Laura M. Sangalli},
   doi = {10.1080/10618600.2022.2070171},
   issn = {15372715},
   issue = {2},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Classification of partially observed functional da,Functional boxplot,Functional depth,Functional outliers,Incomplete functional data,Robustness},
   pages = {341-352},
   publisher = {Taylor and Francis Ltd.},
   title = {Integrated Depths for Partially Observed Functional Data},
   volume = {32},
   year = {2023}
}
@article{Cuevas2007,
   abstract = {Five notions of data depth are considered. They are mostly designed for functional data but they can be also adapted to the standard multivariate case. The performance of these depth notions, when used as auxiliary tools in estimation and classification, is checked through a Monte Carlo study. © 2007 Springer-Verlag.},
   author = {Antonio Cuevas and Manuel Febrero and Ricardo Fraiman},
   doi = {10.1007/s00180-007-0053-0},
   issn = {09434062},
   issue = {3},
   journal = {Computational Statistics},
   keywords = {Depth measures,Functional data,Projections method,Supervised classification},
   month = {9},
   pages = {481-496},
   title = {Robust estimation and classification for functional data via projection-based depth notions},
   volume = {22},
   year = {2007}
}
@article{Yeon2024,
   abstract = {Data depth is a powerful nonparametric tool originally proposed to rank multivariate data from center outward. In this context, one of the most archetypical depth notions is Tukey's halfspace depth. In the last few decades notions of depth have also been proposed for functional data. However, Tukey's depth cannot be extended to handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at center, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in $L^2$, a very general space of functions that include non-smooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in terms of detecting outliers of different types. Three real data examples showcase the proposed depth notion.},
   author = {Hyemin Yeon and Xiongtao Dai and Sara Lopez-Pintado},
   month = {5},
   title = {Regularized Halfspace Depth for Functional Data},
   url = {http://arxiv.org/abs/2311.07034},
   year = {2024}
}
@article{Gijbels2017,
   abstract = {In this paper, we provide an elaboration on the desirable properties of statistical depths for functional data. Although a formal definition has been put forward in the literature, there are still several unclarities to be tackled, and further insights to be gained. Herein, a few interesting connections between the wanted properties are found. In particular, it is demonstrated that the conditions needed for some desirable properties to hold are extremely demanding, and virtually impossible to be met for common depths. We establish adaptations of these properties which prove to be still sensible, and more easily met by common functional depths.},
   author = {Irène Gijbels and Stanislav Nagy},
   doi = {10.1214/17-STS625},
   issn = {08834237},
   issue = {4},
   journal = {Statistical Science},
   keywords = {Data depth,Functional data,Multivariate statistics,Robustness},
   month = {11},
   pages = {630-639},
   publisher = {Institute of Mathematical Statistics},
   title = {On a general definition of depth for functional data},
   volume = {32},
   year = {2017}
}
@article{Nagy2017,
   abstract = {A major drawback of many established depth functionals is their ineffectiveness in identifying functions outlying merely in shape. Herein, a simple modification of functional depth is proposed to provide a remedy for this difficulty. The modification is versatile, widely applicable, and introduced without imposing any assumptions on the data, such as differentiability. It is shown that many favorable attributes of the original depths for functions, including consistency properties, remain preserved for the modified depths. The powerfulness of the new approach is demonstrated on a number of examples for which the known depths fail to identify the outlying functions. Supplementary material for this article is available online.},
   author = {Stanislav Nagy and Irène Gijbels and Daniel Hlubinka},
   doi = {10.1080/10618600.2017.1336445},
   issn = {15372715},
   issue = {4},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Data depth,Functional data,Infimal depth,Integrated depth,Outlying functions,Shape outliers},
   month = {10},
   pages = {883-893},
   publisher = {American Statistical Association},
   title = {Depth-Based Recognition of Shape Outlying Functions},
   volume = {26},
   year = {2017}
}
@article{Nieto-Reyes2016,
   abstract = {The main focus of this work is on providing a formal definition of statistical depth for functional data on the basis of six properties, recognising topological features such as continuity, smoothness and contiguity. Amongst our depth defining properties is one that addresses the delicate challenge of inherent partial observability of functional data, with fulfillment giving rise to a minimal guarantee on the performance of the empirical depth beyond the idealised and practically infeasible case of full observability. As an incidental product, functional depths satisfying our definition achieve a robustness that is commonly ascribed to depth, despite the absence of a formal guarantee in the multivariate definition of depth. We demonstrate the fulfillment or otherwise of our properties for six widely used functional depth proposals, thereby providing a systematic basis for selection of a depth function.},
   author = {Alicia Nieto-Reyes and Heather Battey},
   doi = {10.1214/15-STS532},
   issn = {08834237},
   issue = {1},
   journal = {Statistical Science},
   keywords = {Functional data,Multivariate statistics,Partial observability,Robustness,Statistical depth},
   pages = {61-79},
   publisher = {Institute of Mathematical Statistics},
   title = {A topologically valid definition of depth for functional data},
   volume = {31},
   year = {2016}
}
@article{Nagy2016a,
   abstract = {Several depths suitable for infinite-dimensional functional data that are available in the literature are of the form of an integral of a finite-dimensional depth function. These functionals are characterized by projecting functions into low-dimensional spaces, taking finite-dimensional depths of the projected quantities, and finally integrating these projected marginal depths over a preset collection of projections. In this paper, a general class of integrated depths for functions is considered. Several depths for functional data proposed in the literature during the last decades are members of this general class. A comprehensive study of its most important theoretical properties, including measurability and consistency, is given. It is shown that many, but not all, properties of the integrated depth are shared with the finite-dimensional depth that constitutes its building block. Some pending measurability issues connected with all integrated depth functionals are resolved, a broad new notion of symmetry for functional data is proposed, and difficulties with respect to consistency results are identified. A general universal consistency result for the sample depth version, and for the generalized median, for integrated depth for functions is derived.},
   author = {Stanislav Nagy and Irène Gijbels and Marek Omelka and Daniel Hlubinka},
   doi = {10.1051/ps/2016005},
   issn = {12623318},
   journal = {ESAIM - Probability and Statistics},
   keywords = {Center of symmetry,Functional data,Generalized median,Integrated depth,Measurability,Strong consistency,Weak consistency},
   pages = {95-130},
   publisher = {EDP Sciences},
   title = {Integrated depth for functional data: Statistical properties and consistency},
   volume = {20},
   year = {2016}
}
@article{Yeon2025,
   abstract = {Data depth is a powerful tool originally proposed to rank multivariate data from centre outward. In this context, one of the most archetypical depth notions is Tukey’s halfspace depth. In the last few decades, notions of depth have also been proposed for functional data. However, a naive extension of Tukey’s depth cannot handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data, which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at centre, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in the space of all square-integrable functions, a very general space of functions that include nonsmooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in detecting outliers of different types. Real data examples showcase the proposed depth.},
   author = {Hyemin Yeon and Xiongtao Dai and Sara Lopez-Pintado},
   doi = {10.1093/jrsssb/qkaf030},
   issn = {1369-7412},
   journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
   month = {6},
   publisher = {Oxford University Press (OUP)},
   title = {Regularized halfspace depth for functional data},
   year = {2025}
}
@article{Cuevas2009a,
   abstract = {A general depth measure, based on the use of one-dimensional linear continuous projections, is proposed. The applicability of this idea in different statistical setups (including inference in functional data analysis, image analysis and classification) is discussed. A special emphasis is made on the possible usefulness of this method in some statistical problems where the data are elements of a Banach space. The asymptotic properties of the empirical approximation of the proposed depth measure are investigated. In particular, its asymptotic distribution is obtained through U-statistics techniques. The practical aspects of these ideas are discussed through a small simulation study and a real-data example. © 2008 Elsevier Inc. All rights reserved.},
   author = {Antonio Cuevas and Ricardo Fraiman},
   doi = {10.1016/j.jmva.2008.08.002},
   issn = {0047259X},
   issue = {4},
   journal = {Journal of Multivariate Analysis},
   keywords = {62G07,62G20,Depth measures,Functional data,Projections method,Supervised classification,primary,secondary},
   month = {4},
   pages = {753-766},
   title = {On depth measures and dual statistics. A methodology for dealing with general data},
   volume = {100},
   year = {2009}
}
@article{Mosler2018,
   abstract = {A data depth measures the centrality of a point with respect to an empirical distribution. Postulates are formulated, which a depth for functional data should satisfy, and a general approach is proposed to construct multivariate data depths in Banach spaces. The new approach, mentioned as Phi-depth, is based on depth infima over a proper set Phi of R^d-valued linear functions. Several desirable properties are established for the Phi-depth and a generalized version of it. The general notions include many new depths as special cases. In particular a location-slope depth and a principal component depth are introduced.},
   author = {Karl Mosler and Yulia Polyakova},
   month = {1},
   title = {General notions of depth for functional data},
   url = {http://arxiv.org/abs/1208.1981},
   year = {2018}
}
@article{Ramsay2019,
   abstract = {We study depth measures for multivariate data defined by integrating univariate depth measures, specifically, integrated dual (ID) depth introduced by Cuevas and Fraiman (2009) which integrates univariate simplicial depth, and integrated rank-weighted (IRW) depth, which integrates univariate Tukey depth. We build on the results of Cuevas and Fraiman (2009) to show that IRW depth shares many depth properties with ID depth. Further, we provide additional results on exact computation, decreasing along rays, continuity and breakdown point that apply to both ID and IRW depth. We also establish asymptotic normality and consistency of the sample IRW depths. Lastly, we demonstrate the use of this depth measure with real and simulated datasets: calculating robust location estimators and dd-plots.},
   author = {Kelly Ramsay and Stéphane Durocher and Alexandre Leblanc},
   doi = {10.1016/j.jmva.2019.02.001},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {High-dimensional data depth,Integrated dual depth,dd-plots},
   month = {9},
   pages = {51-69},
   publisher = {Academic Press Inc.},
   title = {Integrated rank-weighted depth},
   volume = {173},
   year = {2019}
}
@article{Bai2020,
   abstract = {Data gaps in surface air quality measurements significantly impair the data quality and the exploration of these valuable data sources. In this study, a novel yet practical method called diurnal-cycle-constrained empirical orthogonal function (DCCEOF) was developed to fill in data gaps present in data records with evident temporal variability. The hourly PM2:5 concentration data retrieved from the national ambient air quality monitoring network in China were used as a demonstration. The DCCEOF method aims to reconstruct the diurnal cycle of PM2:5 concentration from its discrete neighborhood field in space and time firstly and then predict the missing values by calibrating the reconstructed diurnal cycle to the level of valid PM2:5 concentrations observed at adjacent times. The statistical results indicate a high frequency of data gaps in our retrieved hourly PM2:5 concentration record, with PM2:5 concentration measured on about 40 % of the days suffering from data gaps. Further sensitivity analysis results reveal that data gaps in the hourly PM2:5 concentration record may introduce significant bias to its daily averages, especially during clean episodes at which PM2:5 daily averages are observed to be subject to larger uncertainties compared to the polluted days (even in the presence of the same amount of missingness). The cross-validation results indicate that our suggested DCCEOF method has a good prediction accuracy, particularly in predicting daily peaks and/or minima that cannot be restored by conventional interpolation approaches, thus confirming the effectiveness of the consideration of the local diurnal variation pattern in gap filling. By applying the DCCEOF method to the hourly PM2:5 concentration record measured in China from 2014 to 2019, the data completeness ratio was substantially improved while the frequency of days with gapped PM2:5 records reduced from 42.6 % to 5.7 %. In general, our DCCEOF method provides a practical yet effective approach to handle data gaps in time series of geophysical parameters with significant diurnal variability, and this method is also transferable to other data sets with similar barriers because of its self-consistent capability.},
   author = {Kaixu Bai and Ke Li and Jianping Guo and Yuanjian Yang and Ni Bin Chang},
   doi = {10.5194/amt-13-1213-2020},
   issn = {18678548},
   issue = {3},
   journal = {Atmospheric Measurement Techniques},
   month = {3},
   pages = {1213-1226},
   publisher = {Copernicus GmbH},
   title = {Filling the gaps of in situ hourly PM2.5 concentration data with the aid of empirical orthogonal function analysis constrained by diurnal cycles},
   volume = {13},
   year = {2020}
}
@article{Sun2012,
   abstract = {This article proposes a simulation-based method to adjust functional boxplots for correlations when visualizing functional and spatio-temporal data, as well as detecting outliers. We start by investigating the relationship between the spatio-temporal dependence and the 1.5 times the 50% central region empirical outlier detection rule. Then, we propose to simulate observations without outliers on the basis of a robust estimator of the covariance function of the data. We select the constant factor in the functional boxplot to control the probability of correctly detecting no outliers. Finally, we apply the selected factor to the functional boxplot of the original data. As applications, the factor selection procedure and the adjusted functional boxplots are demonstrated on sea surface temperatures, spatio-temporal precipitation and general circulation model (GCM) data. The outlier detection performance is also compared before and after the factor adjustment. © 2011 John Wiley & Sons, Ltd.},
   author = {Ying Sun and Marc G. Genton},
   doi = {10.1002/env.1136},
   issn = {11804009},
   issue = {1},
   journal = {Environmetrics},
   keywords = {Functional data,GCM data,Outlier detection,Precipitation data,Robust covariance,Spatio-temporal data},
   month = {2},
   pages = {54-64},
   title = {Adjusted functional boxplots for spatio-temporal data visualization and outlier detection},
   volume = {23},
   year = {2012}
}
@techReport{Berrendero2020,
   abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.},
   author = {José R Berrendero and Beatriz Bueno-Larraz and Antonio Cuevas},
   journal = {Journal of Machine Learning Research},
   keywords = {Functional data,Mahalanobis distance,kernel methods in statistics,reproducing kernel Hilbert spaces,square root operator},
   pages = {1-33},
   title = {On Mahalanobis Distance in Functional Settings},
   volume = {21},
   url = {http://jmlr.org/papers/v21/18-156.html.},
   year = {2020}
}
@book{Bickel2012,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger For},
   isbn = {9781461436553},
   title = {Inference for Functional Data with Applications},
   url = {http://www.springer.com/series/692},
   year = {2012}
}
@article{Siddiqi2024,
   abstract = {This paper addresses the significant problem of identifying the relevant background and contextual literature related to deep learning (DL) as an evolving technology in order to provide a comprehensive analysis of the application of DL to the specific problem of pneumonia detection via chest X-ray (CXR) imaging, which is the most common and cost-effective imaging technique available worldwide for pneumonia diagnosis. This paper in particular addresses the key period associated with COVID-19, 2020–2023, to explain, analyze, and systematically evaluate the limitations of approaches and determine their relative levels of effectiveness. The context in which DL is applied as both an aid to and an automated substitute for existing expert radiography professionals, who often have limited availability, is elaborated in detail. The rationale for the undertaken research is provided, along with a justification of the resources adopted and their relevance. This explanatory text and the subsequent analyses are intended to provide sufficient detail of the problem being addressed, existing solutions, and the limitations of these, ranging in detail from the specific to the more general. Indeed, our analysis and evaluation agree with the generally held view that the use of transformers, specifically, vision transformers (ViTs), is the most promising technique for obtaining further effective results in the area of pneumonia detection using CXR images. However, ViTs require extensive further research to address several limitations, specifically the following: biased CXR datasets, data and code availability, the ease with which a model can be explained, systematic methods of accurate model comparison, the notion of class imbalance in CXR datasets, and the possibility of adversarial attacks, the latter of which remains an area of fundamental research.},
   author = {Raheel Siddiqi and Sameena Javaid},
   doi = {10.3390/jimaging10080176},
   issn = {2313433X},
   issue = {8},
   journal = {Journal of Imaging},
   keywords = {COVID-19,chest X-ray,convolutional neural network,deep learning,pneumonia detection},
   title = {Deep Learning for Pneumonia Detection in Chest X-ray Images: A Comprehensive Survey},
   volume = {10},
   year = {2024}
}
@article{Ortiz-Toro2022,
   abstract = {Fast and accurate diagnosis is critical for the triage and management of pneumonia, particularly in the current scenario of a COVID-19 pandemic, where this pathology is a major symptom of the infection. With the objective of providing tools for that purpose, this study assesses the potential of three textural image characterisation methods: radiomics, fractal dimension and the recently developed superpixel-based histon, as biomarkers to be used for training Artificial Intelligence (AI) models in order to detect pneumonia in chest X-ray images. Models generated from three different AI algorithms have been studied: K-Nearest Neighbors, Support Vector Machine and Random Forest. Two open-access image datasets were used in this study. In the first one, a dataset composed of paediatric chest X-ray, the best performing generated models achieved an 83.3% accuracy with 89% sensitivity for radiomics, 89.9% accuracy with 93.6% sensitivity for fractal dimension and 91.3% accuracy with 90.5% sensitivity for superpixels based histon. Second, a dataset derived from an image repository developed primarily as a tool for studying COVID-19 was used. For this dataset, the best performing generated models resulted in a 95.3% accuracy with 99.2% sensitivity for radiomics, 99% accuracy with 100% sensitivity for fractal dimension and 99% accuracy with 98.6% sensitivity for superpixel-based histons. The results confirm the validity of the tested methods as reliable and easy-to-implement automatic diagnostic tools for pneumonia.},
   author = {César Ortiz-Toro and Angel García-Pedrero and Mario Lillo-Saavedra and Consuelo Gonzalo-Martín},
   doi = {10.1016/j.compbiomed.2022.105466},
   issn = {18790534},
   issue = {January},
   journal = {Computers in Biology and Medicine},
   keywords = {COVID-19,Chest imaging,Diagnostic imaging,Fractal dimension,Histon,Pneumonia,Radiomics,Superpixels,X-ray},
   pmid = {35585732},
   title = {Automatic detection of pneumonia in chest X-ray images using textural features},
   volume = {145},
   year = {2022}
}
@article{Mujahid2024,
   abstract = {Pneumonia is a dangerous disease that kills millions of children and elderly patients worldwide every year. The detection of pneumonia from a chest x-ray is perpetrated by expert radiologists. The chest x-ray is cheaper and is most often used to diagnose pneumonia. However, chest x-ray-based diagnosis requires expert radiologists which is time-consuming and laborious. Moreover, COVID-19 and pneumonia have similar symptoms which leads to false positives. Machine learning-based solutions have been proposed for the automatic prediction of pneumonia from chest X-rays, however, such approaches lack robustness and high accuracy due to data imbalance and generalization errors. This study focuses on elevating the performance of machine learning models by dealing with data imbalanced problems using data augmentation. Contrary to traditional machine learning models that required hand-crafted features, this study uses transfer learning for automatic feature extraction using Xception and VGG-16 to train classifiers like support vector machine, logistic regression, K nearest neighbor, stochastic gradient descent, extra tree classifier, and gradient boosting machine. Experiments involve the use of hand-crafted features, as well as, transfer learning-based feature extraction for pneumonia detection. Performance comparison using Xception and VGG-16 features suggest that transfer learning-based features tend to show better performance than hand-crafted features and an accuracy of 99.23% can be obtained for pneumonia using chest X-rays.},
   author = {Muhammad Mujahid and Furqan Rustam and Prasun Chakrabarti and Bhargav Mallampati and Isabel De La Torre Diez and Pradeep Gali and Venkata Chunduri and Imran Ashraf},
   doi = {10.3233/THC-230313},
   issn = {09287329},
   issue = {6},
   journal = {Technology and Health Care},
   keywords = {COVID-19,Pneumonia prediction,automatic feature extraction,chest radiographs,transfer learning},
   pages = {3847-3870},
   pmid = {39520166},
   title = {Pneumonia detection on chest X-rays from Xception-based transfer learning and logistic regression},
   volume = {32},
   year = {2024}
}
@article{Wang2017,
   abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weaklysupervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.},
   author = {Xiaosong Wang and Yifan Peng and Le Lu and Zhiyong Lu and Mohammadhadi Bagheri and Ronald M. Summers},
   doi = {10.1109/CVPR.2017.369},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   pages = {3462-3471},
   title = {ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
   volume = {2017-Janua},
   year = {2017}
}
@article{Irvin2019,
   abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.},
   author = {Jeremy Irvin and Pranav Rajpurkar and Michael Ko and Yifan Yu and Silviana Ciurea-Ilcus and Chris Chute and Henrik Marklund and Behzad Haghgoo and Robyn Ball and Katie Shpanskaya and Jayne Seekins and David A. Mong and Safwan S. Halabi and Jesse K. Sandberg and Ricky Jones and David B. Larson and Curtis P. Langlotz and Bhavik N. Patel and Matthew P. Lungren and Andrew Y. Ng},
   doi = {10.1609/aaai.v33i01.3301590},
   isbn = {9781577358091},
   issn = {2159-5399},
   journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
   pages = {590-597},
   title = {CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
   year = {2019}
}
@article{Nguyen2022,
   abstract = {Most of the existing chest X-ray datasets include labels from a list of findings without specifying their locations on the radiographs. This limits the development of machine learning algorithms for the detection and localization of chest abnormalities. In this work, we describe a dataset of more than 100,000 chest X-ray scans that were retrospectively collected from two major hospitals in Vietnam. Out of this raw data, we release 18,000 images that were manually annotated by a total of 17 experienced radiologists with 22 local labels of rectangles surrounding abnormalities and 6 global labels of suspected diseases. The released dataset is divided into a training set of 15,000 and a test set of 3,000. Each scan in the training set was independently labeled by 3 radiologists, while each scan in the test set was labeled by the consensus of 5 radiologists. We designed and built a labeling platform for DICOM images to facilitate these annotation procedures. All images are made publicly available in DICOM format along with the labels of both the training set and the test set.},
   author = {Ha Q. Nguyen and Khanh Lam and Linh T. Le and Hieu H. Pham and Dat Q. Tran and Dung B. Nguyen and Dung D. Le and Chi M. Pham and Hang T.T. Tong and Diep H. Dinh and Cuong D. Do and Luu T. Doan and Cuong N. Nguyen and Binh T. Nguyen and Que V. Nguyen and Au D. Hoang and Hien N. Phan and Anh T. Nguyen and Phuong H. Ho and Dat T. Ngo and Nghia T. Nguyen and Nhan T. Nguyen and Minh Dao and Van Vu},
   doi = {10.1038/s41597-022-01498-w},
   isbn = {4159702201},
   issn = {20524463},
   issue = {1},
   journal = {Scientific Data},
   pages = {1-7},
   pmid = {35858929},
   publisher = {Springer US},
   title = {VinDr-CXR: An open dataset of chest X-rays with radiologist’s annotations},
   volume = {9},
   year = {2022}
}
@article{Kermany2018,
   abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract: [Figure presented] Image-based deep learning classifies macular degeneration and diabetic retinopathy using retinal optical coherence tomography images and has potential for generalized applications in biomedical image interpretation and medical decision making.},
   author = {Daniel S. Kermany and Michael Goldbaum and Wenjia Cai and Carolina C.S. Valentim and Huiying Liang and Sally L. Baxter and Alex McKeown and Ge Yang and Xiaokang Wu and Fangbing Yan and Justin Dong and Made K. Prasadha and Jacqueline Pei and Magdalena Ting and Jie Zhu and Christina Li and Sierra Hewett and Jason Dong and Ian Ziyar and Alexander Shi and Runze Zhang and Lianghong Zheng and Rui Hou and William Shi and Xin Fu and Yaou Duan and Viet A.N. Huu and Cindy Wen and Edward D. Zhang and Charlotte L. Zhang and Oulan Li and Xiaobo Wang and Michael A. Singer and Xiaodong Sun and Jie Xu and Ali Tafreshi and M. Anthony Lewis and Huimin Xia and Kang Zhang},
   doi = {10.1016/j.cell.2018.02.010},
   issn = {10974172},
   issue = {5},
   journal = {Cell},
   keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning},
   pages = {1122-1131.e9},
   pmid = {29474911},
   publisher = {Elsevier Inc.},
   title = {Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning},
   volume = {172},
   url = {https://doi.org/10.1016/j.cell.2018.02.010},
   year = {2018}
}
@article{Lynham2024,
   author = {John Lynham},
   doi = {10.1126/science.adn1146},
   issue = {December},
   pages = {1276-1281},
   title = {Evidence of spillover benefits from large-scale marine protected areas to purse seine fisheries},
   volume = {1281},
   year = {2024}
}
@article{Favoretto2023,
   author = {Fabio Favoretto and Catalina López-sagástegui and Enric Sala and Octavio Aburto-oropeza},
   doi = {10.1126/sciadv.adg0709},
   pages = {1-6},
   title = {The largest fully protected marine area in North America does not harm industrial fishing},
   volume = {0709},
   year = {2023}
}
@article{Ferraro2019,
   author = {Paul J Ferraro and James N Sanchirico and Martin D Smith},
   doi = {10.1073/pnas.1805563115},
   issue = {12},
   pages = {5311-5318},
   title = {Causal inference in coupled human and natural systems},
   volume = {116},
   year = {2019}
}
@article{Gertheiss2024,
   author = {Jan Gertheiss and David Rügamer and Bernard X W Liew and Sonja Greven},
   doi = {10.1002/bimj.202300363},
   keywords = {curve data,functional regression,image data,longitudinal data analysis,object-oriented data analysis},
   title = {Functional Data Analysis : An Introduction and Recent Developments},
   year = {2024}
}
@article{Kerr2019,
   author = {Lisa A Kerr and Jacob P Kritzer and Steven X Cadrin},
   doi = {10.1093/icesjms/fsz014},
   keywords = {baci,closed areas,haddock,marine protected areas,yellowtail flounder},
   pages = {1039-1051},
   title = {Original Article Strengths and limitations of before – after – control – impact analysis for testing the effects of marine protected areas on managed populations},
   volume = {76},
   year = {2019}
}
@article{Wang2015,
   author = {Jane-ling Wang and Jeng-min Chiou},
   doi = {10.1146/))},
   keywords = {classification,clustering and,functional additive model,functional correlation,functional linear regression,functional principal component analysis,time warping},
   pages = {1-41},
   title = {Review of functional data analysis},
   year = {2015}
}
@article{Pronello2023,
   abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
   author = {Nicola Pronello and Rosaria Ignaccolo and Luigi Ippoliti and Sara Fontanella},
   doi = {10.1007/s11222-023-10288-2},
   issn = {15731375},
   issue = {6},
   journal = {Statistics and Computing},
   keywords = {Functional zoning,Manifold data,Mixture models,Shape analysis,Spatial clustering,Surface data},
   month = {12},
   publisher = {Springer},
   title = {Penalized model-based clustering of complex functional data},
   volume = {33},
   year = {2023}
}
@misc{Law2008,
   abstract = {Large-scale (>40 000 km2, >1 yr) ocean iron fertilization (OIF) is being considered as an option for mitigating the increase in atmospheric CO2 concentrations. However OIF will influence trace gas production and atmospheric emissions, with consequences over broad temporal and spatial scales. To illustrate this, the response of nitrous oxide (N 2O) and dimethylsulphide (DMS) in the mesoscale iron addition experiments (FeAXs) and model scenarios of large-scale OIF are examined. FeAXs have shown negligible to minor increases in N2O production, whereas models of long-term OIF suggest significant N2O production with the potential to offset the benefit gained by iron-mediated increases in CO 2 uptake. N2O production and emission will be influenced by the magnitude and rate of vertical particle export, and along-isopycnal N2O transport will necessitate monitoring over large spatial scales. The N2O-O2 relationship provides a monitoring option using oxygen as a proxy, with spatial coverage by Argo and glider-mounted oxygen optodes. Although the initial FeAXs exhibited similar increases (1.5- to 1.6-fold) in DMS, a subsequent sub-arctic Pacific experiment observed DMS consumption relative to unfertilized waters, highlighting regional variability as a complicating factor when predicting the effects of large-scale OIF. DMS cycling and its influence on atmospheric composition may be studied using naturally occurring blooms and be constrained prior to OIF by pre-fertilization spatial mapping and aerial sampling using new technologies. As trace gases may have positive or negative synergistic effects on atmospheric chemistry and climate forcing, the net effect of altered trace gas emissions needs to be considered in both models and monitoring of large-scale OIF. © Inter-Research 2008.},
   author = {C. S. Law},
   doi = {10.3354/meps07549},
   issn = {01718630},
   journal = {Marine Ecology Progress Series},
   keywords = {Dimethlysulphide,Iron fertilization,Nitrous oxide,Remineralization,Trace gases},
   month = {7},
   pages = {283-288},
   title = {Predicting and monitoring the effects of large-scale ocean iron fertilization on marine trace gas emissions},
   volume = {364},
   year = {2008}
}
@techReport{Sikder,
   author = {Nazmul Kabir Sikder and Feras A Batarseh},
   title = {Outlier Detection using AI: A Survey}
}
@article{Jimenez-Varon2024,
   abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
   author = {Cristian F. Jiménez-Varón and Fouzi Harrou and Ying Sun},
   doi = {10.1002/env.2851},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {data depth,functional data,magnitude outliers,pairwise depth,pointwise depth,shape outliers,visualization},
   month = {8},
   publisher = {John Wiley and Sons Ltd},
   title = {Pointwise data depth for univariate and multivariate functional outlier detection},
   volume = {35},
   year = {2024}
}
@article{Chiou2014a,
   abstract = {We propose an extended version of the classical Karhunen-Lòeve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
   author = {Jeng Min Chiou and Yu Ting Chen and Ya Fang Yang},
   doi = {10.5705/ss.2013.305},
   issn = {10170405},
   issue = {4},
   journal = {Statistica Sinica},
   keywords = {Karhunen-LòEve expansion,Mercer's theorem,Multivariate functional data,Normalization,Traffic flow},
   month = {10},
   pages = {1571-1596},
   publisher = {Institute of Statistical Science},
   title = {Multivariate functional principal component analysis: A normalization approach},
   volume = {24},
   year = {2014}
}
@article{Rigueira2025,
   abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
   author = {Xurxo Rigueira and David Olivieri and Maria Araujo and Angeles Saavedra and Maria Pazo},
   doi = {10.5281/zenodo.1},
   title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
   url = {https://doi.org/10.5281/zenodo.1},
   year = {2025}
}
@book{Ferraty2006,
   author = {Frederic Frédéric Ferraty and Philippe Vieu},
   city = {Berlin},
   doi = {10.1007/0-387-36620-2},
   isbn = {978-0-387-30369-7},
   publisher = {Springer},
   title = {Nonparametric Functional Data Analysis},
   year = {2006}
}
@techReport{GrantIngram1983,
   author = {R Grant Ingram},
   journal = {Estuarine, Coastal and Shelf Science},
   keywords = {estuaries,internal tides,mixings,upwelling},
   pages = {333-338},
   title = {Notes and Discussions},
   volume = {16},
   year = {1983}
}
@book{,
   abstract = {Title from content provider. },
   isbn = {1424401151},
   publisher = {[publisher not identified]},
   title = {OCEANS 2006},
   year = {2006}
}
@article{Xia2011,
   abstract = {Environmental fluid dynamic code (EFDC), a numerical estuarine and coastal ocean circulation hydrodynamic model, was used to simulate the distribution of the salinity, temperature, nutrients, and dissolved oxygen (DO) in Perdido Bay and adjacent Gulf of Mexico. External forcing factors included the coupled effects of the astronomical tides, river discharge, and atmospheric winds on the spatial and temporal distributions of salinity and DO. Modeled time series were in good agreement with field observations of water level, nutrients, temperature, salinity, and DO. Perdido Bay and adjacent northern Gulf of Mexico coasts can be divided into two areas according to salinity, water level, and DO concentrations. The first area was lower Perdido Bay and the associated Gulf of Mexico coasts, acting primarily under the influence of tidal forcing, which increases the vertical stratification. The second division was upper Perdido Bay, which was influenced by both tidal forcing and freshwater inflow. Simulations also indicated winds influenced the salinity and DO distributions, with an enhanced surface pressure gradient. Tidal effects were also important for conducting salinity and water quality simulations in Perdido Bay. Low amplitude tides induced relatively weak vertical mixing and favored the establishment of stratification at the bay, especially along deeper bathymetry. Flood tides influenced the distribution of salinity and DO more than ebb tides, specifically along shallow bathymetry. © Coastal Education & Research Foundation 2011.},
   author = {Meng Xia and Paul M. Craig and Christopher M. Wallen and Andrew Stoddard and Jan Mandrup-Poulsen and Machuan Peng and Blake Schaeffer and Zhijun Liu},
   doi = {10.2112/JCOASTRES-D-09-00044.1},
   issn = {07490208},
   issue = {1},
   journal = {Journal of Coastal Research},
   keywords = {EFDC,Perdido Bay,dissolved oxygen,plume,salinity},
   month = {12},
   pages = {73-86},
   title = {Numerical simulation of salinity and dissolved oxygen at Perdido Bay and adjacent coastal ocean},
   volume = {27},
   year = {2011}
}
@misc{Rudnick2016,
   abstract = {Underwater gliders are autonomous underwater vehicles that profile vertically by changing their buoyancy and use wings to move horizontally. Gliders are useful for sustained observation at relatively fine horizontal scales, especially to connect the coastal and open ocean. In this review, research topics are grouped by time and length scales. Large-scale topics addressed include the eastern and western boundary currents and the regional effects of climate variability. The accessibility of horizontal length scales of order 1 km allows investigation of mesoscale and submesoscale features such as fronts and eddies. Because the submesoscales dominate vertical fluxes in the ocean, gliders have found application in studies of biogeochemical processes. At the finest scales, gliders have been used to measure internal waves and turbulent dissipation. The review summarizes gliders' achievements to date and assesses their future in ocean observation.},
   author = {Daniel L. Rudnick},
   doi = {10.1146/annurev-marine-122414-033913},
   issn = {19410611},
   journal = {Annual Review of Marine Science},
   keywords = {Autonomous underwater vehicles,Biogeochemistry,Climate,Internal waves,Mesoscale,Sustained observations},
   month = {1},
   pages = {519-541},
   pmid = {26291384},
   publisher = {Annual Reviews Inc.},
   title = {Ocean Research Enabled by Underwater Gliders},
   volume = {8},
   year = {2016}
}
@inproceedings{Li2023,
   abstract = {Ocean and climate research benefits from global ocean observation initiatives such as Argo, GLOSS, and EMSO. The Argo network, dedicated to ocean profiling, generates a vast volume of observatory data. However, data quality issues from sensor malfunctions and transmission errors necessitate stringent quality assessment. Existing methods, including machine learning, fall short due to limited labeled data and imbalanced datasets. To address these challenges, we propose an Outlier Detection-Enhanced Active Learning (ODEAL) framework for ocean data quality assessment, employing Active Learning (AL) to reduce human experts' workload in the quality assessment workflow and leveraging outlier detection algorithms for effective model initialization. We also conduct extensive experiments on five large-scale realistic Argo datasets to gain insights into our proposed method, including the effectiveness of AL query strategies and the initial set construction approach. The results suggest that our framework enhances quality assessment efficiency by up to 465.5% with the uncertainty-based query strategy compared to random sampling and minimizes overall annotation costs by up to 76.9% using the initial set built with outlier detectors.},
   author = {Na Li and Yiyang Qi and Ruyue Xin and Zhiming Zhao},
   doi = {10.1109/BigData59044.2023.10386969},
   isbn = {9798350324457},
   booktitle = {Proceedings - 2023 IEEE International Conference on Big Data, BigData 2023},
   keywords = {Argo,active learning,initial set construction,machine learning,ocean data quality control},
   pages = {102-107},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Ocean Data Quality Assessment through Outlier Detection-enhanced Active Learning},
   year = {2023}
}
@article{Todorov2024,
   abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
   author = {Valentin Todorov},
   doi = {10.1002/wics.70007},
   issn = {19390068},
   issue = {6},
   journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
   keywords = {R,high dimensions,multivariate,outlier,robust},
   month = {11},
   publisher = {John Wiley and Sons Inc},
   title = {The R Package Ecosystem for Robust Statistics},
   volume = {16},
   year = {2024}
}
@inproceedings{Wallace2024,
   abstract = {Ocean deoxygenation and expansion and intensification of hypoxia is a growing threat to marine biodiversity worldwide, especially in coastal waters. The causes include eutrophication due to excess input of nutrients. However, increasingly, warming and especially circulation- and mixing-related changes to oxygen supply connected with climate change are driving oxygen concentrations downwards. This is now a major problem on the Canadian east coast with growing hypoxic zones in the Lower St. Lawrence Estuary, as well as threats to shelf waters of Nova Scotia and coastal basins, including Bedford Basin. Current measures to protect marine biodiversity (e.g. marine protected areas) are ineffectual in addressing this threat. However, the sudden emergence of a green hydrogen industry offers a possible solution. Here we highlight the threat to eastern Canadian ecosystems, including commercial fish stocks, from hypoxia as well as the underlying causes and the potential for a solution. In the case of the Gulf of St. Lawrence, planned construction of a hydrogen plant near Stephenville, NL, holds potential to mitigate the growing threat through use of the oxygen by-product of hydrogen generation. Based on measurements and tracer studies, the location and magnitude of production appear to be 'just right' to compensate for current oxygen losses. This mitigation approach (direct oxygen injection) has rarely been considered for marine environments to-date and involves magnitudes and timescales of oxygen transport and response that have not been attempted before. Nevertheless, it seems essential and urgent for ocean scientists, engineers and policymakers to work together to address this threat and explore its potential solution. A coordinated research effort into direct oxygen injection, involving broad multidisciplinary expertise should be established immediately. Field scale pilot studies in well-characterized, accessible locations appear to be the next key step required, as well as planning for larger scale deployment. There are several reasons why Bedford Basin, Nova Scotia is an ideal location for pilot studies and demonstrations. The questions that need to be addressed, the suitability of Bedford Basin, pilot study design and practicality issues are presented and discussed.},
   author = {Douglas Wallace and David Austin},
   doi = {10.1109/OCEANS55160.2024.10754451},
   isbn = {9798331540081},
   issn = {01977385},
   booktitle = {Oceans Conference Record (IEEE)},
   keywords = {Bedford Basin,Gulf of St. Lawrence,deoxygenation,direct oxygen injection,ecosystem restoration,green hydrogen,hypoxia,marine ecosystem,reoxygenation},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {The threat of coastal hypoxia in eastern Canadian waters - New opportunities for its mitigation and the potential of Bedford Basin (Nova Scotia) for required pilot study and demonstration},
   year = {2024}
}
@article{Li2019,
   abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
   author = {Yongmou Li and Yijie Wang and Xingkong Ma},
   doi = {10.3233/IDA-184240},
   issn = {15714128},
   issue = {5},
   journal = {Intelligent Data Analysis},
   keywords = {Variational autoencoders,high-dimensional data,outlier detection},
   pages = {991-1002},
   publisher = {IOS Press},
   title = {Variational autoencoder-based outlier detection for high-dimensional data},
   volume = {23},
   year = {2019}
}
@techReport{Graler,
   abstract = {The spcopula R package provides tools to model spatial and spatio-temporal phenomena with spatial and spatio-temporal vine copulas. Copulas allow us to flexibly build multivariate distributions with mixed margins where the copula describes the multivari-ate dependence structure coupling the margins. In classical geostatistics, a multivariate Gaussian distribution is typically assumed and dependence is summarized in a covari-ance matrix implying limitations like elliptical symmetry in the strength of dependence. Copulas allow for dependence structures beyond the Gaussian one, being for instance asymmetric. We developed the spatio-temporal vine copulas such that the bivariate cop-ula families in the lower trees may change with distance across space and time allowing not only for a varying strength of dependence but also for a changing dependence structure. These spatio-temporal distributions are used to predict values at unobserved locations, assess risk, or run simulations. Based on the concept of vine copulas, the spcopula package provides a large set of multivariate distributions. As bivariate spatial copulas do not have any probabilistic restrictions, the spatial vine copula is a powerful approach for modelling skewed or heavy tailed data with complex and potentially asymmetric dependence structures in the spatial and spatio-temporal domain.},
   author = {Benedikt Gräler},
   keywords = {interpolation,multivariate distributions,spatial data,spatial modelling},
   title = {spcopula: Modelling Spatial and Spatio-Temporal Dependence with Copulas in R}
}
@article{Arribas-Gil2014,
   abstract = {We propose a new method to visualize and detect shape outliers in samples of curves. In functional data analysis, we observe curves defined over a given real interval and shape outliers may be defined as those curves that exhibit a different shape from the rest of the sample. Whereas magnitude outliers, that is, curves that lie outside the range of the majority of the data, are in general easy to identify, shape outliers are often masked among the rest of the curves and thus difficult to detect. In this article, we exploit the relationship between two measures of depth for functional data to help to visualize curves in terms of shape and to develop an algorithm for shape outlier detection. We illustrate the use of the visualization tool, the outliergram, through several examples and analyze the performance of the algorithm on a simulation study. Finally, we apply our method to assess cluster quality in a real set of time course microarray data.},
   author = {Ana Arribas-Gil and Juan Romo},
   doi = {10.1093/biostatistics/kxu006},
   issn = {14684357},
   issue = {4},
   journal = {Biostatistics},
   keywords = {Depth for functional data,Outlier visualization,Robust estimation,Time course microarray data},
   month = {7},
   pages = {603-619},
   pmid = {24622037},
   publisher = {Oxford University Press},
   title = {Shape outlier detection and visualization for functional data: The outliergram},
   volume = {15},
   year = {2014}
}
@article{Berrendero2011,
   abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
   author = {J. R. Berrendero and A. Justel and M. Svarc},
   doi = {10.1016/j.csda.2011.03.011},
   issn = {01679473},
   issue = {9},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Dimension reduction,Eigenvalue functions,Explained variability},
   month = {9},
   pages = {2619-2634},
   title = {Principal components for multivariate functional data},
   volume = {55},
   year = {2011}
}
@article{,
   title = {fundy_20190604_99_delayed_corrected_v4}
}
@article{Pizarro2016,
   author = {Oscar Pizarro and Nadin Ramirez and Manuel I. Castillo and Ursula Cifuentes and Winston Rojas and Matias Pizarro-Koch},
   doi = {10.1175/BAMS-D-14-00040.1},
   issn = {00030007},
   issue = {10},
   journal = {Bulletin of the American Meteorological Society},
   month = {10},
   pages = {1783-1789},
   publisher = {American Meteorological Society},
   title = {Underwater glider observations in the oxygen minimum zone off central Chile},
   volume = {97},
   year = {2016}
}
@article{Dietzel2024,
   abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
   author = {Andreas Dietzel and Marco Moretti and Lauren M. Cook},
   doi = {10.1016/j.ecoinf.2024.102561},
   issn = {15749541},
   journal = {Ecological Informatics},
   keywords = {Bayesian projection predictive variable selection,Blue-green infrastructure,Nature-based solutions,Shrinkage prior,Species distribution model,Urban biodiversity},
   month = {7},
   publisher = {Elsevier B.V.},
   title = {Shrinkage-based Bayesian variable selection for species distribution modelling in complex environments: An application to urban biodiversity},
   volume = {81},
   year = {2024}
}
@inproceedings{Zhang2022,
   abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
   author = {Sean Zhang and Varun Ursekar and Leman Akoglu},
   doi = {10.1145/3534678.3539076},
   isbn = {9781450393850},
   booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Apache Spark,data-parallel algorithms,distributed outlier detection},
   month = {8},
   pages = {4530-4540},
   publisher = {Association for Computing Machinery},
   title = {Sparx: Distributed Outlier Detection at Scale},
   year = {2022}
}
@misc{Mirzaie2023,
   abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
   author = {Mostafa Mirzaie and Behshid Behkamal and Mohammad Allahbakhsh and Samad Paydar and Elisa Bertino},
   doi = {10.1016/j.cosrev.2023.100554},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data quality,Data streams,Quality framework,Systematic literature review},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {State of the art on quality control for data streams: A systematic literature review},
   volume = {48},
   year = {2023}
}
@article{Sekulic2020,
   abstract = {High resolution gridded mean daily temperature datasets are valuable for research and applications in agronomy, meteorology, hydrology, ecology, and many other disciplines depending on weather or climate. The gridded datasets and the models used for their estimation are being constantly improved as there is always a need for more accurate datasets as well as for datasets with a higher spatial and temporal resolution. We developed a spatio-temporal regression kriging model for Croatia at 1 km spatial resolution by adapting the spatio-temporal regression kriging model developed for global land areas. A geometrical temperature trend, digital elevation model, and topographic wetness index were used as covariates together with measurements from the Croatian national meteorological network for the year 2008. This model performed better than the global model and previously developed models for Croatia, based on MODIS land surface temperature images. The R2 was 97.8% and RMSE was 1.2 °C for leave-one-out and 5-fold cross-validation. The proposed national model still has a high level of uncertainty at higher altitudes leaving it suitable for agricultural areas that are dominant in lower and medium altitudes.},
   author = {Aleksandar Sekulić and Milan Kilibarda and Dragutin Protić and Melita Perčec Tadić and Branislav Bajat},
   doi = {10.1007/s00704-019-03077-3},
   issn = {14344483},
   issue = {1-2},
   journal = {Theoretical and Applied Climatology},
   keywords = {Gridded data,Mean daily temperature,R meteo package,Spatio-temporal regression kriging},
   month = {4},
   pages = {101-114},
   publisher = {Springer},
   title = {Spatio-temporal regression kriging model of mean daily temperature for Croatia},
   volume = {140},
   year = {2020}
}
@article{Zaba2016,
   abstract = {Large-scale patterns of positive temperature anomalies persisted throughout the surface waters of the North Pacific Ocean during 2014-2015. In the Southern California Current System, measurements by our sustained network of underwater gliders reveal the coastal effects of the recent warming. Regional upper ocean temperature anomalies were greatest since the initiation of the glider network in 2006. Additional observed physical anomalies included a depressed thermocline, high stratification, and freshening; induced biological consequences included changes in the vertical distribution of chlorophyll fluorescence. Contemporaneous surface heat flux and wind strength perturbations suggest that local anomalous atmospheric forcing caused the unusual oceanic conditions.},
   author = {Katherine D. Zaba and Daniel L. Rudnick},
   doi = {10.1002/2015GL067550},
   issn = {19448007},
   issue = {3},
   journal = {Geophysical Research Letters},
   keywords = {California Current System,Spray gliders,downwelling anomaly,warm SST},
   month = {2},
   pages = {1241-1248},
   publisher = {Blackwell Publishing Ltd},
   title = {The 2014-2015 warming anomaly in the Southern California Current System observed by underwater gliders},
   volume = {43},
   year = {2016}
}
@article{Fonvieille2023,
   abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
   author = {Nadège Fonvieille and Christophe Guinet and Martin Saraceno and Baptiste Picard and Martin Tournier and Pauline Goulet and Claudio Campagna and Julieta Campagna and David Nerini},
   doi = {10.1016/j.pocean.2023.103120},
   issn = {00796611},
   journal = {Progress in Oceanography},
   keywords = {Brazil-Malvinas confluence,Functional Data Analysis,Habitat use,Model-based clustering,Southern elephant seals},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {Swimming in an ocean of curves: A functional approach to understanding elephant seal habitat use in the Argentine Basin},
   volume = {218},
   year = {2023}
}
@article{,
   abstract = {Titre: Donnees spatio-temporelles avec R : tout ce que vous avez toujours voulu savoir sans jamais avoir osé le demander RESSTE Network et al. 1,2 Abstract: We present an overview of (geo-)statistical models, methods and techniques for the analysis and prediction of continuous spatio-temporal processes residing in continuous space. Various approaches exist for building statistical models for such processes, estimating their parameters and performing predictions. We cover the Gaussian process approach, very common in spatial statistics and geostatistics, and we focus on R-based implementations of numerical procedures. To illustrate and compare the use of some of the most relevant packages, we treat a real-world application with high-dimensional data. The target variable is the daily mean PM 10 concentration predicted thanks to a chemistry-transport model and observation series collected at monitoring stations across France in 2014. We give R code covering the full work-flow from importing data sets to the prediction of PM 10 concentrations with a fitted parametric model, including the visualization of data, estimation of the parameters of the spatio-temporal covariance function and model selection. We conclude with some elements of comparison between the packages that are available today and some discussion for future developments. Résumé : Nous présentons un aperçu des modèles, méthodes et techniques (géo-)statistiques pour l'analyse et la prévision de processus spatio-temporels continus. De nombreuses approches sont possibles pour la construction de modèles statistiques pour ces processus, l'estimation de leurs paramètres et leur prédiction. Nous avons choisi de présenter l'approche par processus gaussien, la plus communément utilisée en statistiques spatiales et en géostatistiques, ainsi que son implémentation avec le logiciel R. La variable cible est la moyenne de la concentration quotidienne PM 10 à l'échelle de la France, prédite à l'aide d'un modèle de transport en chimie de l'atmosphère et de séries d'observations obtenues à des stations de surveillance de la qualité de l'air. En suivant le fil d'une application réelle de grande dimension, nous comparons certains des paquets R les plus utilisés. Le code R permettant la visualisation des données, l'estimation des paramètres de la fonction de covariance spatio-temporelle ainsi que la sélection d'un modèle et la prédiction de la concentration de PM 10 est également présenté afin d'illustrer l'enchaînement des étapes. Nous concluons avec une comparaison entre les paquets qui sont disponibles aujourd'hui et ainsi que les pistes de développement qui nous paraissent intéressantes.},
   issn = {2102-6238},
   keywords = {62-07,62F99,62M40,Air pollution Mots-clés : Fonction de covariance,Covariance function,Geostatistics,Géostatistique,Krigeage,Kriging,Pollution atmosphérique AMS 2000 subject classific,Space-time},
   title = {Analyzing spatio-temporal data with R: Everything you always wanted to know-but were afraid to ask},
   url = {http://informatique-mia.inra.fr/resste/http://www.sfds.asso.fr/journal}
}
@article{Tang2019,
   abstract = {The joint analysis of spatial and temporal processes poses computational challenges due to the data's high dimensionality. Furthermore, such data are commonly non-Gaussian. In this paper, we introduce a copula-based spatiotemporal model for analyzing spatiotemporal data and propose a semiparametric estimator. The proposed algorithm is computationally simple, since it models the marginal distribution and the spatiotemporal dependence separately. Instead of assuming a parametric distribution, the proposed method models the marginal distributions nonparametrically and thus offers more flexibility. The method also provides a convenient way to construct both point and interval predictions at new times and locations, based on the estimated conditional quantiles. Through a simulation study and an analysis of wind speeds observed along the border between Oregon and Washington, we show that our method produces more accurate point and interval predictions for skewed data than those based on normality assumptions.},
   author = {Yanlin Tang and Huixia J. Wang and Ying Sun and Amanda S. Hering},
   doi = {10.1111/biom.13066},
   issn = {15410420},
   issue = {4},
   journal = {Biometrics},
   keywords = {Markov process,copula,pseudo-likelihood,spatiotemporal},
   month = {12},
   pages = {1156-1167},
   pmid = {31009058},
   publisher = {John Wiley and Sons Inc},
   title = {Copula-based semiparametric models for spatiotemporal data},
   volume = {75},
   year = {2019}
}
@article{Diamant2020,
   abstract = {Measuring and forecasting changes in coastal and deep-water ecosystems and climates requires sustained long-term measurements from marine observation systems. One of the key considerations in analyzing data from marine observatories is quality assurance (QA). The data acquired by these infrastructures accumulates into Giga and Terabytes per year, necessitating an accurate automatic identification of false samples. A particular challenge in the QA of oceanographic datasets is the avoidance of disqualification of data samples that, while appearing as outliers, actually represent real short-term phenomena, that are of importance. In this paper, we present a novel cross-sensor QA approach that validates the disqualification decision of a data sample from an examined dataset by comparing it to samples from related datasets. This group of related datasets is chosen so as to reflect upon the same oceanographic phenomena that enable some prediction of the examined dataset. In our approach, a disqualification is validated if the detected anomaly is present only in the examined dataset, but not in its related datasets. Results for a surface water temperature dataset recorded by our Texas A&M—Haifa Eastern Mediterranean Marine Observatory (THEMO)—over a period of 7 months, show an improved trade-off between accurate and false disqualification rates when compared to two standard benchmark schemes.},
   author = {Roee Diamant and Ilan Shachar and Yizhaq Makovsky and Bruno Miguel Ferreira and Nuno Alexandre Cruz},
   doi = {10.3390/rs12213470},
   issn = {20724292},
   issue = {21},
   journal = {Remote Sensing},
   keywords = {Change detector,Data validation,Ocean observatories,Ocean remote sensing,Prediction of data,Quality assurance,Quality control,Regression},
   month = {11},
   pages = {1-16},
   publisher = {MDPI AG},
   title = {Cross-sensor quality assurance for marine observatories},
   volume = {12},
   year = {2020}
}
@article{Vanem2014,
   abstract = {Bad weather and rough seas continue to be a major cause for ship losses and is thus a significant contributor to the risk to maritime transportation. This stresses the importance of taking severe sea state conditions adequately into account, with due treatment of the uncertainties involved, in ship design and operation in order to enhance safety. Hence, there is a need for appropriate stochastic models describing the variability of sea states. These should also incorporate realistic projections of future return levels of extreme sea states, taking into account long-term trends related to climate change and inherent uncertainties. The stochastic ocean wave model presented in this paper exploits the flexible framework of Bayesian hierarchical space-time models. It allows modelling of complex dependence structures in space and time and incorporation of physical features and prior knowledge, yet at the same time remains intuitive and easily interpreted. Furthermore, by taking a Bayesian approach, the uncertainties of the model parameters are also taken into account. A regression component with CO2 as an explanatory variable has been introduced in order to extract long-term trends in the data. The model has been fitted by monthly maximum significant wave height data for an area in the North Atlantic ocean. The different components of the model will be outlined in the paper, and the results will be discussed. Furthermore, a discussion of possible extensions to the model will be given. © 2013 Springer Science+Business Media New York.},
   author = {Erik Vanem and Arne Bang Huseby and Bent Natvig},
   doi = {10.1007/s10651-013-0251-6},
   issn = {13528505},
   issue = {2},
   journal = {Environmental and Ecological Statistics},
   keywords = {Bayesian hierarchical modelling,Climate change,MCMC,Modelling the effects of climate change,Ocean waves,Spatio-temporal modelling,Stochastic processes},
   pages = {189-220},
   publisher = {Kluwer Academic Publishers},
   title = {Bayesian hierarchical spatio-temporal modelling of trends and future projections in the ocean wave climate with a CO2 regression component},
   volume = {21},
   year = {2014}
}
@techReport{Sahu,
   author = {Sujit K Sahu},
   keywords = {bayesian,modelling,spatio-temporal},
   title = {Bayesian Modeling of Spatio-Temporal Data with R},
   url = {https://www.crcpress.com/Chapman--}
}
@techReport{Gralera,
   abstract = {Copulas are a statistical concept which allows for a novel approach to model dependencies of spatial and spatio-temporal variables. They capture the dependence structure of a multivariate distribution over its whole range detached from its specific margins. In contrast to a single measure of association, this allows for varying strength of dependence throughout the multivariate distribution. Thus, copulas are capable of capturing many different (i.e. non-Gaussian) dependence structures and allow for asymmet-ric dependencies which can be found in many natural processes. We applied this approach to data from the deforestation survey of the Brazilian Amazon in order to capture the dependence of deforestation on a selection of variables .},
   author = {Benedikt Gräler and Hannes Kazianka and Giovana Mira De Espindola},
   title = {Copulas, a novel approach to model spatial and spatio-temporal dependence}
}
@article{Pigoli2014,
   abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
   author = {Davide Pigoli and John A.D. Aston and Ian L. Dryden and Piercesare Secchi},
   doi = {10.1093/biomet/asu008},
   issn = {14643510},
   issue = {2},
   journal = {Biometrika},
   keywords = {Distance metric,Functional data analysis,Procrustes analysis,Shape analysis},
   pages = {409-422},
   publisher = {Oxford University Press},
   title = {Distances and inference for covariance operators},
   volume = {101},
   year = {2014}
}
@techReport{Du,
   abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
   author = {Xuefeng Du and Yiyou Sun and Xiaojin Zhu and Yixuan Li},
   title = {Dream the Impossible: Outlier Imagination with Diffusion Models},
   url = {https://github.com/deeplearning-wisc/dream-ood.}
}
@techReport{Salehi,
   abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
   author = {Mohammadreza Salehi and Salehidehnavi@uva Nl and Hossein Mirzaei and Dan Hendrycks and Yixuan Li and Mohammad Hossein Rohban and Mohammad Sabokrou},
   title = {A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges},
   url = {https://github.com/taslimisina/osr-ood-ad-methods}
}
@article{Ruff2020,
   abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
   author = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Grégoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus-Robert Müller},
   doi = {10.1109/JPROC.2021.3052449},
   month = {9},
   title = {A Unifying Review of Deep and Shallow Anomaly Detection},
   url = {http://arxiv.org/abs/2009.11732 http://dx.doi.org/10.1109/JPROC.2021.3052449},
   year = {2020}
}
@article{Zheng2022,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@article{Ruff2021,
   abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
   author = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Gregoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus Robert Muller},
   doi = {10.1109/JPROC.2021.3052449},
   issn = {15582256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   keywords = {Anomaly detection (AD),deep learning,explainable artificial intelligence,interpretability,kernel methods,neural networks,novelty detection,one-class classification,out-of-distribution (OOD) detection,outlier detection,unsupervised learning.},
   month = {5},
   pages = {756-795},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Unifying Review of Deep and Shallow Anomaly Detection},
   volume = {109},
   year = {2021}
}
@misc{Souiden2022,
   abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
   author = {Imen Souiden and Mohamed Nazih Omri and Zaki Brahmi},
   doi = {10.1016/j.cosrev.2022.100463},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data streams,High dimensional data,Outlier detection},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {A survey of outlier detection in high dimensional data streams},
   volume = {44},
   year = {2022}
}
@article{Herrmann2023,
   abstract = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption, that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under: Technologies > Structure Discovery and Clustering Fundamental Concepts of Data and Knowledge > Data Concepts Technologies > Visualization.},
   author = {Moritz Herrmann and Florian Pfisterer and Fabian Scheipl},
   doi = {10.1002/widm.1491},
   issn = {19424795},
   issue = {3},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   keywords = {anomaly detection,dimension reduction,manifold learning,outlier detection},
   month = {5},
   publisher = {John Wiley and Sons Inc},
   title = {A geometric framework for outlier detection in high-dimensional data},
   volume = {13},
   year = {2023}
}
@article{Kuhnt2016,
   abstract = {A measure especially designed for detecting shape outliers in functional data is presented. It is based on the tangential angles of the intersections of the centred data and can be interpreted like a data depth. Due to its theoretical properties we call it functional tangential angle (FUNTA) pseudo-depth. Furthermore we introduce a robustification (rFUNTA). The existence of intersection angles is ensured through the centring. Assuming that shape outliers in functional data follow a different pattern, the distribution of intersection angles differs. Furthermore we formulate a population version of FUNTA in the context of Gaussian processes. We determine sample breakdown points of FUNTA and compare its performance with respect to outlier detection in simulation studies and a real data example.},
   author = {Sonja Kuhnt and André Rehage},
   doi = {10.1016/j.jmva.2015.10.016},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Bootstrap,Data depth,Functional data,Robust estimate,Shape outlier detection},
   month = {4},
   pages = {325-340},
   publisher = {Elsevier Inc.},
   title = {An angle-based multivariate functional pseudo-depth for shape outlier detection},
   volume = {146},
   url = {http://dx.doi.org/10.1016/j.jmva.2015.10.016},
   year = {2016}
}
@misc{Zimek2012,
   abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
   author = {Arthur Zimek and Erich Schubert and Hans Peter Kriegel},
   doi = {10.1002/sam.11161},
   issn = {19321864},
   issue = {5},
   journal = {Statistical Analysis and Data Mining},
   keywords = {Anomalies in high-dimensional data,Approximate outlier detection,Correlation outlier detection,Curse of dimensionality,Outlier detection in high-dimensional data,Subspace outlier detection},
   pages = {363-387},
   publisher = {John Wiley and Sons Inc},
   title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
   volume = {5},
   year = {2012}
}
@article{Porreca2024,
   abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
   author = {Annamaria Porreca and Fabrizio Maturo},
   doi = {10.1007/s11135-024-01876-z},
   issn = {15737845},
   journal = {Quality and Quantity},
   keywords = {Biodiversity,Diversity,FDA,Functional outlier detection,Hill’s numbers,Normalized Hill’s functions,Standardized Hill’s functions},
   publisher = {Springer Science and Business Media B.V.},
   title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized Hill’s numbers and their integral functions},
   year = {2024}
}
@book{Mateu2022,
   abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
   author = {Jorge. Mateu and Ramon. Giraldo},
   isbn = {9781119387848},
   publisher = {John Wiley \& Sons, Inc.},
   title = {Geostatistical functional data analysis},
   year = {2022}
}
@article{Olteanu2023,
   abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
   author = {Madalina Olteanu and Fabrice Rossi and Florian Yger},
   doi = {10.1016/j.neucom.2023.126634},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Anomaly detection,Meta-survey,Outlier detection},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Meta-survey on outlier and anomaly detection},
   volume = {555},
   year = {2023}
}
@article{Li2024,
   abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
   author = {Jiangwei Li and Jiangwei Li and Chenxu Wang and Fons J. Verbeek and Tanja Schultz and Hui Liu},
   doi = {10.1088/2632-2153/ad2492},
   issn = {26322153},
   issue = {1},
   journal = {Machine Learning: Science and Technology},
   keywords = {clustering,data mining,machine learning,medical data,medoid selection,minimum spanning tree,outlier detection},
   month = {3},
   publisher = {Institute of Physics},
   title = {MS2OD: outlier detection using minimum spanning tree and medoid selection},
   volume = {5},
   year = {2024}
}
@article{Wikle2015,
   abstract = {Spatio-temporal statistical models are increasingly being used across a wide variety of scientific disciplines to describe and predict spatially explicit processes that evolve over time. Although descriptive models that approach this problem from the second-order (covariance) perspective are important, many real-world processes are dynamic, and it can be more efficient in such cases to characterize the associated spatio-temporal dependence by the use of dynamical models. The challenge with the specification of such dynamical models has been related to the curse of dimensionality and the specification of realistic dependence structures. Even in fairly simple linear/Gaussian settings, spatio-temporal statistical models are often over parameterized. This problem is compounded when the spatio-temporal dynamical processes are nonlinear or multivariate. Hierarchical models have proven invaluable in their ability to deal to some extent with this issue by allowing dependency among groups of parameters and science-based parameterizations. Such models are best considered from a Bayesian perspective, with associated computational challenges. Spatio-temporal statistics remains an active and vibrant area of research.},
   author = {Christopher K. Wikle},
   doi = {10.1002/wics.1341},
   issn = {19390068},
   issue = {1},
   journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
   keywords = {Bayesian hierarchical models,Quadratic nonlinearity,Rank reduction,Spatial basis functions,Spatio-temporal dynamic models},
   month = {1},
   pages = {86-98},
   publisher = {Wiley-Blackwell},
   title = {Modern perspectives on statistics for spatio-temporal data},
   volume = {7},
   year = {2015}
}
@book{Ferreira2025,
   abstract = {Several important topics in spatial and spatio-temporal statistics developed in the last 15 years have not received enough attention in textbooks. Modeling Spatio-Temporal Data: Markov Random Fields, Objective Bayes, and Multiscale Models aims to fill this gap by providing an overview of a variety of recently proposed approaches for the analysis of spatial and spatio-temporal datasets, including proper Gaussian Markov random fields, dynamic multiscale spatio-temporal models, and objective priors for spatial and spatio-temporal models. The goal is to make these approaches more accessible to practitioners, and to stimulate additional research in these important areas of spatial and spatio-temporal statistics. Key topics discussed in this book include: • Proper Gaussian Markov random fields and their uses as building blocks for spatio-temporal models and multiscale models. • Hierarchical models with intrinsic conditional autoregressive priors for spatial random effects, including reference priors, results on fast computations, and objective Bayes model selection. • Objective priors for state-space models and a new approximate reference prior for a spatio-temporal model with dynamic spatio-temporal random effects. • Spatio-temporal models based on proper Gaussian Markov random fields for Poisson observations. • Dynamic multiscale spatio-temporal thresholding for spatial clustering and data compression. • Multiscale spatio-temporal assimilation of computer model output and monitoring station data. • Dynamic multiscale heteroscedastic multivariate spatio-temporal models. • The M-open multiple optima paradox and some of its practical implications for multiscale modeling. • Ensembles of dynamic multiscale spatio-temporal models for smooth spatio-temporal processes. The audience for this book are practitioners, researchers, and graduate students in statistics, data science, machine learning, and related fields. Prerequisites for this book are master's-level courses on statistical inference, linear models, and Bayesian statistics. This book can be used as a textbook for a special topics course on spatial and spatio-temporal statistics, as well as supplementary material for graduate courses on spatial and spatio-temporal modeling.},
   author = {Marco A.R. Ferreira},
   publisher = {Taylor and Francis},
   title = {Modeling Spatio-Temporal Data},
   year = {2025}
}
@article{Happ2018,
   abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen–Loève Theorem. For the practically relevant case of a finite Karhunen–Loève representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
   author = {Clara Happ and Sonja Greven},
   doi = {10.1080/01621459.2016.1273115},
   issn = {1537274X},
   issue = {522},
   journal = {Journal of the American Statistical Association},
   keywords = {Dimension reduction,Functional data analysis,Image analysis,Multivariate functional data},
   month = {4},
   pages = {649-659},
   publisher = {American Statistical Association},
   title = {Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains},
   volume = {113},
   year = {2018}
}
@article{Maturo2024,
   abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
   author = {Fabrizio Maturo and Annamaria Porreca},
   doi = {10.1007/s13253-024-00648-4},
   issn = {15372693},
   journal = {Journal of Agricultural, Biological, and Environmental Statistics},
   keywords = {Biodiversity,Diversity,FDA,Functional outlier detection,Hill’s numbers},
   publisher = {Springer},
   title = {Environmental Loss Assessment via Functional Outlier Detection of Transformed Biodiversity Profiles},
   year = {2024}
}
@article{Wikle2013,
   abstract = {Processes in ocean physics, air-sea interaction and ocean biogeochemistry span enormous ranges in spatial and temporal scales, that is, from molecular to planetary and from seconds to millennia. Identifying and implementing sustainable human practices depend critically on our understandings of key aspects of ocean physics and ecology within these scale ranges. The set of all ocean data is distorted such that three- and four-dimensional (i.e., timedependent) in situ data are very sparse, while observations of surface and upper ocean properties from space-borne platforms have become abundant in the past few decades. Precisions in observations of all types vary as well. In the face of these challenges, the interface between Statistics and Oceanography has proven to be a fruitful area for research and the development of useful models. With the recognition of the key importance of identifying, quantifying and managing uncertainty in data and models of ocean processes, a hierarchical perspective has become increasingly productive. As examples, we review a heterogeneous mix of studies from our own work demonstrating Bayesian hierarchical model applications in ocean physics, air-sea interaction, ocean forecasting and ocean ecosystem models. This review is by no means exhaustive and we have endeavored to identify hierarchical modeling work reported by others across the broad range of ocean-related topics reported in the statistical literature. We conclude by noting relevant oceanstatistics problems on the immediate research horizon, and some technical challenges they pose, for example, in terms of nonlinearity, dimensionality and computing ©Institute of Mathematical Statistics, 2013.},
   author = {Christopher K. Wikle and Ralph F. Milliff and Radu Herbei and William B. Leeds},
   doi = {10.1214/13-STS436},
   issn = {08834237},
   issue = {4},
   journal = {Statistical Science},
   keywords = {Bayesian,Biogeochemical,Ecosystem,Ocean vector winds,Quadratic nonlinearity,Sea surface temperature,Spatio-temporal,State-space},
   month = {11},
   pages = {466-486},
   title = {Modern statistical methods in oceanography: A hierarchical perspective},
   volume = {28},
   year = {2013}
}
@article{Liu2018a,
   abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter $\{k\}$ of $\{k\}$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
   author = {Huawen Liu and Xuelong Li and Jiuyong Li and Shichao Zhang},
   doi = {10.1109/TSMC.2017.2718220},
   issn = {21682232},
   issue = {12},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
   keywords = {Dimension reduction,high-dimensional data,k nearest neighbors (kNN),low-rank approximation,outlier detection},
   month = {12},
   pages = {2451-2461},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Efficient Outlier Detection for High-Dimensional Data},
   volume = {48},
   year = {2018}
}
@article{Zarokanellos2022,
   abstract = {Ocean fronts are areas that can support phytoplankton production through fertilization in the sunlit layer and the subduction of biogeochemical properties from the surface to the interior of the ocean. The Almeria-Oran (AO) front is formed from the juxtaposition of fresh inflowing Atlantic waters and more saline re-circulating Mediterranean waters. A fleet of three gliders flying in parallel lines was deployed across the AO to obtain observations in the CALYPSO project. These observations were combined with remote sensing and modeling simulations, thus providing a novel approach to identifying the three-dimensional transport and the submesoscale across-front circulation. The resulting 33 cross-front sections reveal spatial and temporal changes in the frontal boundary, with isopycnals steepening and/or relaxing. The observations revealed strong horizontal density gradients (up to ∼1.4 kg m−3) and the spatial variability was observed over different length scales (∼10–45 km). The potential vorticity decreased across the front due to the vorticity component in the horizontal density gradient direction. The predominant cyclonic relative vorticity on the dense side of the AO is associated with downwelling processes. The biogeochemical observations also suggest vertical transport along coherent pathways through baroclinic instability. Phytoplankton biomass enhancement occurs as a result, and is subducted below the euphotic layer. The observed oxygen filaments show upwelling and downwelling, providing a mechanism for oxygenating deeper layers and reducing the ventilation of deep low-oxygenated waters. Understanding the mechanisms of vertical transport can help us evaluate the dynamics of ocean fronts and their impacts on biological carbon storage.},
   author = {Nikolaos D. Zarokanellos and Daniel L. Rudnick and Maximo Garcia-Jove and Baptiste Mourre and Simon Ruiz and Ananda Pascual and Joaquin Tintoré},
   doi = {10.1029/2021JC017405},
   issn = {21699291},
   issue = {3},
   journal = {Journal of Geophysical Research: Oceans},
   keywords = {Almeria-Oran front,Instability processes,frontal dynamics,phytoplankton and oxygen distribution,submesoscale filaments,vertical motions},
   month = {3},
   publisher = {John Wiley and Sons Inc},
   title = {Frontal Dynamics in the Alboran Sea: 1. Coherent 3D Pathways at the Almeria-Oran Front Using Underwater Glider Observations},
   volume = {127},
   year = {2022}
}
@article{Zhang2016a,
   abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
   author = {Xiaoke Zhang and Jane Ling Wang},
   doi = {10.1214/16-AOS1446},
   issn = {00905364},
   issue = {5},
   journal = {Annals of Statistics},
   keywords = {Asymptotic normality,L2 convergence,Local linear smoothing,Uniform convergence,Weighing schemes},
   month = {10},
   pages = {2281-2321},
   publisher = {Institute of Mathematical Statistics},
   title = {From sparse to dense functional data and beyond},
   volume = {44},
   year = {2016}
}
@inproceedings{Suhermi2024,
   abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
   author = {Novri Suhermi and Rahida Rihhadatul Aisy},
   doi = {10.1109/IC3INA64086.2024.10732718},
   issn = {29945925},
   issue = {2024},
   booktitle = {International Conference on Computer, Control, Informatics and its Applications, IC3INA},
   keywords = {Energy Consumption,Functional Data Analysis,Functional Regression,Prediction},
   pages = {468-471},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Functional Data Analysis for Household Appliance Energy Consumption Prediction},
   year = {2024}
}
@article{Yang2023,
   abstract = {Underwater gliders have been widely used in oceanography for a range of applications. However, unpredictable events like shark strikes or remora attachments can lead to abnormal glider behavior or even loss of the instrument. This paper employs an anomaly detection algorithm to assess operational conditions of underwater gliders in the real-world ocean environment. Prompt alerts are provided to glider pilots upon detecting any anomaly, so that they can take control of the glider to prevent further harm. The detection algorithm is applied to multiple datasets collected in real glider deployments led by the University of Georgia's Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). In order to demonstrate the algorithm generality, the experimental evaluation is applied to four glider deployment datasets, each highlighting various anomalies happening in different scenes. Specifically, we utilize high resolution datasets only available post-recovery to perform detailed analysis of the anomaly and compare it with pilot logs. Additionally, we simulate the online detection based on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery one, the online detection is of great importance as it allows glider pilots to monitor potential abnormal conditions in real time.},
   author = {Ruochu Yang and Chad Lembke and Fumin Zhang and Catherine Edwards},
   month = {7},
   title = {General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Datasets},
   url = {http://arxiv.org/abs/2308.00180},
   year = {2023}
}
@article{Yao2005,
   abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
   author = {Fang Yao and Hans Georg Müller and Jane Ling Wang},
   doi = {10.1198/016214504000001745},
   issn = {01621459},
   issue = {470},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotics,Conditioning,Confidence band,Measurement error,Principal components,Simultaneous inference,Smoothing},
   month = {6},
   pages = {577-590},
   title = {Functional data analysis for sparse longitudinal data},
   volume = {100},
   year = {2005}
}
@article{Kelling2009,
   abstract = {The increasing availability of massive volumes of scientific data requires new synthetic analysis techniques to explore and identify interesting patterns that are otherwise not apparent. For biodiversity studies, a "data-driven" approach is necessary because of the complexity of ecological systems, particularly when viewed at large spatial and temporal scales. Data-intensive science organizes large volumes of data from multiple sources and fields and then analyzes them using techniques tailored to the discovery of complex patterns in high-dimensional data through visualizations, simulations, and various types of model building. Through interpreting and analyzing these models, truly novel and surprising patterns that are "born from the data" can be discovered. These patterns provide valuable insight for concrete hypotheses about the underlying ecological processes that created the observed data. Data-intensive science allows scientists to analyze bigger and more complex systems efficiently, and complements more traditional scientific processes of hypothesis generation and experimental testing to refine our understanding of the natural world. © 2009 by American Institute of Biological Sciences.},
   author = {Steve Kelling and Wesley M. Hochachka and Daniel Fink and Mirek Riedewald and Rich Caruana and Grant Ballard and Giles Hooker},
   doi = {10.1525/bio.2009.59.7.12},
   issn = {00063568},
   issue = {7},
   journal = {BioScience},
   keywords = {Biodiversity,Data-intensive science,Informatics,Machine learning,Statistics},
   pages = {613-620},
   title = {Data-intensive science: A new paradigm for biodiversity studies},
   volume = {59},
   year = {2009}
}
@article{Huang2019,
   abstract = {There has been extensive work on data depth-based methods for robust multivariate data analysis. Recent developments have moved to infinite-dimensional objects, such as functional data. In this work, we propose a notion of depth, the total variation depth, for functional data, which has many desirable features and is well suited for outlier detection. The proposed depth is in the form of an integral of a univariate depth function. We show that the novel formation of the total variation depth leads to useful decomposition associated with shape and magnitude outlyingness of functional data. Compared to magnitude outliers, shape outliers are often masked among the rest of samples and more difficult to identify. We then further develop an effective procedure and visualization tools for detecting both types of outliers, while naturally accounting for the correlation in functional data. The outlier detection performance is investigated through simulations under various outlier models. Finally, the proposed methodology is demonstrated using real datasets of curves, images, and video frames.},
   author = {Huang Huang and Ying Sun},
   doi = {10.1080/00401706.2019.1574241},
   issn = {15372723},
   issue = {4},
   journal = {Technometrics},
   keywords = {Data depth,Functional data,Outlier detection,Shape outliers,Total variation},
   pages = {445-458},
   publisher = {Taylor \& Francis},
   title = {A Decomposition of Total Variation Depth for Understanding Functional Outliers},
   volume = {61},
   url = {https://doi.org/10.1080/00401706.2019.1574241},
   year = {2019}
}
@book{Ramsay2005,
   author = {J.O. Ramsay and B.W. Silverman},
   city = {New York},
   isbn = {1-4419-0319-8},
   publisher = {Springer},
   title = {Functional Data Analysis},
   year = {2005}
}
@article{Rigueira2025a,
   abstract = {Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude–MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
   author = {Xurxo Rigueira and David Olivieri and Maria Araujo and Angeles Saavedra and Maria Pazo},
   doi = {10.1016/j.envsoft.2025.106443},
   issn = {13648152},
   issue = {April},
   journal = {Environmental Modelling and Software},
   keywords = {Anomaly detection,Functional data analysis,Sensor data,Supervised machine learning,Water quality},
   pages = {106443},
   publisher = {Elsevier Ltd},
   title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
   volume = {190},
   url = {https://doi.org/10.1016/j.envsoft.2025.106443},
   year = {2025}
}
@article{Zhang2025b,
   abstract = {We propose a novel test procedure for comparing mean functions across two groups within the reproducing kernel Hilbert space (RKHS) framework. Our proposed method is adept at handling sparsely and irregularly sampled functional data when observation times are random for each subject. Conventional approaches, which are built upon functional principal components analysis, usually assume a homogeneous covariance structure across groups. Nonetheless, justifying this assumption in real-world scenarios can be challenging. To eliminate the need for a homogeneous covariance structure, we first develop a linear approximation for the mean estimator under the RKHS framework; this approximation is a sum of i.i.d. random elements, which naturally leads to the desirable pointwise limiting distributions. Moreover, we establish weak convergence for the mean estimator, allowing us to construct a test statistic for the mean difference. Our method is easily implementable and outperforms some conventional tests in control-ling type I errors across various settings. We demonstrate the finite sample performance of our approach through extensive simulations and two real-world applications.},
   author = {Chi Zhang and Peijun Sang and Yingli Qin},
   doi = {10.1214/25-EJS2348},
   issn = {19357524},
   issue = {1},
   journal = {Electronic Journal of Statistics},
   keywords = {Mean difference detection,pointwise confidence interval,reproducing kernel Hilbert space,weak convergence},
   pages = {792-864},
   title = {Two-sample inference for sparse functional data},
   volume = {19},
   year = {2025}
}
@article{Zhang2025a,
   abstract = {We propose a novel test procedure for comparing mean functions across two groups within the reproducing kernel Hilbert space (RKHS) framework. Our proposed method is adept at handling sparsely and irregularly sampled functional data when observation times are random for each subject. Conventional approaches, which are built upon functional principal components analysis, usually assume a homogeneous covariance structure across groups. Nonetheless, justifying this assumption in real-world scenarios can be challenging. To eliminate the need for a homogeneous covariance structure, we first develop a linear approximation for the mean estimator under the RKHS framework; this approximation is a sum of i.i.d. random elements, which naturally leads to the desirable pointwise limiting distributions. Moreover, we establish weak convergence for the mean estimator, allowing us to construct a test statistic for the mean difference. Our method is easily implementable and outperforms some conventional tests in control-ling type I errors across various settings. We demonstrate the finite sample performance of our approach through extensive simulations and two real-world applications.},
   author = {Chi Zhang and Peijun Sang and Yingli Qin},
   doi = {10.1214/25-EJS2348},
   issn = {19357524},
   issue = {1},
   journal = {Electronic Journal of Statistics},
   keywords = {Mean difference detection,pointwise confidence interval,reproducing kernel Hilbert space,weak convergence},
   pages = {792-864},
   title = {Two-sample inference for sparse functional data},
   volume = {19},
   year = {2025}
}
@article{Genton2000,
   abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and sep-arable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce non-stationary kernels to either stationarity or local stationarity.},
   author = {Marc G Genton},
   doi = {10.1162/15324430260185646},
   issn = {0003-6951},
   journal = {CrossRef Listing of Deleted DOIs},
   keywords = {anisotropic,ary,compactly supported,covariance,isotropic,locally station-,nonstationary,reducible,separable,stationary},
   pages = {299-312},
   title = {10.1162/15324430260185646},
   volume = {1},
   year = {2000}
}
@book{Ojo2022,
   abstract = {We propose two new outlier detection methods, for identifying and classifying different types of outliers in (big) functional data sets. The proposed methods are based on an existing method called Massive Unsupervised Outlier Detection (MUOD). MUOD detects and classifies outliers by computing for each curve, three indices, all based on the concept of linear regression and correlation, which measure outlyingness in terms of shape, magnitude and amplitude, relative to the other curves in the data. ‘Semifast-MUOD’, the first method, uses a sample of the observations in computing the indices, while ‘Fast-MUOD’, the second method, uses the point-wise or L1 median in computing the indices. The classical boxplot is used to separate the indices of the outliers from those of the typical observations. Performance evaluation of the proposed methods using simulated data show significant improvements compared to MUOD, both in outlier detection and computational time. We show that Fast-MUOD is especially well suited to handling big and dense functional datasets with very small computational time compared to other methods. Further comparisons with some recent outlier detection methods for functional data also show superior or comparable outlier detection accuracy of the proposed methods. We apply the proposed methods on weather, population growth, and video data.},
   author = {Oluwasegun Taiwo Ojo and Antonio Fernández Anta and Rosa E. Lillo and Carlo Sguera},
   doi = {10.1007/s11634-021-00460-9},
   isbn = {1163402100460},
   issn = {18625355},
   issue = {3},
   journal = {Advances in Data Analysis and Classification},
   keywords = {Fast-MUOD,Functional data analysis,MUOD,Outlier detection,Semifast-MUOD},
   pages = {725-760},
   publisher = {Springer Berlin Heidelberg},
   title = {Detecting and classifying outliers in big functional data},
   volume = {16},
   year = {2022}
}
@article{Matsumoto2015,
   abstract = {Outlier detection, also known as anomaly detection, is a common data mining task in identifying data points that are outside expected patterns in a given dataset. It has useful applications such as network intrusion, system faults, and fraudulent activity. In addition, real world data are uncertain in nature and they may be represented as uncertain data. In this paper, we propose an improved parallel algorithm for outlier detection on uncertain data using density sampling and develop an implementation running on both GPUs and multi-core CPUs, using the OpenCL framework. Our main focus is on GPUs, as they are a cost effective massively parallel floating point processor that is suitable for many data mining applications. Our implementation exploits some key features in GPUs, and is significantly different from a traditional CPU implementation. We first present an improved uncertain outlier detection algorithm. Then, we demonstrate two parallel micro-clustering implementations. The performance and detection quality comparisons demonstrate the benefits of the improved algorithm and parallel implementation on GPUs.},
   author = {Takazumi Matsumoto and Edward Hung and Man Lung Yiu},
   doi = {10.1007/s10619-014-7155-9},
   issn = {15737578},
   issue = {3},
   journal = {Distributed and Parallel Databases},
   keywords = {GPU,Outlier detection,Parallel processing,Uncertain data},
   pages = {417-447},
   title = {Parallel outlier detection on uncertain data for GPUs},
   volume = {33},
   year = {2015}
}
@article{Cao2023,
   abstract = {Outlier detection is critical in real world. Due to the existence of many outlier detection techniques which often return different results for the same data set, the users have to address the problem of determining which among these techniques is the best suited for their task and tune its parameters. This is particularly challenging in the unsupervised setting, where no labels are available for cross-validation needed for such method and parameter optimization. In this work, we propose AutoOD which uses the existing unsupervised detection techniques to automatically produce high quality outliers without any human tuning. AutoOD's fundamentally new strategy unifies the merits of unsupervised outlier detection and supervised classification within one integrated solution. It automatically tests a diverse set of unsupervised outlier detectors on a target data set, extracts useful signals from their combined detection results to reliably capture key differences between outliers and inliers. It then uses these signals to produce a "custom outlier classifier" to classify outliers, with its accuracy comparable to supervised outlier classification models trained with ground truth labels - without having access to the much needed labels. On a diverse set of benchmark outlier detection datasets, AutoOD consistently outperforms the best unsupervised outlier detector selected from hundreds of detectors. It also outperforms other tuning-free approaches from 12 to 97 points (out of 100) in the F-1 score.},
   author = {Lei Cao and Yizhou Yan and Yu Wang and Samuel Madden and Elke A. Rundensteiner},
   doi = {10.1145/3588700},
   issue = {1},
   journal = {Proceedings of the ACM on Management of Data},
   keywords = {Unsupervised Outlier Detection, Automatic Tuning,},
   pages = {1-27},
   publisher = {Association for Computing Machinery},
   title = {AutoOD: Automatic Outlier Detection},
   volume = {1},
   year = {2023}
}
@book{Ibrahim2009a,
   abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
   author = {Joseph Ibrahim and Ming-Hui Chen and Debajyoti Sinha},
   isbn = {9781441968241},
   issn = {03436993},
   issue = {2},
   journal = {The Elements of Statistical Learning},
   pages = {83–85},
   pmid = {15512507},
   title = {Springer Series in Statistics},
   volume = {27},
   url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
   year = {2009}
}
@book{Ibrahim2009,
   abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
   author = {Joseph Ibrahim and Ming-Hui Chen and Debajyoti Sinha},
   isbn = {9781441968241},
   issn = {03436993},
   issue = {2},
   journal = {The Elements of Statistical Learning},
   pages = {83–85},
   pmid = {15512507},
   title = {Springer Series in Statistics},
   volume = {27},
   url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
   year = {2009}
}
@article{Majesty,
   author = {Between His Majesty and Canadian Hydrographic Service and Whereas Chs and Whereas Chs and Whereas Chs and Chsthe User and Agree To and T H E Following and C H S Data},
   pages = {1-6},
   title = {Licence ( CHS NONNA Licence )}
}
@article{DFO-FisheriesandOceansCanada2023,
   author = {DFO - Fisheries and Oceans Canada},
   journal = {Government of Canada},
   pages = {102001},
   title = {Oceans Act Marine Protected Areas},
   volume = {83},
   url = {https://open.canada.ca/data/en/dataset/a1e18963-25dd-4219-a33f-1a38c4971250},
   year = {2023}
}
@article{Zhu2023,
   abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
   author = {Pengyun Zhu and Chaowei Zhang and Xiaofeng Li and Jifu Zhang and Xiao Qin},
   doi = {10.1109/TKDE.2022.3172167},
   issn = {15582191},
   issue = {6},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {High-dimensional outlier detection,local outlier coulomb force,neighborhood outlier factor,outlier coulomb resultant force,similarity metric},
   month = {6},
   pages = {5506-5520},
   publisher = {IEEE Computer Society},
   title = {A High-Dimensional Outlier Detection Approach Based on Local Coulomb Force},
   volume = {35},
   year = {2023}
}
@book{,
   isbn = {9780933957404},
   publisher = {IEEE},
   title = {2013 OCEANS-San Diego : 23-27 September 2013},
   year = {2014}
}
@techReport{Xu2018,
   abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
   author = {Xiaodan Xu and Huawen Liu and Li Li and Minghai Yao},
   keywords = {data mining,evaluation measurement,high-dimensional data,outlier detection},
   title = {A Comparison of Outlier Detection Techniques for High-Dimensional Data},
   year = {2018}
}
@article{Nonna2024,
   author = {C H S Nonna and Data Portal and Guidance Document and Home Page and Nonna Packages and Interrogating Contributing Sources and Nonna Products and Catzoc Id and Sounding Measurement and Accessing C H S Nonna},
   pages = {1-12},
   title = {CHS NONNA Data Portal Guidance Document Table of Contents},
   year = {2024}
}
@article{Slaets2012,
   abstract = {Functional data that are not perfectly aligned in the sense of not showing peaks and valleys at the precise same locations possess phase variation. This is commonly addressed by preprocessing the data via a warping procedure. As opposed to treating phase variation as a nuisance effect, it is advantageous to recognize it as a possible important source of information for clustering. It is illustrated how results from a multiresolution warping procedure can be used for clustering. This approach allows us to address detailed questions to find local clusters that differ in phase, or clusters that differ in amplitude, or both simultaneously. © 2012 Elsevier B.V. All rights reserved.},
   author = {Leen Slaets and Gerda Claeskens and Mia Hubert},
   doi = {10.1016/j.csda.2012.01.017},
   issn = {01679473},
   issue = {7},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Amplitude variation,Clustering,Functional data,Phase variation,Warping},
   pages = {2360-2374},
   publisher = {Elsevier B.V.},
   title = {Phase and amplitude-based clustering for functional data},
   volume = {56},
   url = {http://dx.doi.org/10.1016/j.csda.2012.01.017},
   year = {2012}
}
@article{Ren2023,
   abstract = {Clustering for multivariate functional data is a challenging problem since the data are represented by a set of curves and functions belonging to an infinite-dimensional space. In this article, we propose a novel clustering method for multivariate functional data using an adaptive density peak detection technique. It is a quick cluster center identification algorithm based on the two measures of each functional data observation: the functional density estimate and the distance to the closest observation with a higher functional density. We suggest two types of functional density estimators for multivariate functional data. The first one is a functional (Formula presented.) -nearest neighbor density estimator based on (a) an L2 distance between raw functional curves, or (b) a semimetric of multivariate functional principal components. The second one is a (Formula presented.) -nearest neighbor density estimator based on multivariate functional principal scores. Our clustering method is computationally fast since it does not need an iterative process. The flexibility and advantages of the method are examined by comparing it with other existing clustering methods in simulation studies. A user-friendly R package FADPclust is developed for public use. Finally, our method is applied to a real case study in lung cancer research.},
   author = {Rui Ren and Kuangnan Fang and Qingzhao Zhang and Xiaofeng Wang},
   doi = {10.1002/sim.9687},
   issn = {10970258},
   issue = {10},
   journal = {Statistics in Medicine},
   keywords = {clustering,density peak detection,k-nearest neighbor density estimation,multivariate functional data},
   pages = {1565-1582},
   pmid = {36825602},
   title = {Multivariate functional data clustering using adaptive density peak detection},
   volume = {42},
   year = {2023}
}
@article{Jacques2014,
   abstract = {Clustering techniques for functional data are reviewed. Four groups of clustering algorithms for functional data are proposed. The first group consists of methods working directly on the evaluation points of the curves. The second groups is defined by filtering methods which first approximate the curves into a finite basis of functions and second perform clustering using the basis expansion coefficients. The third groups is composed of methods which perform simultaneously dimensionality reduction of the curves and clustering, leading to functional representation of data depending on clusters. The last group consists of distance-based methods using clustering algorithms based on specific distances for functional data. A software review as well as an illustration of the application of these algorithms on real data are presented. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {Julien Jacques and Cristian Preda},
   doi = {10.1007/s11634-013-0158-y},
   issn = {18625355},
   issue = {3},
   journal = {Advances in Data Analysis and Classification},
   keywords = {Basis expansion,Clustering,Functional data,Functional principal component analysis},
   pages = {231-255},
   title = {Functional data clustering: A survey},
   volume = {8},
   year = {2014}
}
@article{Chen2025,
   abstract = {In functional data analysis, unsupervised clustering has been extensively conducted and has important implications. In most of the existing functional clustering analyses, it is assumed that there is a single clustering structure across the whole domain of measurement (say, time interval). In some data analyses, for example, the analysis of normalized COVID-19 daily confirmed cases for the U.S. states, it is observed that functions can have different clustering patterns in different time subintervals. To tackle the lack of flexibility of the existing functional clustering techniques, we develop a local clustering approach, which can fully data-dependently identify subintervals, where, in different subintervals, functions have different clustering structures. This approach is built on the basis expansion technique and has a novel penalization form. It simultaneously achieves subinterval identification, clustering, and estimation. Its estimation and clustering consistency properties are rigorously established. In simulation, it significantly outperforms multiple competitors. In the analysis of the COVID-19 case trajectory data, it identifies sensible subintervals and clustering structures. Supplementary materials for this article are available online.},
   author = {Yuanxing Chen and Qingzhao Zhang and Shuangge Ma},
   doi = {10.1080/10618600.2024.2431057},
   issn = {15372715},
   issue = {3},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Functional clustering,Local clustering,Penalized estimation},
   pages = {1075-1090},
   publisher = {Taylor \& Francis},
   title = {Local Clustering for Functional Data},
   volume = {34},
   url = {https://doi.org/10.1080/10618600.2024.2431057},
   year = {2025}
}
@article{Liu2009,
   abstract = {Study of dynamic processes in many areas of science has led to the appearance of functional data sets. It is often the case that individual trajectories vary both in the amplitude space and in the time space. We develop a coherent clustering procedure that allows for temporal aligning. Under this framework, closed form solutions of an EM type learning algorithm are derived. The method can be applied to all types of curve data but is particularly useful when phase variation is present. We demonstrate the method by both simulation studies and an application to human growth curves.},
   author = {Xueli Liu and Mark C.K. Yang},
   doi = {10.1016/j.csda.2008.11.019},
   issn = {01679473},
   issue = {4},
   journal = {Computational Statistics and Data Analysis},
   pages = {1361-1376},
   publisher = {Elsevier B.V.},
   title = {Simultaneous curve registration and clustering for functional data},
   volume = {53},
   url = {http://dx.doi.org/10.1016/j.csda.2008.11.019},
   year = {2009}
}
@article{Katragadda2018,
   abstract = {Parallel test capability, enabled by numerous independent measurement channels has significantly increased throughput in parametric testing. It involves testing of numerous devices simultaneously synchronously or asynchronously. The number of devices tested for a given pad layout is increased by using higher dimensional arrays, the hallmark of which is pad sharing. Parallel testing of multiple devices with shared pads is vulnerable to device fails, where a failing device adversely affects measurement of all other devices. Information about this failing device or compromised measurement would only be evident at post analysis where a retest with a recipe change can then be ordered. In some cases retest is impossible as wafers would have already moved on to subsequent processing steps, thereby losing valuable learning opportunity. On the other hand, having to wait for post analysis requires time. Ideally failure detection and subsequent re-measure is done dynamically while the device is under test. This would require that decision making capability to be implemented in an automated tester equipment. In this work, we will discuss an algorithm based approach to adaptively change the test program allowing testing or skipping devices based on data collected real time while device is under test. The adaptive algorithm is also extended to aid in test time efficiency by eliminating tests based on measurement results of preceding tests.},
   author = {Veenadhar Katragadda and Martin Muthee and Arthur Gasasira and Frank Seelmann and Jiun Hsin Liao},
   doi = {10.1109/ICMTS.2018.8383784},
   isbn = {9781538650691},
   journal = {IEEE International Conference on Microelectronic Test Structures},
   keywords = {Adaptive algorithm,Automated Test Equipment (ATE),Keysight P9000 MPPT,Outlier Detection,Parallel testing,Parametric testing},
   pages = {142-146},
   publisher = {IEEE},
   title = {Algorithm based adaptive parametric testing for outlier detection and test time reduction},
   volume = {2018-March},
   year = {2018}
}
@article{Cuevas2009,
   abstract = {A general depth measure, based on the use of one-dimensional linear continuous projections, is proposed. The applicability of this idea in different statistical setups (including inference in functional data analysis, image analysis and classification) is discussed. A special emphasis is made on the possible usefulness of this method in some statistical problems where the data are elements of a Banach space. The asymptotic properties of the empirical approximation of the proposed depth measure are investigated. In particular, its asymptotic distribution is obtained through U-statistics techniques. The practical aspects of these ideas are discussed through a small simulation study and a real-data example. © 2008 Elsevier Inc. All rights reserved.},
   author = {Antonio Cuevas and Ricardo Fraiman},
   doi = {10.1016/j.jmva.2008.08.002},
   issn = {0047259X},
   issue = {4},
   journal = {Journal of Multivariate Analysis},
   keywords = {62G07,62G20,Depth measures,Functional data,Projections method,Supervised classification,primary,secondary},
   month = {4},
   pages = {753-766},
   title = {On depth measures and dual statistics. A methodology for dealing with general data},
   volume = {100},
   year = {2009}
}
@article{Li2020,
   abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
   author = {Xiaojie Li and Jiancheng Lv and Zhang Yi},
   doi = {10.1109/TCYB.2018.2876615},
   issn = {21682275},
   issue = {5},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Discrimination,outlier detection,outlier factor,structural scores},
   month = {5},
   pages = {2302-2310},
   pmid = {30418896},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Outlier Detection Using Structural Scores in a High-Dimensional Space},
   volume = {50},
   year = {2020}
}
@misc{Correia2024,
   abstract = {Time-series anomaly detection plays an important role in engineering processes, like development, manufacturing and other operations involving dynamic systems. These processes can greatly benefit from advances in the field, as state-of-the-art approaches may aid in cases involving, for example, highly dimensional data. To provide the reader with understanding of the terminology, this survey introduces a novel taxonomy where a distinction between online and offline, and training and inference is made. Additionally, it presents the most popular data sets and evaluation metrics used in the literature, as well as a detailed analysis. Furthermore, this survey provides an extensive overview of the state-of-the-art model-based online semi- and unsupervised anomaly detection approaches for multivariate time-series data, categorising them into different model families and other properties. The biggest research challenge revolves around benchmarking, as currently there is no reliable way to compare different approaches against one another. This problem is two-fold: on the one hand, public data sets suffers from at least one fundamental flaw, while on the other hand, there is a lack of intuitive and representative evaluation metrics in the field. Moreover, the way most publications choose a detection threshold disregards real-world conditions, which hinders the application in the real world. To allow for tangible advances in the field, these issues must be addressed in future work.},
   author = {Lucas Correia and Jan Christoph Goos and Philipp Klein and Thomas Bäck and Anna V. Kononova},
   doi = {10.1016/j.engappai.2024.109323},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Anomaly detection,Model-based,Multivariate,Online,Survey,Time series},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Online model-based anomaly detection in multivariate time series: Taxonomy, survey, research challenges and future directions},
   volume = {138},
   year = {2024}
}
@article{Zhang2024,
   abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
   author = {Xianyong Zhang and Zhong Yuan and Duoqian Miao},
   doi = {10.1109/TKDE.2023.3312108},
   issn = {15582191},
   issue = {5},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Data mining,neighborhood rough sets,outlier detection,three-way decision,uncertainty measurement},
   month = {5},
   pages = {2082-2095},
   publisher = {IEEE Computer Society},
   title = {Outlier Detection Using Three-Way Neighborhood Characteristic Regions and Corresponding Fusion Measurement},
   volume = {36},
   year = {2024}
}
@article{Yu2025,
   abstract = {Functional data analysis (FDA) is an important modern paradigm for handling infinite-dimensional data. An important task in FDA is clustering, which identifies subgroups based on the shapes of measured curves. Considering that derivatives can provide additional useful information about the shapes of functionals, we propose a novel L2 distance between two random functions by incorporating the functions and their derivative information to determine the dissimilarity of curves under a unified scheme for dense observations. The Karhunen–Loève expansion is used to approximate the curves and their derivatives. Cluster membership prediction for each curve intends to minimize the new distances between the observed and predicted curves through subspace projection among all possible clusters. We provide consistent estimators for the curves, curve derivatives, and the proposed distance. Identifiability issues of the clustering procedure are also discussed. The utility of the proposed method is illustrated via simulation studies and applications to two real datasets. The proposed method can considerably improve cluster performance compared with existing functional clustering methods. Supplementary materials for the article are available online.},
   author = {Ping Yu and Gongming Shi and Chunjie Wang and Xinyuan Song},
   doi = {10.1080/10618600.2024.2366499},
   issn = {15372715},
   issue = {1},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Clustering,Curve derivatives,Functional principal component analysis,Identifiability,Projection},
   pages = {47-58},
   publisher = {Taylor \& Francis},
   title = {Distance-based Clustering of Functional Data with Derivative Principal Component Analysis},
   volume = {34},
   url = {https://doi.org/10.1080/10618600.2024.2366499},
   year = {2025}
}
@article{Alcacer2024,
   abstract = {In this study, we introduce an innovative methodology for anomaly detection of curves, applicable to both multivariate and multi-argument functions. This approach distinguishes itself from prior methods by its capability to identify outliers within clustered functional data sets. We achieve this by extending the recent AA + kNN technique, originally designed for multivariate analysis, to functional data contexts. Our method demonstrates superior performance through a comprehensive comparative analysis against twelve state-of-the-art techniques, encompassing simulated scenarios with either a single functional cluster or multiple clusters. Additionally, we substantiate the effectiveness of our approach through its application in three distinct computer vision tasks and a signal processing problem. To facilitate transparency and replication of our results, we provide access to both the code and the datasets used in this research.},
   author = {Aleix Alcacer and Irene Epifanio},
   doi = {10.1371/journal.pone.0311418},
   isbn = {1111111111},
   issn = {19326203},
   issue = {11},
   journal = {PLoS ONE},
   pages = {1-23},
   pmid = {39585824},
   title = {Outlier detection of clustered functional data with image and signal processing applications by archetype analysis},
   volume = {19},
   url = {http://dx.doi.org/10.1371/journal.pone.0311418},
   year = {2024}
}
@article{Ma2017,
   abstract = {An important step in developing individualized treatment strategies is correct identification of subgroups of a heterogeneous population to allow specific treatment for each subgroup. This article considers the problem using samples drawn from a population consisting of subgroups with different mean values, along with certain covariates. We propose a penalized approach for subgroup analysis based on a regression model, in which heterogeneity is driven by unobserved latent factors and thus can be represented by using subject-specific intercepts. We apply concave penalty functions to pairwise differences of the intercepts. This procedure automatically divides the observations into subgroups. To implement the proposed approach, we develop an alternating direction method of multipliers algorithm with concave penalties and demonstrate its convergence. We also establish the theoretical properties of our proposed estimator and determine the order requirement of the minimal difference of signals between groups to recover them. These results provide a sound basis for making statistical inference in subgroup analysis. Our proposed method is further illustrated by simulation studies and analysis of a Cleveland heart disease dataset. Supplementary materials for this article are available online.},
   author = {Shujie Ma and Jian Huang},
   doi = {10.1080/01621459.2016.1148039},
   issn = {1537274X},
   issue = {517},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotic normality,Heterogeneity,Inference,Linear regression,Oracle property},
   pages = {410-423},
   publisher = {Taylor \& Francis},
   title = {A Concave Pairwise Fusion Approach to Subgroup Analysis},
   volume = {112},
   url = {https://doi.org/10.1080/01621459.2016.1148039},
   year = {2017}
}
@article{Saeidi2025,
   abstract = {This paper proposes a new probability density approximation for functional random variables in the reproducing kernel Hilbert space (RKHS). Based on this approximation, a novel method for model-based clustering of functional data named Fdmclust is introduced. The previous study was based on the Gaussian assumption of the functional data due to the independence used for the covariance kernel. This assumption is not necessarily valid for general cases. The proposed method is applicable to both Gaussian and non-Gaussian functional data, utilizing the projection of functional data onto the Mercer kernel rather than the covariance kernel. To this end, PCA and ICA are used on the obtained projections for Gaussian and non-Gaussian functional random variables, respectively. The estimation of parameters is based on the EM-algorithm. The Fdmclust approach is evaluated on several simulated and real datasets, and the results confirm its efficiency.},
   author = {Hanieh Saeidi and Mina Aminghafari and Mehdi Ashkartizabi},
   doi = {10.1016/j.neucom.2025.130768},
   issn = {18728286},
   issue = {July},
   journal = {Neurocomputing},
   keywords = {Functional data,Independent component analysis,Model-based clustering,Probability density for a random function,Reproducing Kernel Hilbert space},
   pages = {130768},
   publisher = {Elsevier B.V.},
   title = {Fdmclust: Functional data model-based clustering using approximation of probability density for a random function in a reproducing Kernel Hilbert space framework},
   volume = {650},
   url = {https://doi.org/10.1016/j.neucom.2025.130768},
   year = {2025}
}
@inbook{Kaufman1990,
   author = {Leonard Kaufman and Peter J. Rousseeuw},
   doi = {10.1002/9780470316801.ch2},
   isbn = {9780470316801},
   booktitle = {Finding Groups in Data: An Introduction to Cluster Analysis},
   pages = {68-125},
   publisher = {John Wiley \& Sons, Inc.},
   title = {Partitioning Around Medoids (Program PAM)},
   year = {1990}
}
@article{Jin2024,
   abstract = {Multisensor data that track system operating behaviors are widely available nowadays from various engineering systems. Measurements from each sensor over time form a curve and can be viewed as functional data. Clustering of these multivariate functional curves is important for studying the operating patterns of systems. One complication in such applications is the possible presence of sensors whose data do not contain relevant information. Hence, it is desirable for the clustering method to equip with an automatic sensor selection procedure. Motivated by a real engineering application, we propose a functional data clustering method that simultaneously removes noninformative sensors and groups functional curves into clusters using informative sensors. Functional principal component analysis is used to transform multivariate functional data into a coefficient matrix for data reduction. We then model the transformed data by a Gaussian mixture distribution to perform model-based clustering with variable selection. Three types of penalties, the individual, variable, and group penalties, are considered to achieve automatic variable selection. Extensive simulations are conducted to assess the clustering and variable selection performance of the proposed methods. The application of the proposed methods to an engineering system with multiple sensors shows the promise of the methods and reveals interesting patterns in the sensor data.History: Kwok-Leung Tsui served as the senior editor for this article.Funding: The research by J. Min and Y. Hong was partially supported by the National Science Foundation [Grant CMMI-1904165] to Virginia Tech. The work by Y. Hong was partially supported by the Virginia Tech College of Science Research Equipment Fund.Data Ethics & Reproducibility Note: The original data set is proprietary and cannot be shared. The full code to replicate the results in this paper, based on summary statistics of the original data, is available at https://github.com/jiem3/MultiFuncClustering . The code applied to a simplified version is available at https://codeocean.com/capsule/4041000/tree/v1 , which covers the data analysis and part of the simulation scenarios with a single data set under each scenario using a fixed set of hyperparameters, for reducing computation time, and at https://doi.org/10.1287/ijds.2022.0034 .},
   author = {Zhongnan Jin and Jie Min and Yili Hong and Pang Du and Qingyu Yang},
   doi = {10.1287/ijds.2022.0034},
   issn = {2694-4022},
   issue = {2},
   journal = {INFORMS Journal on Data Science},
   keywords = {"Keywords: EM algorithm,Gaussian mixture distribution,functional data clustering,functional principal component analysis,group Lasso,signal processing"},
   pages = {203-218},
   publisher = {http://pubsonline.informs.org/journal/ijds},
   title = {Multivariate Functional Clustering with Variable Selection and Application to Sensor Data from Engineering Systems},
   volume = {3},
   year = {2024}
}
@article{Dubey2024,
   abstract = {This article provides an overview on the statistical modeling of complex data as increasingly encountered in modern data analysis. It is argued that such data can often be described as elements of a metric space that satisfies certain structural conditions and features a probability measure. We refer to the random elements of such spaces as random objects and to the emerging field that deals with their statistical analysis as metric statistics. Metric statistics provides methodology, theory and visualization tools for the statistical description, quantification of variation, centrality and quantiles, regression and inference for populations of random objects, inferring these quantities from available data and samples. In addition to a brief review of current concepts, we focus on distance profiles as a major tool for object data in conjunction with the pairwise Wasserstein transports of the underlying one-dimensional distance distributions. These pairwise transports lead to the definition of intuitive and interpretable notions of transport ranks and transport quantiles as well as two-sample inference. An associated profile metric complements the original metric of the object space and may reveal important features of the object data in data analysis. We demonstrate these tools for the analysis of complex data through various examples and visualizations.},
   author = {Paromita Dubey and Yaqing Chen and Hans Georg Müller},
   doi = {10.1214/24-AOS2368},
   issn = {21688966},
   issue = {2},
   journal = {Annals of Statistics},
   keywords = {Distributional data,Fréchet mean,Fréchet regression,Wasserstein metric,functional data analysis,metric variance,profile metric,transport quantile,transport rank,visualization},
   pages = {757-792},
   title = {Metric Statistics: Exploration and Inference for Random Objects With Distance Profiles},
   volume = {52},
   year = {2024}
}
@article{Qu2025,
   author = {Zhuo Qu and Wenlin Dai and Carolina Euan and Ying Sun and Marc G. Genton},
   doi = {10.1007/s11749-024-00952-8},
   issn = {1133-0686},
   issue = {2},
   journal = {Test},
   keywords = {Clustering,Data visualization,Exploratory data analysis,Functional boxplot,Multivariate functional data,Outlier detection},
   pages = {459-482},
   publisher = {Springer Berlin Heidelberg},
   title = {Exploratory functional data analysis},
   volume = {34},
   url = {https://doi.org/10.1007/s11749-024-00952-8},
   year = {2025}
}
@article{Wang2025,
   abstract = {Cluster analysis, or clustering, plays a crucial role across numerous scientific and engineering domains. Despite the wealth of clustering methods proposed over the past decades, each method is typically designed for specific scenarios and presents certain limitations in practical applications. In this paper, we propose depth-based local center clustering (DLCC). This novel method makes use of data depth, which is known to produce a center-outward ordering of sample points in a multivariate space. However, data depth typically fails to capture the multimodal characteristics of \{data\}, something of the utmost importance in the context of clustering. To overcome this, DLCC makes use of a local version of data depth that is based on subsets of \{data\}. From this, local centers can be identified as well as clusters of varying shapes. Furthermore, we propose a new internal metric based on density-based clustering to evaluate clustering performance on \{non-convex clusters\}. Overall, DLCC is a flexible clustering approach that seems to overcome some limitations of traditional clustering methods, thereby enhancing data analysis capabilities across a wide range of application scenarios.},
   author = {Siyi Wang and Alexandre Leblanc and Paul D. McNicholas},
   pages = {1-18},
   title = {Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios},
   url = {http://arxiv.org/abs/2505.09516},
   year = {2025}
}
@article{Wooldridge2025,
   abstract = {I derive a result on the equivalence between the two-way fixed effects (TWFE) estimator and an estimator obtained from a pooled ordinary least squares regression that includes unit-specific time averages and time-period-specific cross-sectional averages—the two-way Mundlak (TWM) regression. The equivalence between TWFE and TWM implies that various estimators used for intervention analysis can be computed using pooled OLS that controls for time-constant treatment cohort indicators, time-period indicators, covariates, and interactions among them—allowing for considerable treatment effect heterogeneity. An extended version of TWFE (ETWFE) is equivalent to the POLS approach. I show that an imputation estimator, derived under no anticipation and parallel trends assumptions, is also equivalent to the POLS/ETWFE estimator. The equivalence among various estimators shows that average treatment effects on the treated are identified by flexible regression. The framework allows for event study estimators, which can be used to test for pre-trends, and flexible estimation that allows for cohort-specific trends.},
   author = {Jeffrey M. Wooldridge},
   doi = {10.1007/s00181-025-02807-z},
   issn = {14358921},
   issue = {5},
   journal = {Empirical Economics},
   keywords = {Heterogeneous treatment effects,Heterogeneous trends,Panel data,Staggered interventions,Two-way fixed effects,Two-way mundlak regression},
   pages = {2545-2587},
   publisher = {Springer Berlin Heidelberg},
   title = {Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators},
   volume = {69},
   url = {https://doi.org/10.1007/s00181-025-02807-z},
   year = {2025}
}
@article{Reynaert2024,
   abstract = {The world has pledged to protect 30 percent of its land and waters by 2030 to halt the rapid deterioration of critical ecosystems. We summarize the state of knowledge about the impacts of protected area policies, with a focus on deforestation and vegetation cover. We discuss critical issues around data and measurement, identify the most commonly-used empirical methods, and summarize empirical evidence across multiple regions of the world. In most cases, protection has had at most a modest impact on forest cover, with stronger effects in areas that face pressure of economic development. We then identify several open areas for research to advance our understanding of the effectiveness of protected area policies: the use of promising recent econometric advancements, shifting focus to direct measures of biodiversity, filling the knowledge gap on the effect of protected area policy in advanced economies, investigating the long-run impacts of protection, and understanding its equilibrium effects.},
   author = {Mathias Reynaert and Eduardo Souza-Rodrigues and Arthur A. van Benthem},
   doi = {10.1016/j.regsciurbeco.2023.103968},
   issn = {18792308},
   issue = {2020},
   journal = {Regional Science and Urban Economics},
   keywords = {Biodiversity,Deforestation,Land protection},
   pages = {1-24},
   title = {The environmental impacts of protected area policy},
   volume = {107},
   year = {2024}
}
@article{DeChaisemartin2020,
   abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator.},
   author = {Clément de Chaisemartin and Xavier D’Haultfœuille},
   doi = {10.1257/aer.20181169},
   issn = {19447981},
   issue = {9},
   journal = {American Economic Review},
   pages = {2964-2996},
   title = {Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects},
   volume = {110},
   year = {2020}
}
@article{Rubin2005,
   abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism - a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials. © 2005 American Statistical Association.},
   author = {Donald B. Rubin},
   doi = {10.1198/016214504000001880},
   issn = {01621459},
   issue = {469},
   journal = {Journal of the American Statistical Association},
   keywords = {Analysis of covariance,Assignment mechanism,Assignment-based causal inference,Bayesian inference,Direct causal effects,Fieller-Creasy,Fisher,Neyman,Observational studies,Principal stratification,Randomized experiments,Rubin causal model},
   pages = {322-331},
   title = {Causal inference using potential outcomes: Design, modeling, decisions},
   volume = {100},
   year = {2005}
}
@article{Grupp2023,
   abstract = {The European Union designates 26% of its landmass as a protected area, limiting economic development to favor biodiversity. This paper uses the staggered introduction of protected-area policies between 1985 and 2020 to study the selection of land for protection and the causal effect of protection on vegetation cover and nightlights. Our results reveal protection did not affect the outcomes in any meaningful way across four decades, all countries, protection cohorts, and a wide range of land and climate attributes. We conclude that European conservation efforts lack ambition because policymakers select land for protection not threatened by development.},
   author = {Tristan Grupp and Prakash Mishra and Mathias Reynaert and Arthur A. van Benthem},
   doi = {10.2139/ssrn.4665580},
   issue = {June},
   journal = {SSRN Electronic Journal},
   keywords = {biodiversity,conservation,cover,deforestation,jel codes,land protection,nightlights,protected areas,q23,q24,q57,r14,staggered difference-in-differences,vegetation},
   title = {An Evaluation of Protected Area Policies in the European Union},
   year = {2023}
}
@article{JoshuaAngrist2014,
   author = {Joshua Angrist},
   issue = {March},
   pages = {373},
   title = {Mostly Harmless Econometrics : An Empiricist ' s Companion},
   year = {2014}
}
@article{Rico-Strafford2023,
   abstract = {F i r s t v e r s i o n : O c t o b e r 2 8 , 2 0 2 2 (c u r r e n t)},
   author = {Jemina Rico-Strafford and Zhenhua Wang and Alexander Pfaff},
   journal = {SSRN Electronic Journal},
   keywords = {comparative analysis,covid-19,macroeconomic impacts,natural disasters},
   pages = {1-8},
   title = {Comparing Protection Types in the Peruvian Amazon: Multiple-Use Protected Areas Did No Worse for Forests},
   year = {2023}
}
@article{Zhang2016,
   abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number Ni of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultradense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
   author = {Xiaoke Zhang and Jane Ling Wang},
   doi = {10.1214/16-AOS1446},
   issn = {00905364},
   issue = {5},
   journal = {Annals of Statistics},
   keywords = {Asymptotic normality,L2 convergence,Local linear smoothing,Uniform convergence,Weighing schemes},
   month = {10},
   pages = {2281-2321},
   publisher = {Institute of Mathematical Statistics},
   title = {From sparse to dense functional data and beyond},
   volume = {44},
   year = {2016}
}
@article{Yao2005,
   abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the co-variance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle. © 2005 American Statistical Association.},
   author = {Fang Yao and Hans Georg Müller and Jane Ling Wang},
   doi = {10.1198/016214504000001745},
   issn = {01621459},
   issue = {470},
   journal = {Journal of the American Statistical Association},
   keywords = {Asymptotics,Conditioning,Confidence band,Measurement error,Principal components,Simultaneous inference,Smoothing},
   month = {6},
   pages = {577-590},
   title = {Functional data analysis for sparse longitudinal data},
   volume = {100},
   year = {2005}
}
@article{Gijbels2017,
   abstract = {In this paper, we provide an elaboration on the desirable properties of statistical depths for functional data. Although a formal definition has been put forward in the literature, there are still several unclarities to be tackled, and further insights to be gained. Herein, a few interesting connections between the wanted properties are found. In particular, it is demonstrated that the conditions needed for some desirable properties to hold are extremely demanding, and virtually impossible to be met for common depths. We establish adaptations of these properties which prove to be still sensible, and more easily met by common functional depths.},
   author = {Irène Gijbels and Stanislav Nagy},
   doi = {10.1214/17-STS625},
   issn = {08834237},
   issue = {4},
   journal = {Statistical Science},
   keywords = {Data depth,Functional data,Multivariate statistics,Robustness},
   month = {11},
   pages = {630-639},
   publisher = {Institute of Mathematical Statistics},
   title = {On a general definition of depth for functional data},
   volume = {32},
   year = {2017}
}
@article{Dang2010,
   abstract = {In extending univariate outlier detection methods to higher dimension, various issues arise: limited visualization methods, inadequacy of marginal methods, lack of a natural order, limited parametric modeling, and, when using Mahalanobis distance, restriction to ellipsoidal contours. To address and overcome such limitations, we introduce nonparametric multivariate outlier identifiers based on multivariate depth functions, which can generate contours following the shape of the data set. Also, we study masking robustness, that is, robustness against misidentification of outliers as nonoutliers. In particular, we define a masking breakdown point (MBP), adapting to our setting certain ideas of Davies and Gather [1993. The identification of multiple outliers (with discussion). Journal of the American Statistical Association 88, 782-801] and Becker and Gather [1999. The masking breakdown point of multivariate outlier identification rules. Journal of the American Statistical Association 94, 947-955] based on the Mahalanobis distance outlyingness. We then compare four affine invariant outlier detection procedures, based on Mahalanobis distance, halfspace or Tukey depth, projection depth, and "Mahalanobis spatial" depth. For the goal of threshold type outlier detection, it is found that the Mahalanobis distance and projection procedures are distinctly superior in performance, each with very high MBP, while the halfspace approach is quite inferior. When a moderate MBP suffices, the Mahalanobis spatial procedure is competitive in view of its contours not constrained to be elliptical and its computational burden relatively mild. A small sampling experiment yields findings completely in accordance with the theoretical comparisons. While these four depth procedures are relatively comparable for the purpose of robust affine equivariant location estimation, the halfspace depth is not competitive with the others for the quite different goal of robust setting of an outlyingness threshold. © 2009 Elsevier B.V. All rights reserved.},
   author = {Xin Dang and Robert Serfling},
   doi = {10.1016/j.jspi.2009.07.004},
   issn = {03783758},
   issue = {1},
   journal = {Journal of Statistical Planning and Inference},
   keywords = {Depth functions,Multivariate analysis,Nonparametric,Outlier identification,Robust},
   month = {1},
   pages = {198-213},
   title = {Nonparametric depth-based multivariate outlier identifiers, and masking robustness properties},
   volume = {140},
   year = {2010}
}
@misc{Zimek2012,
   abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
   author = {Arthur Zimek and Erich Schubert and Hans Peter Kriegel},
   doi = {10.1002/sam.11161},
   issn = {19321864},
   issue = {5},
   journal = {Statistical Analysis and Data Mining},
   keywords = {Anomalies in high-dimensional data,Approximate outlier detection,Correlation outlier detection,Curse of dimensionality,Outlier detection in high-dimensional data,Subspace outlier detection},
   pages = {363-387},
   publisher = {John Wiley and Sons Inc},
   title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
   volume = {5},
   year = {2012}
}
@misc{AbGhani2023,
   abstract = {Clustering high dimensional data is challenging as data dimensionality increases the distance between data points, resulting in sparse regions that degrade clustering performance. Subspace clustering is a common approach for processing high-dimensional data by finding relevant features for each cluster in the data space. Subspace clustering methods extend traditional clustering to account for the constraints imposed by data streams. Data streams are not only high-dimensional, but also unbounded and evolving. This necessitates the development of subspace clustering algorithms that can handle high dimensionality and adapt to the unique characteristics of data streams. Although many articles have contributed to the literature review on data stream clustering, there is currently no specific review on subspace clustering algorithms in high-dimensional data streams. Therefore, this article aims to systematically review the existing literature on subspace clustering of data streams in high-dimensional streaming environments. The review follows a systematic methodological approach and includes 18 articles for the final analysis. The analysis focused on two research questions related to the general clustering process and dealing with the unbounded and evolving characteristics of data streams. The main findings relate to six elements: clustering process, cluster search, subspace search, synopsis structure, cluster maintenance, and evaluation measures. Most algorithms use a two-phase clustering approach consisting of an initialization stage, a refinement stage, a cluster maintenance stage, and a final clustering stage. The density-based top-down subspace clustering approach is more widely used than the others because it is able to distinguish true clusters and outliers using projected micro-clusters. Most algorithms implicitly adapt to the evolving nature of the data stream by using a time fading function that is sensitive to outliers. Future work can focus on the clustering framework, parameter optimization, subspace search techniques, memory-efficient synopsis structures, explicit cluster change detection, and intrinsic performance metrics. This article can serve as a guide for researchers interested in high-dimensional subspace clustering methods for data streams.},
   author = {Nur Laila Ab Ghani and Izzatdin Abdul Aziz and Said Jadid AbdulKadir},
   doi = {10.32604/cmc.2023.035987},
   issn = {15462226},
   issue = {2},
   journal = {Computers, Materials and Continua},
   keywords = {Clustering,concept drift,data stream,evolving data stream,high dimensionality,projected clustering,stream clustering,subspace clustering},
   pages = {4649-4668},
   publisher = {Tech Science Press},
   title = {Subspace Clustering in High-Dimensional Data Streams: A Systematic Literature Review},
   volume = {75},
   year = {2023}
}
@article{Kelling2009,
   abstract = {The increasing availability of massive volumes of scientific data requires new synthetic analysis techniques to explore and identify interesting patterns that are otherwise not apparent. For biodiversity studies, a "data-driven" approach is necessary because of the complexity of ecological systems, particularly when viewed at large spatial and temporal scales. Data-intensive science organizes large volumes of data from multiple sources and fields and then analyzes them using techniques tailored to the discovery of complex patterns in high-dimensional data through visualizations, simulations, and various types of model building. Through interpreting and analyzing these models, truly novel and surprising patterns that are "born from the data" can be discovered. These patterns provide valuable insight for concrete hypotheses about the underlying ecological processes that created the observed data. Data-intensive science allows scientists to analyze bigger and more complex systems efficiently, and complements more traditional scientific processes of hypothesis generation and experimental testing to refine our understanding of the natural world. © 2009 by American Institute of Biological Sciences.},
   author = {Steve Kelling and Wesley M. Hochachka and Daniel Fink and Mirek Riedewald and Rich Caruana and Grant Ballard and Giles Hooker},
   doi = {10.1525/bio.2009.59.7.12},
   issn = {00063568},
   issue = {7},
   journal = {BioScience},
   keywords = {Biodiversity,Data-intensive science,Informatics,Machine learning,Statistics},
   month = {7},
   pages = {613-620},
   title = {Data-intensive science: A new paradigm for biodiversity studies},
   volume = {59},
   year = {2009}
}
@article{Cuevas2007,
   abstract = {Five notions of data depth are considered. They are mostly designed for functional data but they can be also adapted to the standard multivariate case. The performance of these depth notions, when used as auxiliary tools in estimation and classification, is checked through a Monte Carlo study. © 2007 Springer-Verlag.},
   author = {Antonio Cuevas and Manuel Febrero and Ricardo Fraiman},
   doi = {10.1007/s00180-007-0053-0},
   issn = {09434062},
   issue = {3},
   journal = {Computational Statistics},
   keywords = {Depth measures,Functional data,Projections method,Supervised classification},
   month = {9},
   pages = {481-496},
   title = {Robust estimation and classification for functional data via projection-based depth notions},
   volume = {22},
   year = {2007}
}
@misc{Zimek2012,
   abstract = {High-dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term 'curse of dimensionality', more concrete aspects being the so-called 'distance concentration effect', the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high-dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high-dimensional data. In this survey article, we discuss some important aspects of the 'curse of dimensionality' in detail and survey specialized algorithms for outlier detection from both categories. © 2012 Wiley Periodicals, Inc.},
   author = {Arthur Zimek and Erich Schubert and Hans Peter Kriegel},
   doi = {10.1002/sam.11161},
   issn = {19321864},
   issue = {5},
   journal = {Statistical Analysis and Data Mining},
   keywords = {Anomalies in high-dimensional data,Approximate outlier detection,Correlation outlier detection,Curse of dimensionality,Outlier detection in high-dimensional data,Subspace outlier detection},
   pages = {363-387},
   publisher = {John Wiley and Sons Inc},
   title = {A survey on unsupervised outlier detection in high-dimensional numerical data},
   volume = {5},
   year = {2012}
}
@article{Cuevas2009,
   abstract = {A general depth measure, based on the use of one-dimensional linear continuous projections, is proposed. The applicability of this idea in different statistical setups (including inference in functional data analysis, image analysis and classification) is discussed. A special emphasis is made on the possible usefulness of this method in some statistical problems where the data are elements of a Banach space. The asymptotic properties of the empirical approximation of the proposed depth measure are investigated. In particular, its asymptotic distribution is obtained through U-statistics techniques. The practical aspects of these ideas are discussed through a small simulation study and a real-data example. © 2008 Elsevier Inc. All rights reserved.},
   author = {Antonio Cuevas and Ricardo Fraiman},
   doi = {10.1016/j.jmva.2008.08.002},
   issn = {0047259X},
   issue = {4},
   journal = {Journal of Multivariate Analysis},
   keywords = {62G07,62G20,Depth measures,Functional data,Projections method,Supervised classification,primary,secondary},
   month = {4},
   pages = {753-766},
   title = {On depth measures and dual statistics. A methodology for dealing with general data},
   volume = {100},
   year = {2009}
}
@techReport{,
   author = {Nazmul Kabir Sikder and Feras A Batarseh},
   title = {Outlier Detection using AI: A Survey}
}
@article{Evans2015,
   abstract = {In model-based clustering based on normal-mixture models, a few outlying observations can influence the cluster structure and number. This paper develops a method to identify these, however it does not attempt to identify clusters amidst a large field of noisy observations. We identify outliers as those observations in a cluster with minimal membership proportion or for which the cluster-specific variance with and without the observation is very different. Results from a simulation study demonstrate the ability of our method to detect true outliers without falsely identifying many non-outliers and improved performance over other approaches, under most scenarios. We use the contributed R package MCLUST for model-based clustering, but propose a modified prior for the cluster-specific variance which avoids degeneracies in estimation procedures. We also compare results from our outlier method to published results on National Hockey League data.},
   author = {Katie Evans and Tanzy Love and Sally W. Thurston},
   doi = {10.1007/s00357-015-9171-5},
   issn = {14321343},
   issue = {1},
   journal = {Journal of Classification},
   keywords = {Influential points,MCLUST,National Hockey League,Normal-mixture models,Prior},
   month = {4},
   pages = {63-84},
   publisher = {Springer New York LLC},
   title = {Outlier Identification in Model-Based Cluster Analysis},
   volume = {32},
   year = {2015}
}
@techReport{Xu2018,
   abstract = {Outlier detection is a hot topic in machine learning. With the newly emerging technologies and diverse applications, the interest of outlier detection is increasing greatly. Recently, a significant number of outlier detection methods have been witnessed and successfully applied in a wide range of fields, including medical health, credit card fraud and intrusion detection. They can be used for conventional data analysis. However, it is not a trivial work to identify rare behaviors or patterns out from complicated data. In this paper, we provide a brief overview of the outlier detection methods for high-dimensional data, and offer comprehensive understanding of the-state-of-the-art techniques of outlier detection for practitioners. Specifically, we firstly summarize the recent advances on outlier detection for high-dimensional data, and then make an extensive experimental comparison to the popular detection methods on public datasets. Finally, several challenging issues and future research directions are discussed.},
   author = {Xiaodan Xu and Huawen Liu and Li Li and Minghai Yao},
   keywords = {data mining,evaluation measurement,high-dimensional data,outlier detection},
   title = {A Comparison of Outlier Detection Techniques for High-Dimensional Data},
   year = {2018}
}
@techReport{,
   abstract = {The large proportion of irrelevant or noisy features in real-life high-dimensional data presents a significant challenge to subspace/feature selection-based high-dimensional out-lier detection (a.k.a. outlier scoring) methods. These methods often perform the two dependent tasks: relevant feature subset search and outlier scoring independently, consequently retaining features/subspaces irrelevant to the scoring method and downgrading the detection performance. This paper introduces a novel sequential ensemble-based framework SEMSE and its instance CINFO to address this issue. SEMSE learns the sequential ensembles to mutually refine feature selection and outlier scoring by iterative sparse modeling with outlier scores as the pseudo target feature. CINFO instantiates SEMSE by using three successive recurrent components to build such sequential ensembles. Given outlier scores output by an existing outlier scoring method on a feature subset , CINFO first defines a Cantelli's inequality-based out-lier thresholding function to select outlier candidates with a false positive upper bound. It then performs lasso-based sparse regression by treating the outlier scores as the target feature and the original features as predictors on the out-lier candidate set to obtain a feature subset that is tailored for the outlier scoring method. Our experiments show that two different outlier scoring methods enabled by CINFO (i) perform significantly better on 11 real-life high-dimensional data sets, and (ii) have much better resilience to noisy features , compared to their bare versions and three state-of-the-art competitors. The source code of CINFO is available at https://sites.google.com/site/gspangsite/sourcecode.},
   author = {Guansong Pang and Longbing Cao and Ling Chen and Defu Lian and Huan Liu},
   keywords = {Machine Learning Methods Track},
   title = {Sparse Modeling-Based Sequential Ensemble Learning for Effective Outlier Detection in High-Dimensional Numeric Data},
   url = {www.aaai.org}
}
@techReport{Chandola2009,
   abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
   author = {Varun Chandola},
   journal = {ACM Computing Surveys},
   keywords = {Algorithms Additional Key Words and Phrases,Anomaly Detection,Categories and Subject Descriptors,Database Applications-Data Mining General Terms,H28 [Database Management],Outlier Detection},
   title = {Anomaly Detection : A Survey},
   year = {2009}
}
@article{AsirAntonyGnanaSingh2016,
   abstract = {Reliability, lack of error, and security are important improvements to quality of service. Outlier detection is a process of detecting the erroneous parts or abnormal objects in defined populations, and can contribute to secured and error-free services. Outlier detection approaches can be categorized into four types: statistic-based, unsupervised, supervised, and semi-supervised. A model-based outlier detection system with statistical preprocessing is proposed, taking advantage of the statistical approach to preprocess training data and using unsupervised learning to construct the model. The robustness of the proposed system is evaluated using the performance evaluation metrics sum of squared error (SSE) and time to build model (TBM). The proposed system performs better for detecting outliers regardless of the application domain.},
   author = {D. Asir Antony Gnana Singh and E. Jebalamar Leavline},
   doi = {10.22237/jmasm/1462077480},
   issn = {15389472},
   issue = {1},
   journal = {Journal of Modern Applied Statistical Methods},
   keywords = {Anomaly detection,Inter-quartile range,Outlier,Preprocessing},
   pages = {789-801},
   publisher = {Wayne State University},
   title = {Model-based outlier detection system with statistical preprocessing},
   volume = {15},
   year = {2016}
}
@article{Chiou2014,
   abstract = {We propose an extended version of the classical Karhunen-Lòeve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
   author = {Jeng Min Chiou and Yu Ting Chen and Ya Fang Yang},
   doi = {10.5705/ss.2013.305},
   issn = {10170405},
   issue = {4},
   journal = {Statistica Sinica},
   keywords = {Karhunen-LòEve expansion,Mercer's theorem,Multivariate functional data,Normalization,Traffic flow},
   month = {10},
   pages = {1571-1596},
   publisher = {Institute of Statistical Science},
   title = {Multivariate functional principal component analysis: A normalization approach},
   volume = {24},
   year = {2014}
}
@article{James2003,
   abstract = {We develop a flexible model-based procedure for clustering functional data. The technique can be applied to all types of curve data but is particularly useful when individuals are observed at a sparse set of time points. In addition to producing final cluster assignments, the procedure generates predictions and confidence intervals for missing portions of curves. Our approach also provides many useful tools for evaluating the resulting models. Clustering can be assessed visually via low-dimensional representations of the curves, and the regions of greatest separation between clusters can be determined using a discriminant function. Finally, we extend the model to handle multiple functional and finite-dimensional covariates and show how it can be applied to standard finite-dimensional clustering problems involving missing data.},
   author = {Gareth M. James and Catherine A. Sugar},
   doi = {10.1198/016214503000189},
   issn = {01621459},
   issue = {462},
   journal = {Journal of the American Statistical Association},
   keywords = {Curve estimation,Discriminant functions,Functional clustering,High-dimensional data},
   month = {6},
   pages = {397-408},
   title = {Clustering for sparsely sampled functional data},
   volume = {98},
   year = {2003}
}
@inproceedings{Moallemi2021,
   abstract = {Modern Structural Health Monitoring (SHM) systems are becoming of pervasive use in civil engineering because they can track the structural condition and detect damages of critical and civil infrastructures such as buildings, viaducts, and tunnels. Although noticeable work has been done to improve anomaly detection for ensuring public safety, algorithms that can be executed on low-cost hardware for long-term monitoring are still an open issue to the community. This paper presents a new framework that exploits compression techniques to identify anomalies in the structure, avoiding continuous streaming of raw data to the cloud. We used a real installation on a bridge in Italy to test the proposed anomaly detection algorithm. We trained three compression models, namely a Principal Component Analysis (PCA), a fully-connected autoencoder, and a convolutional autoencoder. Performance comparison is also provided through an ablation study that analyzes the impact of various parameters. Results demonstrate that the model-based approach, i.e., PCA, can reach a better accuracy whereas data-driven models, i.e., autoencoders, are limited by training set size.},
   author = {Amirhossein Moallemi and Alessio Burrello and Davide Brunelli and Luca Benini},
   doi = {10.1109/I2MTC50364.2021.9459999},
   isbn = {9781728195391},
   issn = {10915281},
   booktitle = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
   keywords = {Anomaly detection,Compression Techniques,Deep Learning,Edge computing,Structural Health Monitoring},
   month = {5},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Model-based vs. Data-driven Approaches for Anomaly Detection in Structural Health Monitoring: A Case Study},
   volume = {2021-May},
   year = {2021}
}
@inproceedings{Klerx2014,
   abstract = {Model-based anomaly detection in technical systems is an important application field of artificial intelligence. We consider discrete event systems, which is a system class to which a wide range of relevant technical systems belong and for which no comprehensive model-based anomaly detection approach exists so far. The original contributions of this paper are threefold: First, we identify the types of anomalies that occur in discrete event systems and we propose a tailored behavior model that captures all anomaly types, called probabilistic deterministic timed-transition automata (PDTTA). Second, we present a new algorithm to learn a PDTTA from sample observations of a system. Third, we describe an approach to detect anomalies based on a learned PDTTA. An empirical evaluation in a practical application, namely ATM fraud detection, shows promising results.},
   author = {Timo Klerx and Maik Anderka and Hans Kleine Buning and Steffen Priesterjahn},
   doi = {10.1109/ICTAI.2014.105},
   isbn = {9781479965724},
   issn = {10823409},
   booktitle = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
   keywords = {ATM Fraud Detection,Automatic Model Generation,Discrete Event Systems,Model-based Anomaly Detection},
   month = {12},
   pages = {665-672},
   publisher = {IEEE Computer Society},
   title = {Model-Based Anomaly Detection for Discrete Event Systems},
   volume = {2014-December},
   year = {2014}
}
@inproceedings{Suhermi2024,
   abstract = {Over time, the increasing use of home appliances, driven by the industrial revolution, has significantly contributed to overall household energy consumption. The number of appliances and various environmental indicators can also impact energy usage. Therefore, it is important to understand how to optimize energy utilization and improve efficiency. Analyzing energy consumption presents statistical challenges due to the large size, high frequency, complexity, and noise in the data. We investigate the use of Functional Data Analysis (FDA) approaches to address these challenges. Unlike traditional methods that treat each observation as an individual variable contributing to the overall dimension, FDA considers the entire trajectory as a single data curve. In this paper, we provide a step-by-step analysis of functional regression models to quantify the relationship between household energy consumption and several environmental indicators. Our dataset consists of energy consumption recorded in real-time at 10-minute intervals from an observation house. We compare the model performance of our FDA models with linear regression, support vector machine, and random forest. Our empirical results show that functional regression effectively captures the dynamic effects of environmental conditions that vary over time and have the lowest root mean square error and mean absolute percentage error.},
   author = {Novri Suhermi and Rahida Rihhadatul Aisy},
   doi = {10.1109/IC3INA64086.2024.10732718},
   issn = {29945925},
   issue = {2024},
   booktitle = {International Conference on Computer, Control, Informatics and its Applications, IC3INA},
   keywords = {Energy Consumption,Functional Data Analysis,Functional Regression,Prediction},
   pages = {468-471},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Functional Data Analysis for Household Appliance Energy Consumption Prediction},
   year = {2024}
}
@techReport{,
   abstract = {In this paper, a new definition of distance-based outlier and an algorithm, called HilOut, designed to efficiently detect the top n outliers of a large and high-dimensional data set are proposed. Given an integer k, the weight of a point is defined as the sum of the distances separating it from its k nearest-neighbors. Outlier are those points scoring the largest values of weight. The algorithm HilOut makes use of the notion of space-filling curve to linearize the data set, and it consists of two phases. The first phase provides an approximate solution, within a rough factor, after the execution of at most d þ 1 sorts and scans of the data set, with temporal cost quadratic in d and linear in N and in k, where d is the number of dimensions of the data set and N is the number of points in the data set. During this phase, the algorithm isolates points candidate to be outliers and reduces this set at each iteration. If the size of this set becomes n, then the algorithm stops reporting the exact solution. The second phase calculates the exact solution with a final scan examining further the candidate outliers that remained after the first phase. Experimental results show that the algorithm always stops, reporting the exact solution, during the first phase after much less than d þ 1 steps. We present both an in-memory and disk-based implementation of the HilOut algorithm and a thorough scaling analysis for real and synthetic data sets showing that the algorithm scales well in both cases.},
   author = {Fabrizio Angiulli and Clara Pizzuti},
   keywords = {Index Terms-Outlier mining,space-filling curves},
   title = {Outlier Mining in Large High-Dimensional Data Sets}
}
@article{Pigoli2014,
   abstract = {A framework is developed for inference concerning the covariance operator of a functional random process, where the covariance operator itself is an object of interest for statistical analysis. Distances for comparing positive-definite covariance matrices are either extended or shown to be inapplicable to functional data. In particular, an infinite-dimensional analogue of the Procrustes size-and-shape distance is developed. Convergence of finite-dimensional approximations to the infinite-dimensional distance metrics is also shown. For inference, a Fréchet estimator of both the covariance operator itself and the average covariance operator is introduced. A permutation procedure to test the equality of the covariance operators between two groups is also considered. Additionally, the use of such distances for extrapolation to make predictions is explored. As an example of the proposed methodology, the use of covariance operators has been suggested in a philological study of cross-linguistic dependence as a way to incorporate quantitative phonetic information. It is shown that distances between languages derived from phonetic covariance functions can provide insight into the relationships between the Romance languages. © 2014 Biometrika Trust..},
   author = {Davide Pigoli and John A.D. Aston and Ian L. Dryden and Piercesare Secchi},
   doi = {10.1093/biomet/asu008},
   issn = {14643510},
   issue = {2},
   journal = {Biometrika},
   keywords = {Distance metric,Functional data analysis,Procrustes analysis,Shape analysis},
   pages = {409-422},
   publisher = {Oxford University Press},
   title = {Distances and inference for covariance operators},
   volume = {101},
   year = {2014}
}
@article{Nagy2016,
   abstract = {Several depths suitable for infinite-dimensional functional data that are available in the literature are of the form of an integral of a finite-dimensional depth function. These functionals are characterized by projecting functions into low-dimensional spaces, taking finite-dimensional depths of the projected quantities, and finally integrating these projected marginal depths over a preset collection of projections. In this paper, a general class of integrated depths for functions is considered. Several depths for functional data proposed in the literature during the last decades are members of this general class. A comprehensive study of its most important theoretical properties, including measurability and consistency, is given. It is shown that many, but not all, properties of the integrated depth are shared with the finite-dimensional depth that constitutes its building block. Some pending measurability issues connected with all integrated depth functionals are resolved, a broad new notion of symmetry for functional data is proposed, and difficulties with respect to consistency results are identified. A general universal consistency result for the sample depth version, and for the generalized median, for integrated depth for functions is derived.},
   author = {Stanislav Nagy and Irène Gijbels and Marek Omelka and Daniel Hlubinka},
   doi = {10.1051/ps/2016005},
   issn = {12623318},
   journal = {ESAIM - Probability and Statistics},
   keywords = {Center of symmetry,Functional data,Generalized median,Integrated depth,Measurability,Strong consistency,Weak consistency},
   pages = {95-130},
   publisher = {EDP Sciences},
   title = {Integrated depth for functional data: Statistical properties and consistency},
   volume = {20},
   year = {2016}
}
@article{Kuhnt2016,
   abstract = {A measure especially designed for detecting shape outliers in functional data is presented. It is based on the tangential angles of the intersections of the centred data and can be interpreted like a data depth. Due to its theoretical properties we call it functional tangential angle (FUNTA) pseudo-depth. Furthermore we introduce a robustification (rFUNTA). The existence of intersection angles is ensured through the centring. Assuming that shape outliers in functional data follow a different pattern, the distribution of intersection angles differs. Furthermore we formulate a population version of FUNTA in the context of Gaussian processes. We determine sample breakdown points of FUNTA and compare its performance with respect to outlier detection in simulation studies and a real data example.},
   author = {Sonja Kuhnt and André Rehage},
   doi = {10.1016/j.jmva.2015.10.016},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Bootstrap,Data depth,Functional data,Robust estimate,Shape outlier detection},
   month = {4},
   pages = {325-340},
   publisher = {Academic Press Inc.},
   title = {An angle-based multivariate functional pseudo-depth for shape outlier detection},
   volume = {146},
   year = {2016}
}
@article{Berrendero2011,
   abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets. © 2011 Elsevier B.V. All rights reserved.},
   author = {J. R. Berrendero and A. Justel and M. Svarc},
   doi = {10.1016/j.csda.2011.03.011},
   issn = {01679473},
   issue = {9},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Dimension reduction,Eigenvalue functions,Explained variability},
   month = {9},
   pages = {2619-2634},
   title = {Principal components for multivariate functional data},
   volume = {55},
   year = {2011}
}
@misc{Correia2024,
   abstract = {Time-series anomaly detection plays an important role in engineering processes, like development, manufacturing and other operations involving dynamic systems. These processes can greatly benefit from advances in the field, as state-of-the-art approaches may aid in cases involving, for example, highly dimensional data. To provide the reader with understanding of the terminology, this survey introduces a novel taxonomy where a distinction between online and offline, and training and inference is made. Additionally, it presents the most popular data sets and evaluation metrics used in the literature, as well as a detailed analysis. Furthermore, this survey provides an extensive overview of the state-of-the-art model-based online semi- and unsupervised anomaly detection approaches for multivariate time-series data, categorising them into different model families and other properties. The biggest research challenge revolves around benchmarking, as currently there is no reliable way to compare different approaches against one another. This problem is two-fold: on the one hand, public data sets suffers from at least one fundamental flaw, while on the other hand, there is a lack of intuitive and representative evaluation metrics in the field. Moreover, the way most publications choose a detection threshold disregards real-world conditions, which hinders the application in the real world. To allow for tangible advances in the field, these issues must be addressed in future work.},
   author = {Lucas Correia and Jan Christoph Goos and Philipp Klein and Thomas Bäck and Anna V. Kononova},
   doi = {10.1016/j.engappai.2024.109323},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Anomaly detection,Model-based,Multivariate,Online,Survey,Time series},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Online model-based anomaly detection in multivariate time series: Taxonomy, survey, research challenges and future directions},
   volume = {138},
   year = {2024}
}
@article{Liu2018,
   abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter $\{k\}$ of $\{k\}$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
   author = {Huawen Liu and Xuelong Li and Jiuyong Li and Shichao Zhang},
   doi = {10.1109/TSMC.2017.2718220},
   issn = {21682232},
   issue = {12},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
   keywords = {Dimension reduction,high-dimensional data,k nearest neighbors (kNN),low-rank approximation,outlier detection},
   month = {12},
   pages = {2451-2461},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Efficient Outlier Detection for High-Dimensional Data},
   volume = {48},
   year = {2018}
}
@article{Nieto-Reyes2016,
   abstract = {The main focus of this work is on providing a formal definition of statistical depth for functional data on the basis of six properties, recognising topological features such as continuity, smoothness and contiguity. Amongst our depth defining properties is one that addresses the delicate challenge of inherent partial observability of functional data, with fulfillment giving rise to a minimal guarantee on the performance of the empirical depth beyond the idealised and practically infeasible case of full observability. As an incidental product, functional depths satisfying our definition achieve a robustness that is commonly ascribed to depth, despite the absence of a formal guarantee in the multivariate definition of depth. We demonstrate the fulfillment or otherwise of our properties for six widely used functional depth proposals, thereby providing a systematic basis for selection of a depth function.},
   author = {Alicia Nieto-Reyes and Heather Battey},
   doi = {10.1214/15-STS532},
   issn = {08834237},
   issue = {1},
   journal = {Statistical Science},
   keywords = {Functional data,Multivariate statistics,Partial observability,Robustness,Statistical depth},
   pages = {61-79},
   publisher = {Institute of Mathematical Statistics},
   title = {A topologically valid definition of depth for functional data},
   volume = {31},
   year = {2016}
}
@article{Eiteneuer2020,
   abstract = {Anomaly detection is the task of detecting data which differs from the normal behaviour of a system in a given context. In order to approach this problem, data-driven models can be learned to predict current or future observations. Oftentimes, anomalous behaviour depends on the internal dynamics of the system and looks normal in a static context. To address this problem, the model should also operate depending on state. Long Short-Term Memory (LSTM) neural networks have been shown to be particularly useful to learn time sequences with varying length of temporal dependencies and are therefore an interesting general purpose approach to learn the behaviour of arbitrarily complex Cyber-Physical Systems. In order to perform anomaly detection, we slightly modify the standard norm 2 error to incorporate an estimate of model uncertainty. We analyse the approach on artificial and real data.},
   author = {Benedikt Eiteneuer and Oliver Niggemann},
   month = {10},
   title = {LSTM for Model-Based Anomaly Detection in Cyber-Physical Systems},
   url = {http://arxiv.org/abs/2010.15680},
   year = {2020}
}
@article{Liu2018,
   abstract = {How to tackle high dimensionality of data effectively and efficiently is still a challenging issue in machine learning. Identifying anomalous objects from given data has a broad range of real-world applications. Although many classical outlier detection or ranking algorithms have been witnessed during the past years, the high-dimensional problem, as well as the size of neighborhood, in outlier detection have not yet attracted sufficient attention. The former may trigger the distance concentration problem that the distances of observations in high-dimensional space tend to be indiscernible, whereas the latter requires appropriate values for parameters, making models high complex and more sensitive. To partially circumvent these problems, especially the high dimensionality, we introduce a concept called local projection score (LPS) to represent deviation degree of an observation to its neighbors. The LPS is obtained from the neighborhood information by the technique of low-rank approximation. The observation with high LPS is a promising candidate of outlier in high probability. Based on this notion, we propose an efficient and effective outlier detection algorithm, which is also robust to the parameter $\{k\}$ of $\{k\}$ nearest neighbors. Extensive evaluation experiments conducted on twelve public real-world data sets with five popular outlier detection algorithms show that the performance of the proposed method is competitive and promising.},
   author = {Huawen Liu and Xuelong Li and Jiuyong Li and Shichao Zhang},
   doi = {10.1109/TSMC.2017.2718220},
   issn = {21682232},
   issue = {12},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
   keywords = {Dimension reduction,high-dimensional data,k nearest neighbors (kNN),low-rank approximation,outlier detection},
   month = {12},
   pages = {2451-2461},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Efficient Outlier Detection for High-Dimensional Data},
   volume = {48},
   year = {2018}
}
@article{Li2019,
   abstract = {Analysis of high-dimensional data often suffers from the curse of dimensionality and the complicated correlation among dimensions. Dimension reduction methods often are used to alleviate these problems. Existing outlier detection methods based on dimension reduction usually only rely on reconstruction error to detect outlier or apply conventional outlier detection methods to the reduced data, which could deteriorate the performance of outlier detection as only considering part of the information from data. Few studies have been done to combine these two strategies to do outlier detection. In this paper, we proposed an outlier detection method based on Variational Autoencoder (VAE), which combines low-dimensional representation and reconstruction error to detect outliers. Specifically, we first model the data use VAE, then extract four outlier scores from VAE model, finally propose an ensemble method to combine the four outlier scores. The experiments conducted on six real-world datasets show that the proposed method performs better than or at least comparable to state of the art methods.},
   author = {Yongmou Li and Yijie Wang and Xingkong Ma},
   doi = {10.3233/IDA-184240},
   issn = {15714128},
   issue = {5},
   journal = {Intelligent Data Analysis},
   keywords = {Variational autoencoders,high-dimensional data,outlier detection},
   pages = {991-1002},
   publisher = {IOS Press},
   title = {Variational autoencoder-based outlier detection for high-dimensional data},
   volume = {23},
   year = {2019}
}
@article{Yeon2025,
   abstract = {Data depth is a powerful tool originally proposed to rank multivariate data from centre outward. In this context, one of the most archetypical depth notions is Tukey's halfspace depth. In the last few decades, notions of depth have also been proposed for functional data. However, a naive extension of Tukey's depth cannot handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data, which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at centre, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in the space of all square-integrable functions, a very general space of functions that include nonsmooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in detecting outliers of different types. Real data examples showcase the proposed depth.},
   author = {Hyemin Yeon and Xiongtao Dai and Sara Lopez-Pintado},
   doi = {10.1093/jrsssb/qkaf030},
   issn = {14679868},
   issue = {5},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {functional data analysis,functional rankings,infinite dimension,outlier detection,robust statistics},
   month = {11},
   pages = {1553-1575},
   publisher = {Oxford University Press},
   title = {Regularized halfspace depth for functional data},
   volume = {87},
   year = {2025}
}
@misc{Souiden2022,
   abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
   author = {Imen Souiden and Mohamed Nazih Omri and Zaki Brahmi},
   doi = {10.1016/j.cosrev.2022.100463},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data streams,High dimensional data,Outlier detection},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {A survey of outlier detection in high dimensional data streams},
   volume = {44},
   year = {2022}
}
@article{Li2024,
   abstract = {As an essential task in data mining, outlier detection identifies abnormal patterns in numerous applications, among which clustering-based outlier detection is one of the most popular methods for its effectiveness in detecting cluster-related outliers, especially in medical applications. This article presents an advanced method to extract cluster-based outliers by employing a scaled minimum spanning tree (MST) data structure and a new medoid selection method: 1. we compute a scaled MST and iteratively cut the current longest edge to obtain clusters; 2. we apply a new medoid selection method, considering the noise effect to improve the quality of cluster-based outlier identification. The experimental results on real-world data, including extensive medical corpora and other semantically meaningful datasets, demonstrate the wide applicability and outperforming metrics of the proposed method.},
   author = {Jia Li and Jiangwei Li and Chenxu Wang and Fons J. Verbeek and Tanja Schultz and Hui Liu},
   doi = {10.1088/2632-2153/ad2492},
   issn = {26322153},
   issue = {1},
   journal = {Machine Learning: Science and Technology},
   keywords = {clustering,data mining,machine learning,medical data,medoid selection,minimum spanning tree,outlier detection},
   month = {3},
   publisher = {Institute of Physics},
   title = {MS2OD: outlier detection using minimum spanning tree and medoid selection},
   volume = {5},
   year = {2024}
}
@article{Olteanu2023,
   abstract = {The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.},
   author = {Madalina Olteanu and Fabrice Rossi and Florian Yger},
   doi = {10.1016/j.neucom.2023.126634},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Anomaly detection,Meta-survey,Outlier detection},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Meta-survey on outlier and anomaly detection},
   volume = {555},
   year = {2023}
}
@misc{Mirzaie2023,
   abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
   author = {Mostafa Mirzaie and Behshid Behkamal and Mohammad Allahbakhsh and Samad Paydar and Elisa Bertino},
   doi = {10.1016/j.cosrev.2023.100554},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data quality,Data streams,Quality framework,Systematic literature review},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {State of the art on quality control for data streams: A systematic literature review},
   volume = {48},
   year = {2023}
}
@article{Zhang2024,
   abstract = {Outliers carry significant information to reflect an anomaly mechanism, so outlier detection facilitates relevant data mining. In terms of outlier detection, the classical approaches from distances apply to numerical data rather than nominal data, while the recent methods on basic rough sets deal with nominal data rather than numerical data. Aiming at wide outlier detection on numerical, nominal, and hybrid data, this paper investigates three-way neighborhood characteristic regions and corresponding fusion measurement to advance outlier detection. First, neighborhood rough sets are deepened via three-way decision, so they derive three-way neighborhood structures on model boundaries, inner regions, and characteristic regions. Second, the three-way neighborhood characteristic regions motivate the information fusion and weight measurement regarding all features, and thus, a multiple neighborhood outlier factor emerges to establish a new method of outlier detection; furthermore, a relevant outlier detection algorithm (called 3WNCROD, available at https://github. com/BELLoney/3WNCROD) is designed to comprehensively process numerical, nominal, and mixed data. Finally, the 3WNCROD algorithm is experimentally validated, and it generally outperforms 13 contrast algorithms to perform better for outlier detection.},
   author = {Xianyong Zhang and Zhong Yuan and Duoqian Miao},
   doi = {10.1109/TKDE.2023.3312108},
   issn = {15582191},
   issue = {5},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {Data mining,neighborhood rough sets,outlier detection,three-way decision,uncertainty measurement},
   month = {5},
   pages = {2082-2095},
   publisher = {IEEE Computer Society},
   title = {Outlier Detection Using Three-Way Neighborhood Characteristic Regions and Corresponding Fusion Measurement},
   volume = {36},
   year = {2024}
}
@misc{Souiden2022,
   abstract = {The rapid evolution of technology has led to the generation of high dimensional data streams in a wide range of fields, such as genomics, signal processing, and finance. The combination of the streaming scenario and high dimensionality is particularly challenging especially for the outlier detection task. This is due to the special characteristics of the data stream such as the concept drift, the limited time and space requirements, in addition to the impact of the well-known curse of dimensionality in high dimensional space. To the best of our knowledge, few studies have addressed these challenges simultaneously, and therefore detecting anomalies in this context requires a great deal of attention. The main objective of this work is to study the main approaches existing in the literature, to identify a set of comparison criteria, such as the computational cost and the interpretation of outliers, which will help us to reveal the different challenges and additional research directions associated with this problem. At the end of this study, we will draw up a summary report which summarizes the main limits identified and we will detail the different directions of research related to this issue in order to promote research for this community.},
   author = {Imen Souiden and Mohamed Nazih Omri and Zaki Brahmi},
   doi = {10.1016/j.cosrev.2022.100463},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data streams,High dimensional data,Outlier detection},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {A survey of outlier detection in high dimensional data streams},
   volume = {44},
   year = {2022}
}
@misc{Mirzaie2023,
   abstract = {These days, endless streams of data are generated by various sources such as sensors, applications, users, etc. Due to possible issues in sources, such as malfunctions in sensors, platforms, or communication, the generated data might be of low quality, and this can lead to wrong outcomes for the tasks that rely on these data streams. Therefore, controlling the quality of data streams has become increasingly significant. Many approaches have been proposed for controlling the quality of data streams, and hence, various research areas have emerged in this field. To the best of our knowledge, there is no systematic literature review of research papers within this field that comprehensively reviews approaches, classifies them, and highlights the challenges. In this paper, we present the state of the art in the area of quality control of data streams, and characterize it along four dimensions. The first dimension represents the goal of the quality analysis, which can be either quality assessment, or quality improvement. The second dimension focuses on the quality control method, which can be online, offline, or hybrid. The third dimension focuses on the quality control technique, and finally, the fourth dimension represents whether the quality control approach uses any contextual information (inherent, system, organizational, or spatiotemporal context) or not. We compare and critically review the related approaches proposed in the last two decades along these dimensions. We also discuss the open challenges and future research directions.},
   author = {Mostafa Mirzaie and Behshid Behkamal and Mohammad Allahbakhsh and Samad Paydar and Elisa Bertino},
   doi = {10.1016/j.cosrev.2023.100554},
   issn = {15740137},
   journal = {Computer Science Review},
   keywords = {Data quality,Data streams,Quality framework,Systematic literature review},
   month = {5},
   publisher = {Elsevier Ireland Ltd},
   title = {State of the art on quality control for data streams: A systematic literature review},
   volume = {48},
   year = {2023}
}
@inproceedings{Zhang2022,
   abstract = {There is no shortage of outlier detection (OD) algorithms in the literature, yet a vast body of them are designed for a single machine. With the increasing reality of already cloud-resident datasets comes the need for distributed OD techniques. This area, however, is not only understudied but also short of public-domain implementations for practical use. This paper aims to fill this gap: We design Sparx, a data-parallel OD algorithm suitable for shared-nothing infrastructures, which we specifically implement in Apache Spark. Through extensive experiments on three real-world datasets, with several billions of points and millions of features, we show that existing open-source solutions fail to scale up; either by large number of points or high dimensionality, whereas Sparx yields scalable and effective performance. To facilitate practical use of OD on modern-scale datasets, we open-source Sparx under the Apache license at https://tinyurl.com/sparx2022.},
   author = {Sean Zhang and Varun Ursekar and Leman Akoglu},
   doi = {10.1145/3534678.3539076},
   isbn = {9781450393850},
   booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Apache Spark,data-parallel algorithms,distributed outlier detection},
   month = {8},
   pages = {4530-4540},
   publisher = {Association for Computing Machinery},
   title = {Sparx: Distributed Outlier Detection at Scale},
   year = {2022}
}
@article{Chen2025,
   abstract = {Testing the departures from symmetry is a critical issue in statistics. Over the last two decades, substantial effort has been invested in developing tests for central symmetry in multivariate and high-dimensional contexts. Traditional tests, which rely on Euclidean distance, face significant challenges in high-dimensional data. These tests struggle to capture overall central symmetry and are often limited to verifying whether the distribution's center aligns with the coordinate origin, a problem exacerbated by the “curse of dimensionality.” Furthermore, they tend to be computationally intensive, often making them impractical for large datasets. To overcome these limitations, we propose a nonparametric test based on the random projected energy distance, extending the energy distance test through random projections. This method effectively reduces data dimensions by projecting high-dimensional data onto lower-dimensional spaces, with the randomness ensuring maximum preservation of information. Theoretically, as the number of random projections approaches infinity, the risk of power loss from inadequate directions is mitigated. Leveraging U-statistic theory, our test's asymptotic null distribution is standard normal, which holds true regardless of the data dimensionality relative to sample size, thus eliminating the need for re-sampling to determine critical values. For computational efficiency with large datasets, we adopt a divide-and-average strategy, partitioning the data into smaller blocks of size m. Within each block, the estimates of the random projected energy distance are normally distributed. By averaging these estimates across all blocks, we derive a test statistic that is asymptotically standard normal. This method significantly reduces computational complexity from quadratic to linear in sample size, enhancing the feasibility of our test for extensive data analysis. Through extensive numerical studies, we have demonstrated the robust empirical performance of our test in terms of size and power, affirming its practical utility in statistical applications for high-dimensional data.},
   author = {Bo Chen and Feifei Chen and Junxin Wang and Tao Qiu},
   doi = {10.1016/j.csda.2024.108123},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Energy distance,High dimension,Random projections,Symmetry test},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {An efficient and distribution-free symmetry test for high-dimensional data based on energy statistics and random projections},
   volume = {206},
   year = {2025}
}
@article{Elas2023,
   abstract = {Partially observed functional data are frequently encountered in applications and are the object of an increasing interest by the literature. We here address the problem of measuring the centrality of a datum in a partially observed functional sample. We propose an integrated functional depth for partially observed functional data, dealing with the very challenging case where partial observability can occur systematically on any observation of the functional dataset. In particular, differently from many techniques for partially observed functional data, we do not request that some functional datum is fully observed, nor we require that a common domain exist, where all of the functional data are recorded. Because of this, our proposal can also be used in those frequent situations where reconstructions methods and other techniques for partially observed functional data are inapplicable. By means of simulation studies, we demonstrate the very good performances of the proposed depth on finite samples. Our proposal enables the use of benchmark methods based on depths, originally introduced for fully observed data, in the case of partially observed functional data. This includes the functional boxplot, the outliergram and the depth versus depth classifiers. We illustrate our proposal on two case studies, the first concerning a problem of outlier detection in German electricity supply functions, the second regarding a classification problem with data obtained from medical imaging. Supplementary materials for this article are available online.},
   author = {Antonio Elías and Raúl Jiménez and Anna M. Paganoni and Laura M. Sangalli},
   doi = {10.1080/10618600.2022.2070171},
   issn = {15372715},
   issue = {2},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Classification of partially observed functional data,Functional boxplot,Functional depth,Functional outliers,Incomplete functional data,Robustness},
   pages = {341-352},
   publisher = {Taylor and Francis Ltd.},
   title = {Integrated Depths for Partially Observed Functional Data},
   volume = {32},
   year = {2023}
}
@techReport{,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger},
   title = {Springer Series in Statistics}
}
@article{Rigueira2025,
   abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
   author = {Xurxo Rigueira and David Olivieri and Maria Araujo and Angeles Saavedra and Maria Pazo},
   doi = {10.5281/zenodo.1},
   title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
   url = {https://doi.org/10.5281/zenodo.1},
   year = {2025}
}
@article{Tong2022,
   abstract = {The use of the multivariate contaminated normal (MCN) distribution in model-based clustering is recommended to cluster data characterized by mild outliers, the model can at the same time detect outliers automatically and produce robust parameter estimates in each cluster. However, one of the limitations of this approach is that it requires complete data, i.e. the MCN cannot be used directly on data with missing values. In this paper, we develop a framework for fitting a mixture of MCN distributions to incomplete data sets, i.e. data sets with some values missing at random. Parameter estimation is obtained using the expectation-conditional maximization algorithm—a variant of the expectation-maximization algorithm in which the traditional maximization steps are instead replaced by simpler conditional maximization steps. We perform a simulation study to compare the results of our model to a mixture of multivariate normal and Student’s t distributions for incomplete data. The simulation also includes a study on the effect of the percentage of missing data on the performance of the three algorithms. The model is then applied to the Automobile data set (UCI machine learning repository). The results show that, while the Student’s t distribution gives similar classification performance, the MCN works better in detecting outliers with a lower false positive rate of outlier detection. The performance of all the techniques decreases linearly as the percentage of missing values increases.},
   author = {Hung Tong and Cristina Tortora},
   doi = {10.1007/s11634-021-00476-1},
   issn = {18625355},
   issue = {1},
   journal = {Advances in Data Analysis and Classification},
   keywords = {Contaminated normal distribution,Data missing at random,Model-based clustering,Outliers},
   month = {3},
   pages = {5-30},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Model-based clustering and outlier detection with missing data},
   volume = {16},
   year = {2022}
}
@article{Fonvieille2023,
   abstract = {In recent decades, southern elephant seals (SES) have become a species of particular importance in ocean data acquisition. The scientific community has taken advantage of technological advances coupled with suitable SES biological traits to record numerous variables in challenging environments and to study interactions between SES and oceanographic features. In the context of big dataset acquisition, there is a growing need for methodological tools to analyze and extract key data features while integrating their complexity. Although much attention has been paid to study elephant seal foraging strategies, the continuity of their surrounding three-dimensional environments is seldom integrated. Knowledge gaps persist in understanding habitat use by SES, while the representativeness of a predator-based approach to understanding ecosystem structuring is still questioned. In this study, we explore SES habitat use by using a functional data analysis approach (FDA) to describe the foraging environment of five female elephant seals feeding in the Southwestern Atlantic Ocean. Functional principal component analysis followed by model-based clustering were applied to temperature and salinity (TS) profiles from Mercator model outputs to discriminate waters sharing similar thermohaline structures. Secondly, in situ TS profiles recorded by the SES were employed to determine the habitat visited within the range of potential environments identified from the model data. Four Functional Oceanographic Domains (FOD) were identified in the Brazil-Malvinas Confluence, all visited, in varying proportion, by four of the five females studied. We found that the females favored areas where all the FODs converge and mix, generating thermal fronts and eddies. Prey-capture attempts increased in such areas. Our results are in accordance with previous findings, suggesting that (sub-)mesoscale features act as biological hotspots. This study highlights the potential of coupling FDA with model-based clustering for describing complex environments with minimal loss of information. As well as contributing to better understanding of elephant seal habitat use and foraging strategies, this approach opens up a wide range of applications in oceanography and ecology.},
   author = {Nadège Fonvieille and Christophe Guinet and Martin Saraceno and Baptiste Picard and Martin Tournier and Pauline Goulet and Claudio Campagna and Julieta Campagna and David Nerini},
   doi = {10.1016/j.pocean.2023.103120},
   issn = {00796611},
   journal = {Progress in Oceanography},
   keywords = {Brazil-Malvinas confluence,Functional Data Analysis,Habitat use,Model-based clustering,Southern elephant seals},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {Swimming in an ocean of curves: A functional approach to understanding elephant seal habitat use in the Argentine Basin},
   volume = {218},
   year = {2023}
}
@article{Rigueira2025,
   abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
   author = {Xurxo Rigueira and David Olivieri and Maria Araujo and Angeles Saavedra and Maria Pazo},
   doi = {10.5281/zenodo.1},
   title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
   url = {https://doi.org/10.5281/zenodo.1},
   year = {2025}
}
@article{Pronello2023,
   abstract = {High dimensional data, large-scale data, imaging and manifold data are all fostering new frontiers of statistics. These type of data are commonly considered in Functional Data Analysis where they are viewed as infinite-dimensional random vectors in a functional space. The rapid development of new technologies has generated a flow of complex data that have led to the development of new modeling strategies by scientists. In this paper, we basically deal with the problem of clustering a set of complex functional data into homogeneous groups. Working in a mixture model-based framework, we develop a flexible clustering technique achieving dimensionality reduction schemes through an L1 penalization. The proposed procedure results in an integrated modelling approach where shrinkage techniques are applied to enable sparse solutions in both the means and the covariance matrices of the mixture components, while preserving the underlying clustering structure. This leads to an entirely data-driven methodology suitable for simultaneous dimensionality reduction and clustering. The proposed methodology is evaluated through a Monte Carlo simulation study and an empirical analysis of real-world datasets showing different degrees of complexity.},
   author = {Nicola Pronello and Rosaria Ignaccolo and Luigi Ippoliti and Sara Fontanella},
   doi = {10.1007/s11222-023-10288-2},
   issn = {15731375},
   issue = {6},
   journal = {Statistics and Computing},
   keywords = {Functional zoning,Manifold data,Mixture models,Shape analysis,Spatial clustering,Surface data},
   month = {12},
   publisher = {Springer},
   title = {Penalized model-based clustering of complex functional data},
   volume = {33},
   year = {2023}
}
@article{Rigueira2025,
   abstract = {A R T I C L E I N F O Dataset link: https://doi.org/10.5281/zenodo.1 4769545, https://doi.org/10.5281/zenodo.147 69551 Keywords: Water quality Sensor data Functional data analysis Supervised machine learning Anomaly detection A B S T R A C T Reliable anomaly detection is crucial for water resources management, but the complexity of environmental sensor data presents challenges, especially with limited labeled data in water quality analysis. Functional data has experienced significant growth in anomaly detection, but most applications focus on unlabeled datasets. This study assesses the performance of multivariate functional data analysis and compares it with current machine learning models for detecting water quality anomalies on 18 years of expert-annotated data from four monitoring stations along Spain's Ebro River. We propose and validate a multivariate functional model incorporating a new amplitude metric and a nonparametric outlier detector (Multivariate Magnitude, Shape, and Amplitude-MMSA). Additionally, a Random Forest-based machine learning architecture was developed for the same purpose, employing sliding windows and data balancing techniques. The Random Forest model demonstrated the highest performance, achieving an average F1 score of 93%, while MMSA exhibited robustness in scenarios with limited anomalous data or labels.},
   author = {Xurxo Rigueira and David Olivieri and Maria Araujo and Angeles Saavedra and Maria Pazo},
   doi = {10.5281/zenodo.1},
   title = {Multivariate functional data analysis and machine learning methods for anomaly detection in water quality sensor data},
   url = {https://doi.org/10.5281/zenodo.1},
   year = {2025}
}
@article{Claeskens2014,
   abstract = {This article defines and studies a depth for multivariate functional data. By the multivariate nature and by including a weight function, it acknowledges important characteristics of functional data, namely differences in the amount of local amplitude, shape, and phase variation. We study both population and finite sample versions. The multivariate sample of curves may include warping functions, derivatives, and integrals of the original curves for a better overall representation of the functional data via the depth.We present a simulation study and data examples that confirm the good performance of this depth function. Supplementary materials for this article are available online. © 2014 American Statistical Association.},
   author = {Gerda Claeskens and Mia Hubert and Leen Slaets and Kaveh Vakili},
   doi = {10.1080/01621459.2013.856795},
   issn = {1537274X},
   issue = {505},
   journal = {Journal of the American Statistical Association},
   keywords = {Functional data,Multivariate data,Statistical depth,Time warping},
   pages = {411-423},
   publisher = {American Statistical Association},
   title = {Multivariate functional halfspace depth},
   volume = {109},
   year = {2014}
}
@article{Zheng2022,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@article{Jimnez-Varn2024,
   abstract = {Data depth is an efficient tool for robustly summarizing the distribution of functional data and detecting potential magnitude and shape outliers. Commonly used functional data depth notions, such as the modified band depth and extremal depth, are estimated from pointwise depth for each observed functional observation. However, these techniques require calculating one single depth value for each functional observation, which may not be sufficient to characterize the distribution of the functional data and detect potential outliers. This article presents an innovative approach to make the best use of pointwise depth. We propose using the pointwise depth distribution for magnitude outlier visualization and the correlation between pairwise depth for shape outlier detection. Furthermore, a bootstrap-based testing procedure has been introduced for the correlation to test whether there is any shape outlier. The proposed univariate methods are then extended to bivariate functional data. The performance of the proposed methods is examined and compared to conventional outlier detection techniques by intensive simulation studies. In addition, the developed methods are applied to simulated solar energy datasets from a photovoltaic system. Results revealed that the proposed method offers superior detection performance over conventional techniques. These findings will benefit engineers and practitioners in monitoring photovoltaic systems by detecting unnoticed anomalies and outliers.},
   author = {Cristian F. Jiménez-Varón and Fouzi Harrou and Ying Sun},
   doi = {10.1002/env.2851},
   issn = {1099095X},
   issue = {5},
   journal = {Environmetrics},
   keywords = {data depth,functional data,magnitude outliers,pairwise depth,pointwise depth,shape outliers,visualization},
   month = {8},
   publisher = {John Wiley and Sons Ltd},
   title = {Pointwise data depth for univariate and multivariate functional outlier detection},
   volume = {35},
   year = {2024}
}
@article{Happ2018,
   abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen–Loève Theorem. For the practically relevant case of a finite Karhunen–Loève representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
   author = {Clara Happ and Sonja Greven},
   doi = {10.1080/01621459.2016.1273115},
   issn = {1537274X},
   issue = {522},
   journal = {Journal of the American Statistical Association},
   keywords = {Dimension reduction,Functional data analysis,Image analysis,Multivariate functional data},
   month = {4},
   pages = {649-659},
   publisher = {American Statistical Association},
   title = {Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains},
   volume = {113},
   year = {2018}
}
@techReport{,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger},
   title = {Springer Series in Statistics}
}
@article{Maturo2024,
   abstract = {Diversity is vital across various fields like ecology, business, and medicine. From a statistical standpoint, determining diversity presents consistent methodological hurdles, regardless of the specific context. For instance, in ecology, while biodiversity is widely acknowledged as beneficial for ecosystems, there is no universally accepted measure due to diversity’s multidimensional nature. Recent research has introduced functional data analysis to address diversity profiles, which are inherently complex and multidimensional. However, a notable limitation is the need for a precise strategy to identify anomalous ecological communities. This study proposes a novel approach to biodiversity assessment using a functional outlier detection system by extending the functional box plot and outliergram to the context of suitable transformations of Hill’s numbers. This research holds significance in identifying early warning signs preceding biodiversity loss and the presence of potential pollutants or invasive species in ecological communities.},
   author = {Fabrizio Maturo and Annamaria Porreca},
   doi = {10.1007/s13253-024-00648-4},
   issn = {15372693},
   journal = {Journal of Agricultural, Biological, and Environmental Statistics},
   keywords = {Biodiversity,Diversity,FDA,Functional outlier detection,Hill’s numbers},
   publisher = {Springer},
   title = {Environmental Loss Assessment via Functional Outlier Detection of Transformed Biodiversity Profiles},
   year = {2024}
}
@article{Zheng2022,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@techReport{,
   abstract = {Recent advances in cDNA and oligonucleotide DNA arrays have made it possible to measure the abundance of mRNA transcripts for many genes simultaneously. The analysis of such experiments is nontrivial because of large data size and many levels of variation introduced at different stages of the experiments. The analysis is further complicated by the large differences that may exist among different probes used to interrogate the same gene. However, an attractive feature of high-density oligonucleotide arrays such as those produced by photolithography and inkjet technology is the standardization of chip manufacturing and hybridization process. As a result, probe-specific biases, although significant, are highly reproducible and predictable, and their adverse effect can be reduced by proper modeling and analysis methods. Here, we propose a statistical model for the probe-level data, and develop model-based estimates for gene expression indexes. We also present model-based methods for identifying and handling cross-hybridizing probes and contaminating array regions. Applications of these results will be presented elsewhere.},
   author = {Cheng Li and Wing Hung Wong},
   title = {Model-based analysis of oligonucleotide arrays: Expression index computation and outlier detection},
   url = {www.pnas.orgcgidoi10.1073pnas.011404098}
}
@article{Zheng2022,
   abstract = {The term of Curse of Dimensionality implicitly expresses the challenge for anomaly detection in a high-dimensional space. Because the distribution of anomalies in the high-dimensional spatial data is usually too sparse to provide sufficient information for detecting anomalies. In addition, irrelevant attributes may be seen as noise in the input data, which masks the true anomalies, so that it is difficult to choose a subspace of the input data that highlights the relevant attributes. In this case, the task becomes even harder if one aims at learning a compact boundary to distinguish anomalies from normal data. To address this issue, we proposed a detection method using the combination of an autoencoder and a hypersphere. In addition, an angle kernel and a radius kernel are also derived in order to learn a compact boundary of distinguishing anomalous and normal instances. Results show that our method outperforms the state-of-the-art detection methods in anomalous detection accuracy and the ability of learning a compact boundary. Moreover, our method also addresses the issue of blurred boundary in searching normal data in high dimensional dataset and when the information is insufficient due to a limited number of potential anomalies. We find that the measurement of angle similarity between data points during searching gains more advantages for learning a compact boundary than using the measurement of distance similarity. Since angle similarity is not only helpful for flexibly controlling search in normal data region, but also tightens the searched region of anomalies nearby the boundary. We also find that noise in data as a negative factor can deteriorate detection accuracy much more quickly than dimensionality does. Our findings indicate that the determination of hypersphere radius relies more on data dimensionality in a high-dimensional space than that in a low-dimensional space. However, in a low-dimensional space the radius is more likely correlated with data volume.},
   author = {Jian Zheng and Hongchun Qu and Zhaoni Li and Lin Li and Xiaoming Tang},
   doi = {10.1016/j.asoc.2022.109146},
   issn = {15684946},
   journal = {Applied Soft Computing},
   month = {8},
   pages = {109146},
   publisher = {Elsevier BV},
   title = {A deep hypersphere approach to high-dimensional anomaly detection},
   volume = {125},
   year = {2022}
}
@article{Nagy2017,
   abstract = {A major drawback of many established depth functionals is their ineffectiveness in identifying functions outlying merely in shape. Herein, a simple modification of functional depth is proposed to provide a remedy for this difficulty. The modification is versatile, widely applicable, and introduced without imposing any assumptions on the data, such as differentiability. It is shown that many favorable attributes of the original depths for functions, including consistency properties, remain preserved for the modified depths. The powerfulness of the new approach is demonstrated on a number of examples for which the known depths fail to identify the outlying functions. Supplementary material for this article is available online.},
   author = {Stanislav Nagy and Irène Gijbels and Daniel Hlubinka},
   doi = {10.1080/10618600.2017.1336445},
   issn = {15372715},
   issue = {4},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Data depth,Functional data,Infimal depth,Integrated depth,Outlying functions,Shape outliers},
   month = {10},
   pages = {883-893},
   publisher = {American Statistical Association},
   title = {Depth-Based Recognition of Shape Outlying Functions},
   volume = {26},
   year = {2017}
}
@article{Li2020,
   abstract = {Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.},
   author = {Xiaojie Li and Jiancheng Lv and Zhang Yi},
   doi = {10.1109/TCYB.2018.2876615},
   issn = {21682275},
   issue = {5},
   journal = {IEEE Transactions on Cybernetics},
   keywords = {Discrimination,outlier detection,outlier factor,structural scores},
   month = {5},
   pages = {2302-2310},
   pmid = {30418896},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Outlier Detection Using Structural Scores in a High-Dimensional Space},
   volume = {50},
   year = {2020}
}
@article{Todorov2024,
   abstract = {In the last few years, the number of R packages implementing different robust statistical methods have increased substantially. There are now numerous packages for computing robust multivariate location and scatter, robust multivariate analysis like principal components and discriminant analysis, robust linear models, and other algorithms dedicated to cope with outliers and other irregularities in the data. This abundance of package options may be overwhelming for both beginners and more experienced R users. Here we provide an overview of the most important 25 R packages for different tasks. As metrics for the importance of each package, we consider its maturity and history, the number of total and average monthly downloads from CRAN (The Comprehensive R Archive Network), and the number of reverse dependencies. Then we briefly describe what each of these package does. After that we elaborate on the several above-mentioned topics of robust statistics, presenting the methodology and the implementation in R and illustrating the application on real data examples. Particular attention is paid to the robust methods and algorithms suitable for high-dimensional data. The code for all examples is accessible on the GitHub repository https://github.com/valentint/robust-R-ecosystem-WIREs.},
   author = {Valentin Todorov},
   doi = {10.1002/wics.70007},
   issn = {19390068},
   issue = {6},
   journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
   keywords = {R,high dimensions,multivariate,outlier,robust},
   month = {11},
   publisher = {John Wiley and Sons Inc},
   title = {The R Package Ecosystem for Robust Statistics},
   volume = {16},
   year = {2024}
}
@techReport{,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger},
   title = {Springer Series in Statistics}
}
@article{Zhu2023,
   abstract = {Traditional outlier detections are inadequate for high-dimensional data analysis due to the interference of distance tending to be concentrated ('curse of dimensionality'). Inspired by the Coulomb's law, we propose a new high-dimensional data similarity measure vector, which consists of outlier Coulomb force and outlier Coulomb resultant force. Outlier Coulomb force not only effectively gauges similarity measures among data objects, but also fully reflects differences among dimensions of data objects by vector projection in each dimension. More importantly, Coulomb resultant force can effectively measure deviations of data objects from a data center, making detection results interpretable. We introduce a new neighborhood outlier factor, which drives the development of a high-dimensional outlier detection algorithm. In our approach, attribute values with a high deviation degree is treated as interpretable information of outlier data. Finally, we implement and evaluate our algorithm using the UCI and synthetic datasets. Our experimental results show that the algorithm effectively alleviates the interference of 'Curse of Dimensionality'. The findings confirm that high-dimensional outlier data originated by the algorithm are interpretable.},
   author = {Pengyun Zhu and Chaowei Zhang and Xiaofeng Li and Jifu Zhang and Xiao Qin},
   doi = {10.1109/TKDE.2022.3172167},
   issn = {15582191},
   issue = {6},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   keywords = {High-dimensional outlier detection,local outlier coulomb force,neighborhood outlier factor,outlier coulomb resultant force,similarity metric},
   month = {6},
   pages = {5506-5520},
   publisher = {IEEE Computer Society},
   title = {A High-Dimensional Outlier Detection Approach Based on Local Coulomb Force},
   volume = {35},
   year = {2023}
}
@article{Welbaum2025,
   abstract = {Misalignment often occurs in functional data and can severely impact their clustering results. A clustering algorithm for misaligned functional data is developed, by adapting the original mean shift algorithm in the Euclidean space. This mean shift algorithm is applied to the quotient space of the orbits of the square root velocity functions induced by the misaligned functional data, in which the elastic distance is equipped. Convergence properties of this algorithm are studied. The efficacy of the algorithm is demonstrated through simulations and various real data applications.},
   author = {Andrew Welbaum and Wanli Qiao},
   doi = {10.1016/j.csda.2024.108107},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Clustering,Elastic distance,Functional data,Gradient ascent,Mean shift,Misalignment},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Mean shift-based clustering for misaligned functional data},
   volume = {206},
   year = {2025}
}
@article{Ruff2021,
   abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
   author = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Grégoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus-Robert Müller},
   doi = {10.1109/JPROC.2021.3052449},
   month = {2},
   title = {A Unifying Review of Deep and Shallow Anomaly Detection},
   url = {http://arxiv.org/abs/2009.11732 http://dx.doi.org/10.1109/JPROC.2021.3052449},
   year = {2021}
}
@article{Dietzel2024,
   abstract = {Robust, quantitative understanding of the diverse ecological needs of species is needed to inform effective biodiversity conservation, now and in the future, but is lacking for most species. The advent of “big data” in ecology presents unprecedented opportunities to fill this gap and to disentangle the diverse drivers of biodiversity. Variable and model selection in sparse (small sample sizes for most species), high-dimensional (large pool of candidate predictors) problems is, however, non-trivial. Here, we employ cross-validated Bayesian projection predictive variable selection and shrinkage priors to identify, from a list of 70 ecological and biophysical candidate predictor variables, the minimal subset that best predicts the habitat preferences and distributions of 103 species of amphibians, birds, butterflies, dragonflies, and grasshoppers using the city of Zurich, Switzerland, as a case study. We contrast the predictive performance and ecological inference of models fit with the full set of predictors using shrinkage priors (exhaustive models) to models fit with a limited number of predictors obtained by compiling predictors from the full list of predictors using weakly informative priors (selective models). We show that exhaustive models excel in predictive performance, albeit at the cost of greater model complexity compared to selective models. Results from the selective models reveal the importance of access to aquatic habitat for a wide range of taxa, relative to other drivers such as urbanisation, vegetation and environmental hazards. These results are complemented by more nuanced insights from the exhaustive models into the importance of specific types of aquatic habitat (ponds, lakes, streams) and vegetation (herb, shrub, canopy cover) for the distribution of urban biodiversity, as well as the different spatial scales at which drivers are of predictive relevance. Our findings demonstrate the potential of shrinkage-based Bayesian variable selection to leverage big ecological data for species distribution modelling, and contribute to the development of concrete guidelines for urban planning and infrastructure design that account for biodiversity conservation.},
   author = {Andreas Dietzel and Marco Moretti and Lauren M. Cook},
   doi = {10.1016/j.ecoinf.2024.102561},
   issn = {15749541},
   journal = {Ecological Informatics},
   keywords = {Bayesian projection predictive variable selection,Blue-green infrastructure,Nature-based solutions,Shrinkage prior,Species distribution model,Urban biodiversity},
   month = {7},
   publisher = {Elsevier B.V.},
   title = {Shrinkage-based Bayesian variable selection for species distribution modelling in complex environments: An application to urban biodiversity},
   volume = {81},
   year = {2024}
}
@techReport{,
   abstract = {Utilizing auxiliary outlier datasets to regularize the machine learning model has demonstrated promise for out-of-distribution (OOD) detection and safe prediction. Due to the labor intensity in data collection and cleaning, automating outlier data generation has been a long-desired alternative. Despite the appeal, generating photo-realistic outliers in the high dimensional pixel space has been an open challenge for the field. To tackle the problem, this paper proposes a new framework DREAM-OOD, which enables imagining photo-realistic outliers by way of diffusion models, provided with only the in-distribution (ID) data and classes. Specifically, DREAM-OOD learns a text-conditioned latent space based on ID data, and then samples outliers in the low-likelihood region via the latent, which can be decoded into images by the diffusion model. Different from prior works [1, 2], DREAM-OOD enables visualizing and understanding the imagined outliers, directly in the pixel space. We conduct comprehensive quantitative and qualitative studies to understand the efficacy of DREAM-OOD, and show that training with the samples generated by DREAM-OOD can benefit OOD detection performance. Code is publicly available at https://github.com/deeplearning-wisc/dream-ood.},
   author = {Xuefeng Du and Yiyou Sun and Xiaojin Zhu and Yixuan Li},
   title = {Dream the Impossible: Outlier Imagination with Diffusion Models},
   url = {https://github.com/deeplearning-wisc/dream-ood.}
}
@article{Alemn-Gmez2022,
   abstract = {Functional magnetic resonance imaging (fMRI) is a non-invasive technique that facilitates the study of brain activity by measuring changes in blood flow. Brain activity signals can be recorded during the alternate performance of given tasks, that is, task fMRI (tfMRI), or during resting-state, that is, resting-state fMRI (rsfMRI), as a measure of baseline brain activity. This contributes to the understanding of how the human brain is organized in functionally distinct subdivisions. fMRI experiments from high-resolution scans provide hundred of thousands of longitudinal signals for each individual, corresponding to brain activity measurements over each voxel of the brain along the duration of the experiment. In this context, we propose novel visualization techniques for high-dimensional functional data relying on depth-based notions that enable computationally efficient 2-dim representations of fMRI data, which elucidate sample composition, outlier presence, and individual variability. We believe that this previous step is crucial to any inferential approach willing to identify neuroscientific patterns across individuals, tasks, and brain regions. We present the proposed technique via an extensive simulation study, and demonstrate its application on a motor and language tfMRI experiment.},
   author = {Yasser Alemán-Gómez and Ana Arribas-Gil and Manuel Desco and Antonio Elías and Juan Romo},
   doi = {10.1002/sim.9342},
   issn = {10970258},
   issue = {11},
   journal = {Statistics in Medicine},
   keywords = {FMRI,data visualization,dimensionality reduction,functional depth,multidimensional outliers},
   month = {5},
   pages = {2005-2024},
   pmid = {35118686},
   publisher = {John Wiley and Sons Ltd},
   title = {Depthgram: Visualizing outliers in high-dimensional functional data with application to fMRI data exploration},
   volume = {41},
   year = {2022}
}
@article{Diquigiovanni2022,
   abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.},
   author = {Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
   doi = {10.1016/j.jmva.2021.104879},
   issn = {10957243},
   journal = {Journal of Multivariate Analysis},
   keywords = {Conformal Prediction,Distribution-free prediction set,Exact prediction set,Finite-sample prediction set,Functional data,Prediction band},
   month = {5},
   publisher = {Academic Press Inc.},
   title = {Conformal prediction bands for multivariate functional data},
   volume = {189},
   year = {2022}
}
@article{Amovin-Assagba2022,
   abstract = {In an industrial context, the activity of sensors is recorded at a high frequency. A challenge is to automatically detect abnormal measurement behavior. Considering the sensor measures as functional data, the problem can be formulated as the detection of outliers in a multivariate functional data set. Due to the heterogeneity of this data set, the proposed contaminated mixture model both clusters the multivariate functional data into homogeneous groups and detects outliers. The main advantage of this procedure over its competitors is that it does not require to specify the proportion of outliers. Model inference is performed through an Expectation-Conditional Maximization algorithm, and the BIC is used to select the number of clusters. Numerical experiments on simulated data demonstrate the high performance achieved by the inference algorithm. In particular, the proposed model outperforms the competitors. Its application on the real data which motivated this study allows to correctly detect abnormal behaviors.},
   author = {Martial Amovin-Assagba and Irène Gannaz and Julien Jacques},
   doi = {10.1016/j.csda.2022.107496},
   issn = {01679473},
   journal = {Computational Statistics and Data Analysis},
   keywords = {Contaminated Gaussian mixture,EM algorithm,Model-based clustering,Multivariate functional data,Outlier detection},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {Outlier detection in multivariate functional data through a contaminated mixture model},
   volume = {174},
   year = {2022}
}
@article{Ojo2023,
   abstract = {We present definitions and properties of the fast massive unsupervised outlier detection (FastMUOD) indices, used for outlier detection (OD) in functional data. FastMUOD detects outliers by computing, for each curve, an amplitude, magnitude, and shape index meant to target the corresponding types of outliers. Some methods adapting FastMUOD to outlier detection in multivariate functional data are then proposed. These include applying FastMUOD on the components of the multivariate data and using random projections. Moreover, these techniques are tested on various simulated and real multivariate functional datasets. Compared with the state of the art in multivariate functional OD, the use of random projections showed the most effective results with similar, and in some cases improved, OD performance. Based on the proportion of random projections that flag each multivariate function as an outlier, we propose a new graphical tool, the magnitude-shape-amplitude (MSA) plot, useful for visualizing the magnitude, shape and amplitude outlyingness of multivariate functional data.},
   author = {Oluwasegun Taiwo Ojo and Antonio Fernández Anta and Marc G. Genton and Rosa E. Lillo},
   doi = {10.1002/sta4.567},
   issn = {20491573},
   issue = {1},
   journal = {Stat},
   keywords = {FastMUOD,functional data,functional outlier detection,multivariate functional data,outlier classification,video data},
   month = {1},
   publisher = {John Wiley and Sons Inc},
   title = {Multivariate functional outlier detection using the fast massive unsupervised outlier detection indices},
   volume = {12},
   year = {2023}
}
@article{Porreca2025,
   abstract = {Diversity is fundamental in many disciplines, such as ecology, business, biology, and medicine. From a statistical perspective, calculating a measure of diversity, whatever the context of reference, always poses the same methodological challenges. For example, in the ecological field, although biodiversity is widely recognised as a positive element of an ecosystem, and there are decades of studies in this regard, there is no consensus measure to evaluate it. The problem is that diversity is a complex, multidimensional, and multivariate concept. Limiting to the idea of diversity as variety, recent studies have presented functional data analysis to deal with diversity profiles and their inherently high-dimensional nature. A limitation of this recent research is that the identification of anomalies currently still focuses on univariate measures of biodiversity. This study proposes an original approach to identifying anomalous patterns in environmental communities’ biodiversity by leveraging functional boxplots and functional clustering. The latter approaches are implemented to standardised and normalised Hill’s numbers treating them as functional data and Hill’s numbers integral functions. Each of these functional transformations offers a peculiar and exciting point of view and interpretation. This research is valuable for identifying warning signs that precede pathological situations of biodiversity loss and the presence of possible pollutants.},
   author = {Annamaria Porreca and Fabrizio Maturo},
   doi = {10.1007/s11135-024-01876-z},
   issn = {15737845},
   issue = {3},
   journal = {Quality and Quantity},
   keywords = {Biodiversity,Diversity,FDA,Functional outlier detection,Hill’s numbers,Normalized Hill’s functions,Standardized Hill’s functions},
   month = {6},
   pages = {2025-2052},
   publisher = {Springer Science and Business Media B.V.},
   title = {Identifying anomalous patterns in ecological communities’ diversity: leveraging functional boxplots and clustering of normalized Hill’s numbers and their integral functions},
   volume = {59},
   year = {2025}
}
@techReport{,
   abstract = {Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together. All the implementations and benchmarks reported in the paper can be found at :},
   author = {Mohammadreza Salehi and Salehidehnavi@uva Nl and Hossein Mirzaei and Dan Hendrycks and Yixuan Li and Mohammad Hossein Rohban and Mohammad Sabokrou},
   title = {A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges},
   url = {https://github.com/taslimisina/osr-ood-ad-methods}
}
@book{Becker2013,
   abstract = {This Festschrift in honour of Ursula Gather’s 60th birthday deals with modern topics in the field of robust statistical methods, especially for time series and regression analysis, and with statistical methods for complex data structures. The individual contributions of leading experts provide a textbook-style overview of the topic, supplemented by current research results and questions. The statistical theory and methods in this volume aim at the analysis of data which deviate from classical stringent model assumptions, which contain outlying values and/or have a complex structure. Written for researchers as well as master and PhD students with a good knowledge of statistics.},
   author = {Claudia Becker and Roland Fried and Sonja Kuhnt},
   doi = {10.1007/978-3-642-35494-6},
   isbn = {9783642354946},
   journal = {Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather},
   month = {1},
   pages = {1-379},
   publisher = {Springer Berlin Heidelberg},
   title = {Robustness and complex data structures: Festschrift in honour of Ursula Gather},
   year = {2013}
}
@book{Mateu2022,
   abstract = {"Spatial functional data (SFD) arises when we have functional data (curves or images) at each one of the several sites or areas of a region. Statistics for SFD is concerned with the application of methods for modeling this type of data. All the fields of spatial statistics (point patterns, areal data and geostatistics) have been adapted to the study of SFD. For example, in point patterns analysis, the functional mark correlation function is proposed as a counterpart of the mark correlation function; in areal data, analysis of a functional areal dataset consisting of population pyramids for 38 neighborhoods in Barcelona (Spain) has been proposed; and in geostatistical analysis diverse approaches for kriging of functional data have been given. In the last few years, some alternatives have been adapted for considering models for SFD, where the estimation of the spatial correlation is of interest. When a functional variable is measured in sites of a region, i.e. when there is a realisation of a functional random field (spatial functional stochastic process), it is important to test for significant spatial autocorrelation and study this correlation if present. Assessing whether SFD are or are not spatially correlated allows us to properly formulate a functional model. However, searching in the literature, it is clear that amongst the several categories of spatial functional methods, functional geostatistics has been much more developed considering both new methodological approaches and analysis of a wide range of case studies covering a wealth of varied fields of applications"-- Provided by publisher. Introduction to geostatistical functional data analysis -- Mathematical foundations of functional kriging in Hilbert spaces and Riemannian manifolds -- Universal, residual and external drift functional kriging -- Extending functional kriging when data are multivariate curves : some technical considerations and operational solutions -- Geostatistical analysis in Bayes spaces : probability densities and compositional data -- Spatial functional data analysis for probability density functions : compositional functional data vs distributional data approach -- Clustering spatial functional data -- Nonparametric statistical analysis of spatially distributed functional data -- A non parametric algorithm for spatially dependent functional data : bagging Voronoi for clustering, dimensional reduction and regression -- Non-parametric inference for spatio-temporal data based on local null hypothesis testing for functional data -- A penalized regression model for spatial functional data with application to the analysis of the production of waste in Venice Province -- Quasi-maximum likelihood estimators for functional linear spatial autoregressive models -- Spatial prediction and optimal sampling for multivariate functional random fields -- Spatio-temporal functional data analysis -- A comparison of spatio-temporal and functional kriging approaches -- From spatio-temporal smoothing to functional spatial regression : a penalized approach.},
   author = {Jorge. Mateu and Ramon. Giraldo},
   isbn = {9781119387848},
   publisher = {John Wiley \& Sons, Inc.},
   title = {Geostatistical functional data analysis},
   year = {2022}
}
@article{Herrmann2023,
   abstract = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption, that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under: Technologies > Structure Discovery and Clustering Fundamental Concepts of Data and Knowledge > Data Concepts Technologies > Visualization.},
   author = {Moritz Herrmann and Florian Pfisterer and Fabian Scheipl},
   doi = {10.1002/widm.1491},
   issn = {19424795},
   issue = {3},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   keywords = {anomaly detection,dimension reduction,manifold learning,outlier detection},
   month = {5},
   publisher = {John Wiley and Sons Inc},
   title = {A geometric framework for outlier detection in high-dimensional data},
   volume = {13},
   year = {2023}
}
@article{Ruff2021,
   abstract = {Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in detection performance on complex data sets, such as large collections of images or text. These results have sparked a renewed interest in the AD problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review, we aim to identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that are enriched by the use of recent explainability techniques and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in AD.},
   author = {Lukas Ruff and Jacob R. Kauffmann and Robert A. Vandermeulen and Gregoire Montavon and Wojciech Samek and Marius Kloft and Thomas G. Dietterich and Klaus Robert Muller},
   doi = {10.1109/JPROC.2021.3052449},
   issn = {15582256},
   issue = {5},
   journal = {Proceedings of the IEEE},
   keywords = {Anomaly detection (AD),deep learning,explainable artificial intelligence,interpretability,kernel methods,neural networks,novelty detection,one-class classification,out-of-distribution (OOD) detection,outlier detection,unsupervised learning.},
   month = {5},
   pages = {756-795},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Unifying Review of Deep and Shallow Anomaly Detection},
   volume = {109},
   year = {2021}
}
